 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF'
diff --git a/src/Medical_KG/embeddings/gpu.py b/src/Medical_KG/embeddings/gpu.py
index 6b890f8eb131508467f671ed2aa4bf43d4fe5348..9ecc9feaa692ef43263010a35b9c9a20b99d3c4e 100644
--- a/src/Medical_KG/embeddings/gpu.py
+++ b/src/Medical_KG/embeddings/gpu.py
@@ -36,50 +36,55 @@ class GPUValidator:
             or not torch_module.cuda.is_available()
         ):
             raise GPURequirementError(
                 "GPU required for embeddings but torch.cuda.is_available() returned False"
             )
         try:
             result = subprocess.run(
                 ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                 check=True,
                 stdout=subprocess.PIPE,
                 stderr=subprocess.PIPE,
                 text=True,
                 timeout=5,
             )
         except FileNotFoundError as exc:  # pragma: no cover - extremely unlikely in tests
             raise GPURequirementError(
                 "GPU required for embeddings but nvidia-smi is not available"
             ) from exc
         except subprocess.SubprocessError as exc:
             raise GPURequirementError("Failed to execute nvidia-smi for GPU validation") from exc
         names = [line.strip() for line in result.stdout.splitlines() if line.strip()]
         if not names:
             raise GPURequirementError(
                 "GPU required for embeddings but no devices were reported by nvidia-smi"
             )
+        mem_info = getattr(torch_module.cuda, "mem_get_info", None)
+        if callable(mem_info):
+            free_mem, total_mem = mem_info()
+            if total_mem is None or float(total_mem) <= 0.0:
+                raise GPURequirementError("GPU reported invalid memory capacity")

     def validate_vllm(self, endpoint: str) -> None:
         if not self.should_require_gpu():
             return
         url = endpoint.rstrip("/") + "/health"
         status_code = self._get(url)
         if status_code != 200:
             raise GPURequirementError(f"vLLM health check at {url} returned status {status_code}")


     def _get(self, url: str) -> int:
         if self.http_getter:
             return self.http_getter(url)
         client = create_client(timeout=2.0)
         try:
             response = client.get(url)
             return response.status_code
         finally:
             client.close()


 def enforce_gpu_or_exit(
     *, endpoint: str | None = None, validator: GPUValidator | None = None
 ) -> None:
     """Validate GPU availability and exit with code 99 on failure."""
diff --git a/src/Medical_KG/embeddings/metrics.py b/src/Medical_KG/embeddings/metrics.py
new file mode 100644
index 0000000000000000000000000000000000000000..9f7c26c0d1e836508752794d41001cd059c1ac64
--- /dev/null
+++ b/src/Medical_KG/embeddings/metrics.py
@@ -0,0 +1,29 @@
+"""Prometheus metrics for embedding service operations."""
+
+from __future__ import annotations
+
+from Medical_KG.compat.prometheus import Counter, Histogram
+
+EMBEDDING_REQUESTS = Counter(
+    "embedding_requests_total",
+    "Total number of embedding batches processed",
+    ["model", "device"],
+)
+
+EMBEDDING_ERRORS = Counter(
+    "embedding_request_errors_total",
+    "Total number of embedding failures",
+    ["model", "device"],
+)
+
+EMBEDDING_LATENCY = Histogram(
+    "embedding_request_latency_seconds",
+    "Latency of embedding requests",
+    ["model", "device"],
+)
+
+__all__ = [
+    "EMBEDDING_ERRORS",
+    "EMBEDDING_LATENCY",
+    "EMBEDDING_REQUESTS",
+]
diff --git a/src/Medical_KG/embeddings/service.py b/src/Medical_KG/embeddings/service.py
index b9e57ff217935f46a8972f9be08e85e33cc871f3..1b1dfce8430a9a12948a113f0c6384b30d6bd261 100644
--- a/src/Medical_KG/embeddings/service.py
+++ b/src/Medical_KG/embeddings/service.py
@@ -1,73 +1,91 @@
 """High-level embedding orchestration for dense and sparse representations."""

 from __future__ import annotations

+import logging
 import time
 from dataclasses import dataclass, field
 from typing import List, Sequence

-from .gpu import GPUValidator
+from .gpu import GPURequirementError, GPUValidator
+from .metrics import EMBEDDING_ERRORS, EMBEDDING_LATENCY, EMBEDDING_REQUESTS
 from .qwen import QwenEmbeddingClient
 from .splade import SPLADEExpander


+LOGGER = logging.getLogger(__name__)
+
+
 @dataclass(slots=True)
 class EmbeddingMetrics:
     """Capture simple throughput metrics for embedding operations."""

     dense_tokens_per_second: float = 0.0
     dense_batch_size: int = 0
     sparse_terms_per_second: float = 0.0


 @dataclass(slots=True)
 class EmbeddingService:
     """Combine dense (Qwen) and sparse (SPLADE) embedding backends."""

     qwen: QwenEmbeddingClient
     splade: SPLADEExpander
     metrics: EmbeddingMetrics = field(default_factory=EmbeddingMetrics)
     gpu_validator: GPUValidator | None = None

     def embed_texts(
         self, texts: Sequence[str], *, sparse_texts: Sequence[str] | None = None
     ) -> tuple[List[List[float]], List[dict[str, float]]]:
         if not texts:
             return [], []
+        device_label = "gpu" if self.gpu_validator else "cpu"
         if self.gpu_validator:
-            self.gpu_validator.validate()
+            try:
+                self.gpu_validator.validate()
+            except GPURequirementError:
+                LOGGER.warning("GPU unavailable for embeddings, falling back to CPU")
+                device_label = "cpu_fallback"
         dense_start = time.perf_counter()
-        dense_vectors = self.qwen.embed(texts)
+        try:
+            dense_vectors = self.qwen.embed(texts)
+        except Exception:
+            EMBEDDING_ERRORS.labels(model=self.qwen.model, device=device_label).inc()
+            raise
         dense_duration = max(time.perf_counter() - dense_start, 1e-6)
         total_tokens = sum(len(text.split()) for text in texts)
         self.metrics.dense_tokens_per_second = total_tokens / dense_duration
         self.metrics.dense_batch_size = max(len(texts), 1)

         sparse_inputs = list(sparse_texts) if sparse_texts is not None else list(texts)
         sparse_start = time.perf_counter()
         sparse_vectors = self.splade.expand(sparse_inputs)
         sparse_duration = max(time.perf_counter() - sparse_start, 1e-6)
         total_terms = sum(len(terms) for terms in sparse_vectors)
         self.metrics.sparse_terms_per_second = total_terms / sparse_duration if total_terms else 0.0

+        duration = max(time.perf_counter() - dense_start, 1e-6)
+        EMBEDDING_REQUESTS.labels(model=self.qwen.model, device=device_label).inc(len(texts))
+        EMBEDDING_LATENCY.labels(model=self.qwen.model, device=device_label).observe(duration)
+
         return dense_vectors, sparse_vectors

     def embed_concepts(self, concepts: Sequence["ConceptLike"]) -> None:
         texts = [concept.to_embedding_text() for concept in concepts]
         dense_vectors, sparse_vectors = self.embed_texts(texts)
         for concept, dense, sparse in zip(concepts, dense_vectors, sparse_vectors):
             concept.embedding_qwen = dense
             concept.splade_terms = sparse


 class ConceptLike:
     """Protocol-like runtime type used for duck typing in embedding service."""

     embedding_qwen: List[float]
     splade_terms: dict[str, float]

     def to_embedding_text(self) -> str:  # pragma: no cover - documented contract
         raise NotImplementedError


 __all__ = ["EmbeddingMetrics", "EmbeddingService"]
diff --git a/src/Medical_KG/kg/fhir.py b/src/Medical_KG/kg/fhir.py
index dba7a71afce1506b8b18066e6fad0193d598ef7f..53ad998378dd3f79fe428d3f141b9e971c5f36d8 100644
--- a/src/Medical_KG/kg/fhir.py
+++ b/src/Medical_KG/kg/fhir.py
@@ -1,29 +1,29 @@
 from __future__ import annotations

 from dataclasses import dataclass
-from typing import Any, Iterable, Mapping
+from typing import Any, Iterable, Mapping, MutableMapping, Sequence


 @dataclass(slots=True)
 class FhirResource:
     resource_type: str
     payload: Mapping[str, Any]


 class ConceptLexicon:
     """Validates that codes originate from allowed code systems."""

     def __init__(self, vocabulary: Mapping[str, Iterable[str]] | None = None) -> None:
         self._vocabulary = {system: set(codes) for system, codes in (vocabulary or {}).items()}

     def validate(self, system: str, code: str) -> None:
         allowed = self._vocabulary.get(system)
         if allowed is not None and code not in allowed:
             raise ValueError(f"Code {code!r} is not registered for system {system!r}")


 class EvidenceExporter:
     """Converts KG nodes into simplified FHIR Evidence resources."""

     def __init__(
         self,
@@ -79,25 +79,165 @@ class EvidenceExporter:
             "id": node.get("id"),
             "name": node.get("name"),
             "status": "active",
             "characteristic": characteristics,
         }
         return FhirResource(resource_type="EvidenceVariable", payload=payload)

     def export_provenance(
         self,
         extraction_activity: Mapping[str, Any],
         *,
         target_reference: str,
     ) -> FhirResource:
         agent = {
             "type": {"coding": [{"system": "http://terminology.hl7.org/CodeSystem/provenance-participant-type", "code": "author"}]},
             "who": {"display": extraction_activity.get("model")},
         }
         payload = {
             "resourceType": "Provenance",
             "target": [{"reference": target_reference}],
             "recorded": extraction_activity.get("timestamp"),
             "activity": {"display": extraction_activity.get("prompt_hash")},
             "agent": [agent],
         }
         return FhirResource(resource_type="Provenance", payload=payload)
+
+
+@dataclass(slots=True)
+class GraphMapping:
+    """Simple container describing nodes and relationships for KG writes."""
+
+    nodes: list[dict[str, Any]]
+    relationships: list[dict[str, Any]]
+
+
+class FhirGraphMapper:
+    """Map core FHIR resources into graph-friendly dictionaries."""
+
+    def __init__(self, *, lexicon: ConceptLexicon | None = None) -> None:
+        self.lexicon = lexicon or ConceptLexicon()
+
+    def map_patient(self, patient: Mapping[str, Any]) -> GraphMapping:
+        patient_id = str(patient.get("id"))
+        identifiers = patient.get("identifier", [])
+        patient_node: dict[str, Any] = {
+            "label": "Patient",
+            "id": patient_id,
+            "gender": patient.get("gender"),
+            "birth_date": patient.get("birthDate"),
+            "extensions": patient.get("extension", []),
+            "identifier_values": [identifier.get("value") for identifier in identifiers if identifier.get("value")],
+        }
+        nodes: list[dict[str, Any]] = [patient_node]
+        relationships: list[dict[str, Any]] = []
+        for identifier in identifiers:
+            value = identifier.get("value")
+            if not value:
+                continue
+            system = identifier.get("system") or ""
+            identifier_id = f"{system}|{value}" if system else value
+            nodes.append(
+                {
+                    "label": "Identifier",
+                    "id": identifier_id,
+                    "system": system or None,
+                    "value": value,
+                }
+            )
+            relationships.append(
+                {
+                    "type": "HAS_IDENTIFIER",
+                    "start_id": patient_id,
+                    "end_id": identifier_id,
+                    "properties": {},
+                }
+            )
+        return GraphMapping(nodes=nodes, relationships=relationships)
+
+    def map_condition(self, condition: Mapping[str, Any]) -> GraphMapping:
+        condition_id = str(condition.get("id"))
+        codes = self._extract_codes(condition.get("code"))
+        node = {
+            "label": "Condition",
+            "id": condition_id,
+            "codes": codes,
+            "severity": self._code_as_dict(condition.get("severity")),
+            "clinical_status": self._code_as_dict(condition.get("clinicalStatus")),
+        }
+        relationships = self._subject_relationship(condition, condition_id, "HAS_CONDITION")
+        return GraphMapping(nodes=[node], relationships=relationships)
+
+    def map_medication_statement(self, statement: Mapping[str, Any]) -> GraphMapping:
+        statement_id = str(statement.get("id"))
+        dosage = self._parse_dosage(statement.get("dosage", []))
+        node = {
+            "label": "MedicationStatement",
+            "id": statement_id,
+            "codes": self._extract_codes(statement.get("medicationCodeableConcept")),
+            "status": statement.get("status"),
+            "dosage": dosage,
+            "effective_period": statement.get("effectivePeriod"),
+        }
+        relationships = self._subject_relationship(statement, statement_id, "HAS_MEDICATION")
+        return GraphMapping(nodes=[node], relationships=relationships)
+
+    def map_observation(self, observation: Mapping[str, Any]) -> GraphMapping:
+        observation_id = str(observation.get("id"))
+        quantity = observation.get("valueQuantity", {})
+        node = {
+            "label": "Observation",
+            "id": observation_id,
+            "codes": self._extract_codes(observation.get("code")),
+            "value": quantity.get("value"),
+            "unit": quantity.get("unit"),
+            "reference_range": observation.get("referenceRange"),
+        }
+        relationships = self._subject_relationship(observation, observation_id, "HAS_OBSERVATION")
+        return GraphMapping(nodes=[node], relationships=relationships)
+
+    def _subject_relationship(
+        self, resource: Mapping[str, Any], target_id: str, rel_type: str
+    ) -> list[dict[str, Any]]:
+        subject = resource.get("subject") or {}
+        reference = subject.get("reference")
+        if not reference:
+            return []
+        return [
+            {
+                "type": rel_type,
+                "start_id": reference,
+                "end_id": target_id,
+                "properties": {},
+            }
+        ]
+
+    def _extract_codes(self, concept: Mapping[str, Any] | None) -> list[dict[str, str]]:
+        if not concept:
+            return []
+        codes: list[dict[str, str]] = []
+        for coding in concept.get("coding", []):
+            system = coding.get("system")
+            code = coding.get("code")
+            if not system or not code:
+                continue
+            self.lexicon.validate(system, code)
+            entry: MutableMapping[str, str] = {"system": system, "code": code}
+            if coding.get("display"):
+                entry["display"] = coding["display"]
+            codes.append(dict(entry))
+        return codes
+
+    def _code_as_dict(self, concept: Mapping[str, Any] | None) -> dict[str, str] | None:
+        codes = self._extract_codes(concept) if concept else []
+        return codes[0] if codes else None
+
+    def _parse_dosage(self, entries: Sequence[Mapping[str, Any]]) -> Mapping[str, Any] | None:
+        if not entries:
+            return None
+        entry = dict(entries[0])
+        return {
+            "text": entry.get("text"),
+            "timing": entry.get("timing"),
+            "route": self._code_as_dict(entry.get("route")),
+            "dose_and_rate": entry.get("doseAndRate"),
+        }
diff --git a/tests/conftest.py b/tests/conftest.py
index 1828d18b235f75af2475a2a498ec190177ed70bd..47c5d037ba3c65fde70649270f66c8b1d3a366b3 100644
--- a/tests/conftest.py
+++ b/tests/conftest.py
@@ -23,50 +23,83 @@ if str(SRC) not in sys.path:
 if "fastapi" not in sys.modules:
     fastapi_module = types.ModuleType("fastapi")

     class _FastAPI:
         def __init__(self, *args: Any, **kwargs: Any) -> None:
             self.args = args
             self.kwargs = kwargs

     class _APIRouter:
         def __init__(self, *args: Any, **kwargs: Any) -> None:
             self.args = args
             self.kwargs = kwargs
             self.routes: list[tuple[str, Callable[..., Any]]] = []

         def post(self, path: str, **_options: Any) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
             def _decorator(func: Callable[..., Any]) -> Callable[..., Any]:
                 self.routes.append((path, func))
                 return func

             return _decorator

     fastapi_module.FastAPI = _FastAPI
     fastapi_module.APIRouter = _APIRouter
     sys.modules["fastapi"] = fastapi_module

+if "httpx" not in sys.modules:
+    class _DummyResponse:
+        def __init__(self, status_code: int = 200) -> None:
+            self.status_code = status_code
+
+        def json(self) -> dict[str, object]:
+            return {}
+
+        def raise_for_status(self) -> None:
+            if self.status_code >= 400:
+                raise RuntimeError("HTTP error")
+
+    class _DummyHttpx:
+        def __init__(self, *args: Any, **kwargs: Any) -> None:
+            self.status_code = kwargs.get("status_code", 200)
+
+        def __enter__(self) -> "_DummyHttpx":
+            return self
+
+        def __exit__(self, *exc_info: object) -> None:
+            return None
+
+        def get(self, *_: Any, **__: Any) -> _DummyResponse:
+            return _DummyResponse(self.status_code)
+
+        def post(self, *_: Any, **__: Any) -> _DummyResponse:
+            return _DummyResponse(self.status_code)
+
+        def close(self) -> None:
+            return None
+
+    sys.modules["httpx"] = types.SimpleNamespace(Client=_DummyHttpx, HTTPError=RuntimeError)
+
 from trace import Trace

 import pytest

 from Medical_KG.ingestion.ledger import LedgerEntry
 from Medical_KG.retrieval.models import RetrievalRequest, RetrievalResponse, RetrievalResult, RetrieverScores
 from Medical_KG.retrieval.types import JSONValue, SearchHit, VectorHit
 from Medical_KG.utils.optional_dependencies import get_httpx_module


 @pytest.fixture
 def monkeypatch_fixture(monkeypatch: pytest.MonkeyPatch) -> pytest.MonkeyPatch:
     return monkeypatch

 _TRACE = Trace(count=True, trace=False)


 def _activate_tracing() -> None:  # pragma: no cover - instrumentation only
     trace_func = cast(Any, _TRACE.globaltrace)
     if trace_func is None:
         return
     sys.settrace(trace_func)
     threading.settrace(trace_func)


diff --git a/tests/embeddings/conftest.py b/tests/embeddings/conftest.py
index 884a24d490c321a2e20e243c4d1d6c4717433f23..e289ea0ecf008360c6a27c3f4e738c5c35745a62 100644
--- a/tests/embeddings/conftest.py
+++ b/tests/embeddings/conftest.py
@@ -1,39 +1,70 @@
 from __future__ import annotations

 import sys
 from types import SimpleNamespace

+import pytest
+
+
+class _MetricStub:
+    def __init__(self) -> None:
+        self.calls: list[tuple[str, dict[str, str] | float]] = []
+
+    def labels(self, **labels: str) -> "_MetricStub":
+        self.calls.append(("labels", labels))
+        return self
+
+    def inc(self, amount: float = 1.0) -> None:
+        self.calls.append(("inc", amount))
+
+    def observe(self, value: float) -> None:
+        self.calls.append(("observe", value))
+

 class _DummyResponse:
     def __init__(self, status_code: int = 200) -> None:
         self.status_code = status_code

     def json(self) -> dict[str, object]:
         return {"data": []}

     def raise_for_status(self) -> None:
         if self.status_code >= 400:
             raise RuntimeError("HTTP error")


 class _DummyClient:
     def __init__(self, *_, **__) -> None:
         self._status = 200

     def __enter__(self) -> "_DummyClient":
         return self

     def __exit__(self, exc_type, exc, tb) -> bool:
         return False

     def get(self, url: str) -> _DummyResponse:
         return _DummyResponse(self._status)

     def post(self, url: str, json: dict[str, object]) -> _DummyResponse:
         return _DummyResponse()

     def close(self) -> None:
         return None


 sys.modules.setdefault("httpx", SimpleNamespace(Client=_DummyClient))
+
+
+@pytest.fixture(autouse=True)
+def stub_embedding_metrics(monkeypatch: pytest.MonkeyPatch) -> dict[str, _MetricStub]:
+    requests = _MetricStub()
+    errors = _MetricStub()
+    latency = _MetricStub()
+    monkeypatch.setattr("Medical_KG.embeddings.metrics.EMBEDDING_REQUESTS", requests)
+    monkeypatch.setattr("Medical_KG.embeddings.metrics.EMBEDDING_ERRORS", errors)
+    monkeypatch.setattr("Medical_KG.embeddings.metrics.EMBEDDING_LATENCY", latency)
+    monkeypatch.setattr("Medical_KG.embeddings.service.EMBEDDING_REQUESTS", requests)
+    monkeypatch.setattr("Medical_KG.embeddings.service.EMBEDDING_ERRORS", errors)
+    monkeypatch.setattr("Medical_KG.embeddings.service.EMBEDDING_LATENCY", latency)
+    return {"requests": requests, "errors": errors, "latency": latency}
diff --git a/tests/embeddings/test_embedding_service.py b/tests/embeddings/test_embedding_service.py
index 89235acc6418373c518fbb3727eee1ce7e214d62..d1701d0e48c10f58d6d254dfffa4652f7db706e5 100644
--- a/tests/embeddings/test_embedding_service.py
+++ b/tests/embeddings/test_embedding_service.py
@@ -1,37 +1,39 @@
 from __future__ import annotations

 from dataclasses import dataclass, field
 from typing import Sequence

 import pytest

-from Medical_KG.embeddings.gpu import GPUValidator
+from Medical_KG.embeddings.gpu import GPURequirementError, GPUValidator
 from Medical_KG.embeddings.service import EmbeddingMetrics, EmbeddingService


 class FakeQwen:
+    model = "fake-qwen"
+
     def __init__(self) -> None:
         self.calls: list[Sequence[str]] = []

     def embed(self, texts: Sequence[str]) -> list[list[float]]:
         self.calls.append(texts)
         return [[float(len(text)) for _ in range(2)] for text in texts]


 class FakeSplade:
     def __init__(self) -> None:
         self.calls: list[Sequence[str]] = []

     def expand(self, texts: Sequence[str]) -> list[dict[str, float]]:
         self.calls.append(texts)
         return [{word: 1.0} for word in texts]


 class RecordingValidator(GPUValidator):
     def __init__(self) -> None:
         super().__init__()
         self.called = False

     def validate(self) -> None:  # type: ignore[override]
         self.called = True

@@ -47,25 +49,62 @@ class Concept:


 def test_embed_texts_updates_metrics() -> None:
     service = EmbeddingService(qwen=FakeQwen(), splade=FakeSplade())
     dense, sparse = service.embed_texts(["alpha", "beta"])
     assert len(dense) == 2
     assert len(sparse) == 2
     assert isinstance(service.metrics, EmbeddingMetrics)
     assert service.metrics.dense_tokens_per_second > 0
     assert service.metrics.sparse_terms_per_second >= 0


 def test_embed_texts_invokes_gpu_validator() -> None:
     validator = RecordingValidator()
     service = EmbeddingService(qwen=FakeQwen(), splade=FakeSplade(), gpu_validator=validator)
     service.embed_texts(["alpha"])
     assert validator.called is True


 def test_embed_concepts_assigns_vectors() -> None:
     service = EmbeddingService(qwen=FakeQwen(), splade=FakeSplade())
     concept = Concept("condition example")
     service.embed_concepts([concept])
     assert concept.embedding_qwen[0] == pytest.approx(len(concept.label))
     assert concept.splade_terms[concept.label] == 1.0
+
+
+def test_embed_texts_records_metrics(stub_embedding_metrics: dict[str, object]) -> None:
+    service = EmbeddingService(qwen=FakeQwen(), splade=FakeSplade())
+    service.embed_texts(["alpha", "beta"])
+    request_calls = stub_embedding_metrics["requests"].calls  # type: ignore[index]
+    latency_calls = stub_embedding_metrics["latency"].calls  # type: ignore[index]
+    assert any(call[0] == "labels" for call in request_calls)
+    assert any(call[0] == "inc" and call[1] == 2 for call in request_calls)
+    assert any(call[0] == "observe" for call in latency_calls)
+
+
+def test_embedding_service_fallback_logs_warning(
+    caplog: pytest.LogCaptureFixture, stub_embedding_metrics: dict[str, object]
+) -> None:
+    class FailingGPU(GPUValidator):
+        def validate(self) -> None:  # type: ignore[override]
+            raise GPURequirementError("no gpu")
+
+    service = EmbeddingService(qwen=FakeQwen(), splade=FakeSplade(), gpu_validator=FailingGPU())
+    with caplog.at_level("WARNING"):
+        service.embed_texts(["alpha"])
+    assert "falling back to CPU" in caplog.text
+    labels_call = next(call for call in stub_embedding_metrics["requests"].calls if call[0] == "labels")  # type: ignore[index]
+    assert labels_call[1]["device"] == "cpu_fallback"  # type: ignore[index]
+
+
+def test_embedding_service_records_error_on_failure(stub_embedding_metrics: dict[str, object]) -> None:
+    class FailingQwen(FakeQwen):
+        def embed(self, texts: Sequence[str]) -> list[list[float]]:  # type: ignore[override]
+            raise RuntimeError("boom")
+
+    service = EmbeddingService(qwen=FailingQwen(), splade=FakeSplade())
+    with pytest.raises(RuntimeError):
+        service.embed_texts(["alpha"])
+    error_calls = stub_embedding_metrics["errors"].calls  # type: ignore[index]
+    assert any(call[0] == "inc" for call in error_calls)
diff --git a/tests/embeddings/test_gpu.py b/tests/embeddings/test_gpu.py
index e9d6ff9e57c1498de6f5718d53c716bd357a0530..2c1ca9e382fc7d7b60b641c4ec4123d1197c0fe9 100644
--- a/tests/embeddings/test_gpu.py
+++ b/tests/embeddings/test_gpu.py
@@ -1,46 +1,66 @@
 from __future__ import annotations

 from types import SimpleNamespace

 import pytest

 from Medical_KG.embeddings.gpu import GPURequirementError, GPUValidator, enforce_gpu_or_exit


 @pytest.fixture(autouse=True)
 def reset_env(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.delenv("REQUIRE_GPU", raising=False)
-    stub = SimpleNamespace(cuda=SimpleNamespace(is_available=lambda: True))
-    monkeypatch.setattr("Medical_KG.embeddings.gpu.TORCH", stub)
+    stub = SimpleNamespace(
+        cuda=SimpleNamespace(is_available=lambda: True, mem_get_info=lambda: (1024.0, 2048.0))
+    )
+    monkeypatch.setattr("Medical_KG.embeddings.gpu.load_torch", lambda: stub)
+    monkeypatch.setattr(
+        "Medical_KG.embeddings.gpu.subprocess.run",
+        lambda *args, **kwargs: SimpleNamespace(stdout="gpu0"),
+    )


 def test_gpu_validator_skips_when_disabled(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setenv("REQUIRE_GPU", "0")
     validator = GPUValidator()
     validator.validate()  # does not raise


 def test_gpu_validator_raises_without_gpu(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setenv("REQUIRE_GPU", "1")
     stub = SimpleNamespace(cuda=SimpleNamespace(is_available=lambda: False))
-    monkeypatch.setattr("Medical_KG.embeddings.gpu.TORCH", stub)
+    monkeypatch.setattr("Medical_KG.embeddings.gpu.load_torch", lambda: stub)
     validator = GPUValidator()
     with pytest.raises(GPURequirementError):
         validator.validate()


+def test_gpu_validator_checks_memory(monkeypatch: pytest.MonkeyPatch) -> None:
+    monkeypatch.setenv("REQUIRE_GPU", "1")
+    stub = SimpleNamespace(
+        cuda=SimpleNamespace(
+            is_available=lambda: True,
+            mem_get_info=lambda: (0.0, 0.0),
+        )
+    )
+    monkeypatch.setattr("Medical_KG.embeddings.gpu.load_torch", lambda: stub)
+    validator = GPUValidator()
+    with pytest.raises(GPURequirementError, match="memory"):
+        validator.validate()
+
+
 def test_gpu_validator_validates_vllm(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setenv("REQUIRE_GPU", "1")
     validator = GPUValidator(http_getter=lambda url: 503)
     with pytest.raises(GPURequirementError):
         validator.validate_vllm("http://localhost:8000")


 def test_enforce_gpu_or_exit(monkeypatch: pytest.MonkeyPatch) -> None:
     class FailingValidator(GPUValidator):
         def validate(self) -> None:  # type: ignore[override]
             raise GPURequirementError("missing gpu")

     with pytest.raises(SystemExit) as excinfo:
         enforce_gpu_or_exit(validator=FailingValidator())
     assert excinfo.value.code == 99
diff --git a/tests/kg/fixtures/__init__.py b/tests/kg/fixtures/__init__.py
new file mode 100644
index 0000000000000000000000000000000000000000..cce1570012cfd1ddf395fb445a1c063967f832a5
--- /dev/null
+++ b/tests/kg/fixtures/__init__.py
@@ -0,0 +1,9 @@
+"""Fixtures for knowledge-graph unit tests."""
+
+from .neo4j import FakeNeo4jDriver, sample_relationship_records, sample_result_records
+
+__all__ = [
+    "FakeNeo4jDriver",
+    "sample_relationship_records",
+    "sample_result_records",
+]
diff --git a/tests/kg/fixtures/neo4j.py b/tests/kg/fixtures/neo4j.py
new file mode 100644
index 0000000000000000000000000000000000000000..9dcbfe35e10a919cc419f893371ebd765d524d02
--- /dev/null
+++ b/tests/kg/fixtures/neo4j.py
@@ -0,0 +1,214 @@
+from __future__ import annotations
+
+import copy
+import json
+import re
+from dataclasses import dataclass, field
+from typing import Any, Dict, Iterable, Mapping, MutableMapping, Sequence
+
+from Medical_KG.kg.writer import NODE_KEYS
+
+_NODE_MATCH = re.compile(r"MATCH \((\w+):(\w+) \{(\w+): \$(\w+)\}\)")
+_REL_MATCH = re.compile(r"MERGE \((\w+)\)-\[[^:]*:(\w+)\]->\((\w+)\)")
+_SET_PROP = re.compile(r"SET r\.(\w+) = \$(\w+)")
+
+
+@dataclass(slots=True)
+class FakeNeo4jDriver:
+    """Minimal in-memory Neo4j driver emulating transactional semantics."""
+
+    unique_constraints: MutableMapping[str, list[tuple[str, ...]]] = field(default_factory=dict)
+    _nodes: dict[str, dict[str, dict[str, Any]]] = field(init=False, repr=False)
+    _relationships: list[dict[str, Any]] = field(init=False, repr=False)
+    _unique_index: dict[str, dict[tuple[str, ...], dict[tuple[Any, ...], str]]] = field(
+        init=False, repr=False
+    )
+
+    def __post_init__(self) -> None:
+        self._nodes: dict[str, dict[str, dict[str, Any]]] = {}
+        self._relationships: list[dict[str, Any]] = []
+        self._unique_index: dict[str, dict[tuple[str, ...], dict[tuple[Any, ...], str]]] = {}
+
+    def session(self) -> "_FakeSession":
+        return _FakeSession(self)
+
+    def add_unique_constraint(self, label: str, *properties: str) -> None:
+        constraints = list(self.unique_constraints.get(label, []))
+        constraints.append(tuple(properties))
+        self.unique_constraints[label] = constraints
+
+    def get_node(self, label: str, key_value: str) -> dict[str, Any] | None:
+        return copy.deepcopy(self._nodes.get(label, {}).get(key_value))
+
+    @property
+    def relationships(self) -> list[dict[str, Any]]:
+        return copy.deepcopy(self._relationships)
+
+
+@dataclass(slots=True)
+class _FakeSession:
+    driver: FakeNeo4jDriver
+
+    def write_transaction(self, fn: Any, *args: Any, **kwargs: Any) -> Any:
+        transaction = _FakeTransaction(self.driver)
+        try:
+            result = fn(transaction, *args, **kwargs)
+        except Exception:
+            transaction.rollback()
+            raise
+        else:
+            transaction.commit()
+            return result
+
+    def close(self) -> None:  # pragma: no cover - parity with real driver
+        return None
+
+
+@dataclass(slots=True)
+class _FakeTransaction:
+    driver: FakeNeo4jDriver
+    _pending_nodes: dict[str, dict[str, dict[str, Any]]] = field(init=False, repr=False)
+    _pending_relationships: list[dict[str, Any]] = field(init=False, repr=False)
+    _unique_index: dict[str, dict[tuple[str, ...], dict[tuple[Any, ...], str]]] = field(
+        init=False, repr=False
+    )
+
+    def __post_init__(self) -> None:
+        self._pending_nodes = copy.deepcopy(self.driver._nodes)
+        self._pending_relationships = copy.deepcopy(self.driver._relationships)
+        self._unique_index = copy.deepcopy(self.driver._unique_index)
+
+    def run(self, statement: str, parameters: Mapping[str, Any] | None = None) -> Sequence[Mapping[str, Any]]:
+        parameters = dict(parameters or {})
+        if statement.strip().startswith("MERGE (n:"):
+            self._merge_node(statement, parameters)
+            return []
+        matches = list(_NODE_MATCH.finditer(statement))
+        rel_match = _REL_MATCH.search(statement)
+        if matches and rel_match:
+            self._merge_relationship(matches, rel_match, statement, parameters)
+            return []
+        raise ValueError(f"Unsupported Cypher statement: {statement}")
+
+    def commit(self) -> None:
+        self.driver._nodes = self._pending_nodes
+        self.driver._relationships = self._pending_relationships
+        self.driver._unique_index = self._unique_index
+
+    def rollback(self) -> None:
+        self._pending_nodes = copy.deepcopy(self.driver._nodes)
+        self._pending_relationships = copy.deepcopy(self.driver._relationships)
+        self._unique_index = copy.deepcopy(self.driver._unique_index)
+
+    def _merge_node(self, statement: str, parameters: Mapping[str, Any]) -> None:
+        label_match = re.search(r"MERGE \(n:(\w+)", statement)
+        key_match = re.search(r"\$props\.(\w+)\}", statement)
+        if not label_match or not key_match:
+            raise ValueError(f"Unable to parse node MERGE: {statement}")
+        label = label_match.group(1)
+        key = key_match.group(1)
+        raw_props = dict(parameters.get("props", {}))
+        props = {k: v for k, v in raw_props.items() if v is not None}
+        null_keys = [key for key, value in raw_props.items() if value is None]
+        if key not in props:
+            raise ValueError(f"Payload for {label} missing key '{key}'")
+        label_nodes = self._pending_nodes.setdefault(label, {})
+        key_value = str(props[key])
+        existing = label_nodes.get(key_value, {})
+        merged = dict(existing)
+        merged.update(props)
+        for null_key in null_keys:
+            merged.pop(null_key, None)
+        for constraint in self._constraints_for(label):
+            values = tuple(merged.get(prop) for prop in constraint)
+            if None in values:
+                continue
+            index = self._unique_index.setdefault(label, {}).setdefault(constraint, {})
+            owner = index.get(values)
+            if owner is None:
+                index[values] = key_value
+            elif owner != key_value:
+                raise ValueError(
+                    f"Constraint violation on {label} for fields {constraint}: {json.dumps(values)}"
+                )
+        label_nodes[key_value] = merged
+
+    def _merge_relationship(
+        self,
+        node_matches: Iterable[re.Match[str]],
+        rel_match: re.Match[str],
+        statement: str,
+        parameters: Mapping[str, Any],
+    ) -> None:
+        start_match, end_match = list(node_matches)[:2]
+        start_var, start_label, start_key, start_param = start_match.groups()
+        end_var, end_label, end_key, end_param = end_match.groups()
+        rel_type = rel_match.group(2)
+        start_id = str(parameters[start_param])
+        end_id = str(parameters[end_param])
+        if start_id not in self._pending_nodes.get(start_label, {}):
+            raise ValueError(f"Missing start node {start_label}:{start_id}")
+        if end_id not in self._pending_nodes.get(end_label, {}):
+            raise ValueError(f"Missing end node {end_label}:{end_id}")
+        rel_props: Dict[str, Any] = {}
+        if "rel_props" in parameters:
+            rel_props.update({k: v for k, v in dict(parameters["rel_props"]).items() if v is not None})
+        for prop, param in _SET_PROP.findall(statement):
+            value = parameters.get(param)
+            if value is not None:
+                rel_props[prop] = value
+            elif prop in rel_props:
+                rel_props.pop(prop, None)
+        existing = next(
+            (
+                rel
+                for rel in self._pending_relationships
+                if rel["type"] == rel_type
+                and rel["start"] == (start_label, start_id)
+                and rel["end"] == (end_label, end_id)
+            ),
+            None,
+        )
+        payload = {"type": rel_type, "start": (start_label, start_id), "end": (end_label, end_id), "properties": rel_props}
+        if existing:
+            existing["properties"].update(rel_props)
+        else:
+            self._pending_relationships.append(payload)
+
+    def _constraints_for(self, label: str) -> Iterable[tuple[str, ...]]:
+        defaults: list[tuple[str, ...]] = []
+        key = NODE_KEYS.get(label)
+        if key:
+            defaults.append((key,))
+        return list(self.driver.unique_constraints.get(label, [])) + defaults
+
+
+def sample_result_records() -> list[dict[str, Any]]:
+    """Return a canned Neo4j result for query builder tests."""
+
+    return [
+        {"document": {"uri": "doc://1", "title": "Example"}, "score": 0.91},
+        {"document": {"uri": "doc://2", "title": "Follow-up"}, "score": 0.88},
+    ]
+
+
+def sample_relationship_records() -> list[dict[str, Any]]:
+    """Return sample relationship payloads for graph assertions."""
+
+    return [
+        {
+            "type": "HAS_CHUNK",
+            "start": {"label": "Document", "id": "doc://1"},
+            "end": {"label": "Chunk", "id": "chunk-1"},
+            "properties": {"order": 1},
+        },
+        {
+            "type": "MEASURES",
+            "start": {"label": "Evidence", "id": "ev-1"},
+            "end": {"label": "Outcome", "id": "out-1"},
+            "properties": {"confidence": 0.9},
+        },
+    ]
+
+
+__all__ = ["FakeNeo4jDriver", "sample_relationship_records", "sample_result_records"]
diff --git a/tests/kg/test_fhir_mapper.py b/tests/kg/test_fhir_mapper.py
new file mode 100644
index 0000000000000000000000000000000000000000..0e20b8b89c137371bb144040779b54a249580269
--- /dev/null
+++ b/tests/kg/test_fhir_mapper.py
@@ -0,0 +1,110 @@
+from __future__ import annotations
+
+import pytest
+
+from Medical_KG.kg.fhir import ConceptLexicon, FhirGraphMapper
+
+
+@pytest.fixture()
+def mapper() -> FhirGraphMapper:
+    lexicon = ConceptLexicon(
+        {
+            "http://loinc.org": {"12345-6", "23456-7"},
+            "http://snomed.info/sct": {"111", "222"},
+            "http://www.nlm.nih.gov/research/umls/rxnorm": {"555"},
+        }
+    )
+    return FhirGraphMapper(lexicon=lexicon)
+
+
+def test_map_patient_creates_identifier_relationships(mapper: FhirGraphMapper) -> None:
+    patient = {
+        "resourceType": "Patient",
+        "id": "Patient/p-1",
+        "gender": "female",
+        "birthDate": "1980-01-01",
+        "extension": [{"url": "ethnicity", "valueString": "Hispanic"}],
+        "identifier": [
+            {"system": "http://hospital", "value": "123"},
+            {"value": "legacy"},
+        ],
+    }
+    mapping = mapper.map_patient(patient)
+    patient_node = next(node for node in mapping.nodes if node["label"] == "Patient")
+    assert patient_node["gender"] == "female"
+    assert "123" in patient_node["identifier_values"]
+    assert len(mapping.relationships) == 2
+    assert all(rel["type"] == "HAS_IDENTIFIER" for rel in mapping.relationships)
+
+
+def test_map_condition_links_to_patient(mapper: FhirGraphMapper) -> None:
+    condition = {
+        "resourceType": "Condition",
+        "id": "Condition/c-1",
+        "subject": {"reference": "Patient/p-1"},
+        "code": {"coding": [{"system": "http://snomed.info/sct", "code": "111", "display": "Asthma"}]},
+        "severity": {"coding": [{"system": "http://snomed.info/sct", "code": "222"}]},
+        "clinicalStatus": {"coding": [{"system": "http://snomed.info/sct", "code": "111"}]},
+    }
+    mapping = mapper.map_condition(condition)
+    assert mapping.nodes[0]["codes"][0]["code"] == "111"
+    assert mapping.relationships == [
+        {"type": "HAS_CONDITION", "start_id": "Patient/p-1", "end_id": "Condition/c-1", "properties": {}}
+    ]
+
+
+def test_map_medication_statement_captures_dosage(mapper: FhirGraphMapper) -> None:
+    statement = {
+        "resourceType": "MedicationStatement",
+        "id": "MedicationStatement/m-1",
+        "status": "active",
+        "subject": {"reference": "Patient/p-1"},
+        "medicationCodeableConcept": {
+            "coding": [
+                {
+                    "system": "http://www.nlm.nih.gov/research/umls/rxnorm",
+                    "code": "555",
+                    "display": "Example Drug",
+                }
+            ]
+        },
+        "effectivePeriod": {"start": "2024-01-01"},
+        "dosage": [
+            {
+                "text": "1 tablet",
+                "timing": {"repeat": {"frequency": 1}},
+                "route": {"coding": [{"system": "http://snomed.info/sct", "code": "111"}]},
+            }
+        ],
+    }
+    mapping = mapper.map_medication_statement(statement)
+    node = mapping.nodes[0]
+    assert node["dosage"]["text"] == "1 tablet"
+    assert mapping.relationships[0]["type"] == "HAS_MEDICATION"
+
+
+def test_map_observation_records_values(mapper: FhirGraphMapper) -> None:
+    observation = {
+        "resourceType": "Observation",
+        "id": "Observation/o-1",
+        "subject": {"reference": "Patient/p-1"},
+        "code": {"coding": [{"system": "http://loinc.org", "code": "12345-6"}]},
+        "valueQuantity": {"value": 4.2, "unit": "mg/dL"},
+        "referenceRange": [{"low": {"value": 3.0}, "high": {"value": 5.0}}],
+    }
+    mapping = mapper.map_observation(observation)
+    node = mapping.nodes[0]
+    assert node["value"] == 4.2
+    assert node["unit"] == "mg/dL"
+    assert mapping.relationships[0]["type"] == "HAS_OBSERVATION"
+
+
+def test_invalid_codeable_concept_raises(mapper: FhirGraphMapper) -> None:
+    with pytest.raises(ValueError):
+        mapper.map_condition(
+            {
+                "id": "Condition/c-2",
+                "subject": {"reference": "Patient/p-1"},
+                "code": {"coding": [{"system": "http://snomed.info/sct", "code": "999"}]},
+            }
+        )
diff --git a/tests/kg/test_schema.py b/tests/kg/test_schema.py
index 54d30f715efa191a601d9bd5759742674e1a4e64..23800907e0bd6cc1925c9a4f3811a7e2663730be 100644
--- a/tests/kg/test_schema.py
+++ b/tests/kg/test_schema.py
@@ -1,22 +1,61 @@
 from Medical_KG.kg.schema import CDKOSchema, NodeProperty, NodeSchema


 def test_node_schema_partitions_required_and_optional() -> None:
     schema = NodeSchema(
         label="Example",
         properties=[
             NodeProperty("id", "string", required=True, description="identifier"),
             NodeProperty("label", "string", description="display"),
         ],
     )
     details = schema.as_dict()
     assert details["required"] == [{"name": "id", "type": "string", "description": "identifier"}]
     assert details["optional"] == [{"name": "label", "type": "string", "description": "display"}]


 def test_default_schema_contains_expected_relationships() -> None:
     schema = CDKOSchema.default()
     assert "Document" in schema.nodes
     assert "HAS_CHUNK" in schema.relationships
     constraint_statements = [constraint.statement for constraint in schema.constraints]
     assert any("Document" in stmt for stmt in constraint_statements)
+
+
+def test_schema_constraints_include_identifier_uniqueness() -> None:
+    schema = CDKOSchema.default()
+    statements = [constraint.statement for constraint in schema.constraints]
+    assert any("Identifier" in stmt and "IS UNIQUE" in stmt for stmt in statements)
+
+
+def test_schema_indexes_cover_vector_and_fulltext() -> None:
+    schema = CDKOSchema.default()
+    statements = [index.statement for index in schema.indexes]
+    assert any("VECTOR INDEX chunk_qwen_idx" in stmt for stmt in statements)
+    assert any("FULLTEXT INDEX chunk_text_ft" in stmt for stmt in statements)
+
+
+def test_schema_migration_diff_detects_removed_constraint() -> None:
+    baseline = CDKOSchema.default()
+    modified = CDKOSchema.default()
+    removed = modified.constraints.pop()
+    baseline_statements = {constraint.statement for constraint in baseline.constraints}
+    modified_statements = {constraint.statement for constraint in modified.constraints}
+    assert removed.statement in baseline_statements - modified_statements
+
+
+def test_schema_describe_lists_nodes_and_relationships() -> None:
+    schema = CDKOSchema.default()
+    description = schema.describe()
+    assert "nodes" in description and "relationships" in description
+    assert "Document" in description["nodes"]
+    assert "HAS_CHUNK" in description["relationships"]
+
+
+def test_schema_introspection_returns_indexes_and_relationship_types() -> None:
+    schema = CDKOSchema.default()
+    introspection = schema.as_statements()
+    assert "constraints" in introspection
+    assert "indexes" in introspection
+    assert "relationship_types" in introspection
+    assert "HAS_CHUNK" in introspection["relationship_types"]
diff --git a/tests/kg/test_writer.py b/tests/kg/test_writer.py
index 60d806d08c461d0d8eb0e1de4c1ecaca919d6ffd..7517bc44698809370dfbbd325c4a478d54668db5 100644
--- a/tests/kg/test_writer.py
+++ b/tests/kg/test_writer.py
@@ -1,31 +1,35 @@
 from __future__ import annotations

+from typing import Any
+
 import pytest

 from Medical_KG.kg.writer import KnowledgeGraphWriter, WriteStatement

+from tests.kg.fixtures import FakeNeo4jDriver
+

 @pytest.fixture()
 def writer() -> KnowledgeGraphWriter:
     return KnowledgeGraphWriter()


 def test_merge_unknown_label_raises(writer: KnowledgeGraphWriter) -> None:
     with pytest.raises(ValueError, match="Unknown node label"):
         writer._merge_node("Unknown", {"id": "x"})  # type: ignore[attr-defined]


 def test_write_document_and_chunk_relationship(writer: KnowledgeGraphWriter) -> None:
     writer.write_document({"uri": "doc://1", "id": "doc-1", "title": "Doc"})
     writer.write_chunk({"id": "chunk-1", "text": "hello"}, document_uri="doc://1", order=3)
     statements = list(writer.statements)
     assert any("MERGE (n:Document" in stmt.cypher for stmt in statements)
     rel = statements[-1]
     assert rel.cypher.endswith("MERGE (d)-[r:HAS_CHUNK]->(c) SET r.order = $order")
     assert rel.parameters["order"] == 3


 def test_write_identifier_requires_fields(writer: KnowledgeGraphWriter) -> None:
     with pytest.raises(ValueError):
         writer.write_identifier({"scheme": "NCT"}, document_uri="doc://1")

@@ -75,25 +79,80 @@ def test_write_evidence_variable_links(writer: KnowledgeGraphWriter) -> None:
         document_uri="doc://1",
         extraction_activity_id="activity-1",
     )
     statements = list(writer.statements)
     assert any("REPORTS" in stmt.cypher for stmt in statements)
     assert any("WAS_GENERATED_BY_VAR" in stmt.cypher for stmt in statements)


 def test_write_adverse_event_links(writer: KnowledgeGraphWriter) -> None:
     writer.write_adverse_event(
         {"id": "ae-1", "count": 10, "denominator": 100},
         study_nct_id="nct-1",
         arm_id="arm-1",
     )
     statements = list(writer.statements)
     assert any("HAS_AE" in stmt.cypher for stmt in statements)
     assert any("MATCH (a:Arm" in stmt.cypher for stmt in statements)


 def test_statements_property_returns_copy(writer: KnowledgeGraphWriter) -> None:
     writer.write_document({"uri": "doc://1", "id": "doc-1"})
     statements = list(writer.statements)
     assert isinstance(statements[0], WriteStatement)
     statements.clear()
     assert list(writer.statements)  # original unchanged
+
+
+def _execute(writer: KnowledgeGraphWriter, driver: FakeNeo4jDriver) -> None:
+    statements = list(writer.statements)
+
+    def _tx(tx: Any) -> None:
+        for statement in statements:
+            tx.run(statement.cypher, statement.parameters)
+
+    session = driver.session()
+    session.write_transaction(_tx)
+
+
+def test_batch_transaction_rolls_back_on_error(writer: KnowledgeGraphWriter) -> None:
+    driver = FakeNeo4jDriver()
+    writer.write_document({"uri": "doc://rollback", "id": "doc-rollback"})
+    writer.write_relationship(
+        "SIMILAR_TO",
+        "chunk-start",
+        "chunk-end",
+        start_label="Chunk",
+        end_label="Chunk",
+    )
+    with pytest.raises(ValueError):
+        _execute(writer, driver)
+    assert driver.get_node("Document", "doc://rollback") is None
+
+
+def test_property_updates_remove_nulls() -> None:
+    driver = FakeNeo4jDriver()
+    writer = KnowledgeGraphWriter()
+    writer.write_document({"uri": "doc://update", "id": "doc-update", "title": "Initial", "language": "en"})
+    _execute(writer, driver)
+
+    writer = KnowledgeGraphWriter()
+    writer.write_document({"uri": "doc://update", "id": "doc-update", "title": None, "language": "fr"})
+    _execute(writer, driver)
+
+    node = driver.get_node("Document", "doc://update")
+    assert node is not None
+    assert node["language"] == "fr"
+    assert "title" not in node
+
+
+def test_constraint_violation_raises(writer: KnowledgeGraphWriter) -> None:
+    driver = FakeNeo4jDriver()
+    driver.add_unique_constraint("Document", "id")
+    writer.write_document({"uri": "doc://one", "id": "conflict"})
+    _execute(writer, driver)
+
+    writer = KnowledgeGraphWriter()
+    writer.write_document({"uri": "doc://two", "id": "conflict"})
+    with pytest.raises(ValueError):
+        _execute(writer, driver)
+    assert driver.get_node("Document", "doc://two") is None

EOF
)
