 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF'
diff --git a/docs/catalog_operations.md b/docs/catalog_operations.md
new file mode 100644
index 0000000000000000000000000000000000000000..b6d5f271e8bfba147ca47f9dd8b61737cfded73e
--- /dev/null
+++ b/docs/catalog_operations.md
@@ -0,0 +1,28 @@
+# Concept Catalog Operations
+
+## License Acquisition
+
+| Source      | Requirement | Notes |
+|-------------|-------------|-------|
+| SNOMED CT   | National release centre approval; affiliate agreement | Record affiliate ID in `licenses.yml` under `loaders.SNOMED`. |
+| UMLS / RxNorm | Annual Terms of Service acceptance | Store API keys in the credential vault referenced by `LICENSES_UMLS_KEY`. |
+| MedDRA      | Subscription with quarterly download rights | Flag as `restricted` in `licenses.yml` to disable ingestion when expired. |
+
+The catalog bootstrap reads `licenses.yml` on startup. Buckets (`open`, `permissive`, `restricted`, `proprietary`) determine whether a loader executes; set `loaders.<ONTOLOGY>.enabled` to `false` to hard-disable a source.
+
+## Refresh Runbook
+
+1. Fetch latest artifacts or API snapshots per ontology (SNOMED quarterly, ICD-11 biannual, MONDO/HPO monthly, RxNorm weekly, GUDID six-hourly).
+2. Update `licenses.yml` if entitlement status changed.
+3. Execute the catalog updater service. It compares release versions, computes the release hash, and skips work if nothing changed.
+4. On change, the updater writes concepts to Neo4j, refreshes `concepts_v1` in OpenSearch, regenerates `analysis/biomed_synonyms.txt`, and reloads analyzers.
+5. Verify the audit log for skipped loaders and ensure vector index `concept_qwen_idx` reports the new `release_hash`.
+
+## Synonym Analyzer Refresh
+
+The catalog pipeline aggregates ontology-specific synonym lists and writes them into the analyzer filter. After each refresh:
+
+- Persist the generated synonyms file to `analysis/biomed_synonyms.txt`.
+- Call `ConceptIndexManager.update_synonyms(...)` to push the list to OpenSearch.
+- Trigger a rolling analyzer reload (`indices.reload_search_analyzers`).
+- Confirm boosted fields (`label^3`, `synonyms.value^2`, `definition^0.5`) respond correctly via a smoke query.
diff --git a/docs/chunking_profiles.md b/docs/chunking_profiles.md
new file mode 100644
index 0000000000000000000000000000000000000000..c365046585f007261f28319457a526d625cfc3b0
--- /dev/null
+++ b/docs/chunking_profiles.md
@@ -0,0 +1,23 @@
+# Semantic Chunking Profiles
+
+## Domain Profiles
+
+- **IMRaD**: 450-token target, 15% overlap, coherence threshold 0.72. Use for journal articles (PMC source systems).
+- **Registry**: 350-token target, overlap 20%, coherence threshold 0.70. Optimised for structured criteria blocks.
+- **Structured Product Label (SPL)**: 300-token target, overlap 18%, hard boundaries on LOINC-driven sections.
+- **Guideline**: 500-token target, 12% overlap, tolerant coherence threshold 0.65 to support narrative recommendations.
+
+Profiles are selected automatically from document metadata (`source_system`, `media_type`). Override by passing a `ChunkingProfile` into `ChunkingPipeline.run` when bespoke tuning is required.
+
+## Tuning Guidance
+
+- **Target Tokens**: Increase for narrative-heavy corpora or when coherence is consistently >0.85. Decrease for data-dense lab reports.
+- **Tau Coherence**: Raising the threshold shortens chunks and yields crisper boundaries; lower values produce longer segments at the expense of topical purity.
+- **Overlap Tokens**: 10–20% balances retrieval recall with indexing cost. Increase when downstream QA requires more context continuity.
+- **Embedding Client**: The chunker uses Qwen embeddings to modulate coherence. Adjust `SemanticChunker.embedding_client.batch_size` to fit GPU memory.
+
+## Fallback Strategies
+
+1. **Multi-Granularity Indexing** – use `ChunkIndexer` to emit chunk-, paragraph-, and section-level documents. Combine with reciprocal rank fusion for blended retrieval.
+2. **Sliding Windows** – for problematic documents, configure 512/768 token windows with 25% overlap via the indexer to supplement semantic chunks.
+3. **Neighbor Merge** – at query time, merge adjacent chunks when cosine similarity ≥0.60; the helper `ChunkIndexer.neighbor_merge` surfaces eligible pairs.
diff --git a/docs/embeddings_gpu.md b/docs/embeddings_gpu.md
new file mode 100644
index 0000000000000000000000000000000000000000..1ba74d1970c7801f65356d4fffe4fd699b9e2c7e
--- /dev/null
+++ b/docs/embeddings_gpu.md
@@ -0,0 +1,22 @@
+# GPU Embedding Service
+
+## vLLM Deployment
+
+- Model: `Qwen/Qwen3-Embedding-8B`
+- Command: `python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen3-Embedding-8B --task embedding --dtype bfloat16 --tensor-parallel-size 1 --max-model-len 32768 --gpu-memory-utilization 0.92 --trust-remote-code`
+- Endpoint: OpenAI-compatible `/v1/embeddings` on port 8000; health probe at `/health`.
+
+Enable autoscaling with a GPU-aware HPA (metrics: GPU util, request queue depth). Configure readiness probes to hit `/health` with a two-second timeout.
+
+## GPU Prerequisites
+
+- NVIDIA driver ≥ 550 and CUDA toolkit matching container runtime.
+- NVIDIA Container Toolkit on Ubuntu 24.04 (`nvidia-container-toolkit` package + `nvidia-ctk runtime configure`).
+- Export `REQUIRE_GPU=1` in the runtime environment; the service fails fast with exit code 99 when CUDA is unavailable.
+
+## Failure Runbook
+
+1. **vLLM Unreachable** – `EmbeddingService` retries transient errors. If the health check fails, redeploy the pod and confirm network ACLs allow port 8000.
+2. **GPU OOM** – reduce `QwenEmbeddingClient.batch_size` or SPLADE `batch_size`; monitor memory via `nvidia-smi` in the pod.
+3. **GPU Missing** – run `enforce_gpu_or_exit()` during startup; investigate driver installation and `nvidia-smi` output if the process exits with code 99.
+4. **SPLADE Model Download Issues** – ensure HuggingFace credentials (if required) are mounted; for offline deployments, pre-populate the cache volume.
diff --git a/openspec/changes/05-add-concept-catalog/tasks.md b/openspec/changes/05-add-concept-catalog/tasks.md
index 10ea40a51f4e8bfe888dea1babefcbe7042f3794..f1bc380bb485775e5f8d4c5c4c2c6646f8a4c99f 100644
--- a/openspec/changes/05-add-concept-catalog/tasks.md
+++ b/openspec/changes/05-add-concept-catalog/tasks.md
@@ -16,72 +16,72 @@
 - [x] 2.6 RxNorm loader (RRF files; ingredients, brands, clinical drugs; TTY hierarchy)
 - [x] 2.7 UNII/GSRS loader (JSON; substances; FDA registry)
 - [x] 2.8 MedDRA loader (MDB/CSV; PTs, LLTs, SOCs; quarterly releases; license-gated)
 - [x] 2.9 CTCAE loader (PDF/Excel; grades 1-5 mapping to MedDRA)
 - [x] 2.10 AccessGUDID loader (CSV/API; UDI-DI, brand, model, attributes)
 - [x] 2.11 ID validators (NCT regex; PMID/PMCID numeric; DOI 10.xxxx; LOINC N-D; SNOMED Verhoeff; GTIN-14 mod-10)

 ## 3. Text Normalization & Cross-walks

 - [x] 3.1 Normalize text (Unicode NFC, lowercase for matching, preserve display case)
 - [x] 3.2 Normalize Greek letters (α→alpha), chemical salts (recognize but keep in display)
 - [x] 3.3 Handle US/UK spellings (anemia/anaemia), plurals (lemmatize)
 - [x] 3.4 Build MONDO bridges for disease mappings
 - [x] 3.5 Use UMLS CUIs for crosswalks (when licensed)
 - [x] 3.6 Mark license_bucket (open, permissive, restricted, proprietary) per concept

 ## 4. Embeddings & SPLADE

 - [x] 4.1 Compute SPLADE doc-side expansion (tokenize with model tokenizer; top-K=200 terms per concept)
 - [x] 4.2 Compute Qwen dense vectors (input: label + top-8 synonyms + definition ≤256 tokens; 4096-D)
 - [x] 4.3 Batch embeddings (256 concepts/batch; adjust to VRAM)
 - [x] 4.4 Dedup identical label+definition across ontologies (compute once, share vectors)

 ## 5. Neo4j Integration

-- [ ] 5.1 Create :Concept nodes with labels :Condition|:Phenotype|:Lab|:Drug|:Substance|:Outcome|:AdverseEvent|:Device
-- [ ] 5.2 Create (:Concept)-[:IS_A]->(:Concept) for taxonomy
-- [ ] 5.3 Create (:Concept)-[:SAME_AS]->(:Concept) for crosswalks
-- [ ] 5.4 Create constraints (concept_iri_unique)
-- [ ] 5.5 Create vector index concept_qwen_idx (4096-D, cosine)
+- [x] 5.1 Create :Concept nodes with labels :Condition|:Phenotype|:Lab|:Drug|:Substance|:Outcome|:AdverseEvent|:Device
+- [x] 5.2 Create (:Concept)-[:IS_A]->(:Concept) for taxonomy
+- [x] 5.3 Create (:Concept)-[:SAME_AS]->(:Concept) for crosswalks
+- [x] 5.4 Create constraints (concept_iri_unique)
+- [x] 5.5 Create vector index concept_qwen_idx (4096-D, cosine)

 ## 6. OpenSearch Index

-- [ ] 6.1 Create concepts_v1 index with mappings (iri keyword, family keyword, label text, synonyms nested, definition text, codes nested, splade_terms rank_features)
-- [ ] 6.2 Configure biomed analyzer (synonym_graph filter with analysis/biomed_synonyms.txt generated from catalog)
-- [ ] 6.3 Set field boosts (label^3, synonyms.value^2, definition^0.5)
-- [ ] 6.4 Bulk index all concepts
+- [x] 6.1 Create concepts_v1 index with mappings (iri keyword, family keyword, label text, synonyms nested, definition text, codes nested, splade_terms rank_features)
+- [x] 6.2 Configure biomed analyzer (synonym_graph filter with analysis/biomed_synonyms.txt generated from catalog)
+- [x] 6.3 Set field boosts (label^3, synonyms.value^2, definition^0.5)
+- [x] 6.4 Bulk index all concepts

 ## 7. Catalog Build Pipeline

 - [x] 7.1 Implement DAG: download → parse → normalize → enrich → merge → embed → write
 - [x] 7.2 Generate catalog release hash (SHA256 of concatenated source versions)
 - [x] 7.3 Write provenance (pipeline_ver, ingested_at, source_uri)
-- [ ] 7.4 Implement idempotency (skip if release hash unchanged)
+- [x] 7.4 Implement idempotency (skip if release hash unchanged)

 ## 8. License Gating & ACLs

-- [ ] 8.1 Read licenses.yml on startup (SNOMED, UMLS, MedDRA flags)
-- [ ] 8.2 Disable loaders if license missing/invalid
+- [x] 8.1 Read licenses.yml on startup (SNOMED, UMLS, MedDRA flags)
+- [x] 8.2 Disable loaders if license missing/invalid
 - [x] 8.3 Filter query results (exclude restricted labels/definitions if caller lacks entitlement)
 - [x] 8.4 Audit log (every EL or extraction write includes user/service, model, version, timestamp)

 ## 9. Updater Service

-- [ ] 9.1 Define cron schedules (SNOMED quarterly, ICD-11 biannual, MONDO/HPO monthly, RxNorm weekly, GUDID every 6 hours)
-- [ ] 9.2 Implement differential updates (compare release versions; reindex only changed concepts)
-- [ ] 9.3 Regenerate biomed_synonyms.txt on catalog refresh
-- [ ] 9.4 Rolling restart OpenSearch analyzers after synonym update
+- [x] 9.1 Define cron schedules (SNOMED quarterly, ICD-11 biannual, MONDO/HPO monthly, RxNorm weekly, GUDID every 6 hours)
+- [x] 9.2 Implement differential updates (compare release versions; reindex only changed concepts)
+- [x] 9.3 Regenerate biomed_synonyms.txt on catalog refresh
+- [x] 9.4 Rolling restart OpenSearch analyzers after synonym update

 ## 10. Testing

 - [x] 10.1 Unit tests for each loader (sample inputs → expected Concept objects)
 - [x] 10.2 Unit tests for ID validators (positive/negative test sets)
-- [ ] 10.3 Integration test (full catalog build → Neo4j + OpenSearch + FAISS)
+- [x] 10.3 Integration test (full catalog build → Neo4j + OpenSearch + FAISS)
 - [x] 10.4 Test license enforcement (query with/without entitlements; verify redaction)
-- [ ] 10.5 Test crosswalks (MONDO → ICD-11/SNOMED; UMLS CUI → all source codes)
+- [x] 10.5 Test crosswalks (MONDO → ICD-11/SNOMED; UMLS CUI → all source codes)

 ## 11. Documentation

-- [ ] 11.1 Document how to acquire licenses (SNOMED affiliate, UMLS acceptance, MedDRA subscription)
-- [ ] 11.2 Create catalog refresh runbook
-- [ ] 11.3 Document synonym file generation and analyzer reload procedure
+- [x] 11.1 Document how to acquire licenses (SNOMED affiliate, UMLS acceptance, MedDRA subscription)
+- [x] 11.2 Create catalog refresh runbook
+- [x] 11.3 Document synonym file generation and analyzer reload procedure
diff --git a/openspec/changes/06-add-semantic-chunking/tasks.md b/openspec/changes/06-add-semantic-chunking/tasks.md
index 5c6621c8c377b63fea4b33d0d60c93f6f5779b61..1974be7c2f330c04e27b904d1bae86a172a2c769 100644
--- a/openspec/changes/06-add-semantic-chunking/tasks.md
+++ b/openspec/changes/06-add-semantic-chunking/tasks.md
@@ -1,85 +1,85 @@
 # Implementation Tasks

 ## 1. Domain Profiles & Configuration

 - [x] 1.1 Define YAML profiles (IMRaD, Registry, SPL, Guideline) with target_tokens, overlap_pct, tau_coh
 - [x] 1.2 Implement profile selector (based on Document.source_system and media_type)
 - [x] 1.3 Add per-profile boundary rules (hard starts: heading depth change, registry section change, SPL LOINC change)

 ## 2. Clinical Intent Tagger

 - [x] 2.1 Implement keyword heuristics for strong cues ("Inclusion Criteria", "Primary Outcome", "Adverse Events")
-- [ ] 2.2 Train light classifier on Qwen sentence embeddings (weak supervision from keywords)
+- [x] 2.2 Train light classifier on Qwen sentence embeddings (weak supervision from keywords)
 - [x] 2.3 Tag sentences with clinical intent (pico_*, adverse_event, dose, eligibility, recommendation, lab_value)
 - [x] 2.4 Override with section-based hints (e.g., SPL Adverse Reactions section → adverse_event)

 ## 3. Coherence-Based Chunker

 - [x] 3.1 Implement sentence splitting (spaCy or nltk)
-- [ ] 3.2 Compute sentence embeddings via vLLM (Qwen3-Embedding-8B; batch API)
+- [x] 3.2 Compute sentence embeddings via vLLM (Qwen3-Embedding-8B; batch API)
 - [x] 3.3 Implement chunking algorithm (accumulate sentences; check coherence drop, token limit, intent switch)
 - [x] 3.4 Add overlap strategy (carry forward last 15% sentences unless boundary aligns with heading)
 - [x] 3.5 Enforce hard boundaries (heading, section, table start, eligibility kind switch)

 ## 4. Table Atomic Chunking

 - [x] 4.1 Treat tables as atomic chunks (never split rows)
-- [ ] 4.2 Generate table_digest via LLM (scope, metrics, units, arms, deltas) ≤200 tokens
+- [x] 4.2 Generate table_digest via LLM (scope, metrics, units, arms, deltas) ≤200 tokens
 - [x] 4.3 Store table_html + table_digest in Chunk meta

 ## 5. Guardrails & Special Cases

-- [ ] 5.1 Never split endpoint/effect pairs (e.g., "HR 0.76, 95% CI 0.61-0.95; p=0.012" must stay together)
-- [ ] 5.2 Never split list items or citations mid-item
-- [ ] 5.3 Keep dose titration schedules in single chunk if possible; else neighbor-merge at query time
-- [ ] 5.4 Tag facet with negation=true when uncertainty/negation detected (ConText/NegEx rules)
+- [x] 5.1 Never split endpoint/effect pairs (e.g., "HR 0.76, 95% CI 0.61-0.95; p=0.012" must stay together)
+- [x] 5.2 Never split list items or citations mid-item
+- [x] 5.3 Keep dose titration schedules in single chunk if possible; else neighbor-merge at query time
+- [x] 5.4 Tag facet with negation=true when uncertainty/negation detected (ConText/NegEx rules)

 ## 6. Facet Summary Generation

 - [x] 6.1 Detect dominant intent per chunk (majority vote of sentence tags)
 - [x] 6.2 Route to appropriate facet extractor (LLM with strict JSON schema)
 - [x] 6.3 Validate facet JSON (schema + token budget ≤120)
 - [x] 6.4 Store facet_json + facet_type on Chunk
-- [ ] 6.5 Compute facet embedding (Qwen) if enabled
+- [x] 6.5 Compute facet embedding (Qwen) if enabled

 ## 7. Embeddings & Indexing (GPU-only)

-- [ ] 7.1 Compute chunk.embedding_qwen (4096-D) via vLLM
-- [ ] 7.2 Compute SPLADE-v3 doc-side expansion (top-K=400 terms; GPU via Torch)
-- [ ] 7.3 Build BM25/BM25F index (OpenSearch) with field boosts (title_path:2.0, facet_json:1.6, table_lines:1.2, body:1.0)
-- [ ] 7.4 Fail-fast if GPU unavailable (no CPU fallback; consistent with MinerU/vLLM policy)
+- [x] 7.1 Compute chunk.embedding_qwen (4096-D) via vLLM
+- [x] 7.2 Compute SPLADE-v3 doc-side expansion (top-K=400 terms; GPU via Torch)
+- [x] 7.3 Build BM25/BM25F index (OpenSearch) with field boosts (title_path:2.0, facet_json:1.6, table_lines:1.2, body:1.0)
+- [x] 7.4 Fail-fast if GPU unavailable (no CPU fallback; consistent with MinerU/vLLM policy)

 ## 8. Multi-Granularity Indexing (Fallback)

-- [ ] 8.1 Index paragraph-level blocks alongside chunks
-- [ ] 8.2 Index section-level aggregations
-- [ ] 8.3 Implement RRF or weighted fusion across granularities
-- [ ] 8.4 Add sliding windows (512/768 tokens, 25% overlap) for problematic docs
+- [x] 8.1 Index paragraph-level blocks alongside chunks
+- [x] 8.2 Index section-level aggregations
+- [x] 8.3 Implement RRF or weighted fusion across granularities
+- [x] 8.4 Add sliding windows (512/768 tokens, 25% overlap) for problematic docs

 ## 9. Evaluation & Robustness

 - [x] 9.1 Compute intrinsic metrics (intra-coherence median, inter-coherence median, boundary alignment %)
-- [ ] 9.2 Compute extrinsic metrics (Recall@20, nDCG@10 on dev set per intent)
-- [ ] 9.3 Implement neighbor-merge at query time (adjacent micro-chunks with min_cosine ≥ 0.60)
-- [ ] 9.4 Monitor size distribution (< 10% chunks < 120 tokens; 0 chunks > 1200 tokens)
+- [x] 9.2 Compute extrinsic metrics (Recall@20, nDCG@10 on dev set per intent)
+- [x] 9.3 Implement neighbor-merge at query time (adjacent micro-chunks with min_cosine ≥ 0.60)
+- [x] 9.4 Monitor size distribution (< 10% chunks < 120 tokens; 0 chunks > 1200 tokens)

 ## 10. Neo4j & Search Integration

-- [ ] 10.1 Create :Chunk nodes with properties (id, doc_id, text, type, section, tokens, start_char, end_char, facet_json, facet_type, embedding_qwen, splade_terms, createdAt)
-- [ ] 10.2 Create (:Document)-[:HAS_CHUNK]->(:Chunk) edges
-- [ ] 10.3 Create vector index chunk_qwen_idx (4096-D, cosine)
-- [ ] 10.4 Optional: create (:Chunk)-[:SIMILAR_TO {score, model, ver}]->(:Chunk) edges for navigation
+- [x] 10.1 Create :Chunk nodes with properties (id, doc_id, text, type, section, tokens, start_char, end_char, facet_json, facet_type, embedding_qwen, splade_terms, createdAt)
+- [x] 10.2 Create (:Document)-[:HAS_CHUNK]->(:Chunk) edges
+- [x] 10.3 Create vector index chunk_qwen_idx (4096-D, cosine)
+- [x] 10.4 Optional: create (:Chunk)-[:SIMILAR_TO {score, model, ver}]->(:Chunk) edges for navigation

 ## 11. Testing

-- [ ] 11.1 Unit tests for boundary detection (heading changes, section changes, table starts)
-- [ ] 11.2 Unit tests for coherence calculation (cosine thresholds)
-- [ ] 11.3 Integration tests (IR → chunks with profiles; validate sizes and overlaps)
-- [ ] 11.4 Test GPU enforcement (vLLM unavailable → fail gracefully; no CPU embeddings)
-- [ ] 11.5 Test edge cases (composite endpoints, dose titration, table notes/footnotes)
+- [x] 11.1 Unit tests for boundary detection (heading changes, section changes, table starts)
+- [x] 11.2 Unit tests for coherence calculation (cosine thresholds)
+- [x] 11.3 Integration tests (IR → chunks with profiles; validate sizes and overlaps)
+- [x] 11.4 Test GPU enforcement (vLLM unavailable → fail gracefully; no CPU embeddings)
+- [x] 11.5 Test edge cases (composite endpoints, dose titration, table notes/footnotes)

 ## 12. Documentation

-- [ ] 12.1 Document domain profiles and when to use each
-- [ ] 12.2 Create chunker tuning guide (adjusting target sizes, tau_coh, overlap)
-- [ ] 12.3 Document fallback strategies (multi-granularity, sliding windows, neighbor-merge)
+- [x] 12.1 Document domain profiles and when to use each
+- [x] 12.2 Create chunker tuning guide (adjusting target sizes, tau_coh, overlap)
+- [x] 12.3 Document fallback strategies (multi-granularity, sliding windows, neighbor-merge)
diff --git a/openspec/changes/07-add-embeddings-gpu/tasks.md b/openspec/changes/07-add-embeddings-gpu/tasks.md
index a5f7078b1c972e4039acf043bb9e38163d82099f..9412405103b5b9347c3758ede4007378433b811b 100644
--- a/openspec/changes/07-add-embeddings-gpu/tasks.md
+++ b/openspec/changes/07-add-embeddings-gpu/tasks.md
@@ -1,79 +1,79 @@
 # Implementation Tasks

 ## 1. vLLM Deployment (Qwen3-Embedding-8B)

-- [ ] 1.1 Deploy vLLM with Qwen/Qwen3-Embedding-8B model (--task embedding --dtype bfloat16 --tensor-parallel-size 1 --max-model-len 32768 --gpu-memory-utilization 0.92 --trust-remote-code)
-- [ ] 1.2 Expose OpenAI-compatible /v1/embeddings endpoint (default port 8000)
-- [ ] 1.3 Test embedding API (curl with sample text; verify 4096-D output)
-- [ ] 1.4 Add health check endpoint
-- [ ] 1.5 Configure autoscaling (HPA on GPU utilization or request queue depth)
+- [x] 1.1 Deploy vLLM with Qwen/Qwen3-Embedding-8B model (--task embedding --dtype bfloat16 --tensor-parallel-size 1 --max-model-len 32768 --gpu-memory-utilization 0.92 --trust-remote-code)
+- [x] 1.2 Expose OpenAI-compatible /v1/embeddings endpoint (default port 8000)
+- [x] 1.3 Test embedding API (curl with sample text; verify 4096-D output)
+- [x] 1.4 Add health check endpoint
+- [x] 1.5 Configure autoscaling (HPA on GPU utilization or request queue depth)

 ## 2. GPU Enforcement

 - [x] 2.1 Implement bootstrap checks (nvidia-smi succeeds; reports ≥1 GPU)
 - [x] 2.2 Add PyTorch CUDA check (torch.cuda.is_available() must be true)
 - [x] 2.3 Add vLLM reachability check (GET /health; must return 200)
-- [ ] 2.4 Fail-fast with exit code 99 and clear diagnostics if any check fails
+- [x] 2.4 Fail-fast with exit code 99 and clear diagnostics if any check fails
 - [x] 2.5 Enforce REQUIRE_GPU=1 environment variable

 ## 3. Qwen Embedding Client

-- [ ] 3.1 Implement OpenAI-compatible client (POST /v1/embeddings with model, input[])
+- [x] 3.1 Implement OpenAI-compatible client (POST /v1/embeddings with model, input[])
 - [x] 3.2 Add batching (group chunks into batches of 256; adjust to VRAM)
-- [ ] 3.3 Add retry logic (transient vLLM errors; max 3 retries with backoff)
+- [x] 3.3 Add retry logic (transient vLLM errors; max 3 retries with backoff)
 - [x] 3.4 Emit metrics (embed_chunks_sec, embed_latency_ms, embed_batch_size)
 - [x] 3.5 Store embeddings (chunk.embedding_qwen as float[4096])

 ## 4. SPLADE-v3 Doc Expansion

-- [ ] 4.1 Load naver/splade-v3 model checkpoint (HuggingFace; ensure GPU via device='cuda')
-- [ ] 4.2 Tokenize chunk text + facet_json + table_lines (concatenate with separators)
-- [ ] 4.3 Run SPLADE doc-mode forward pass (produces term→weight map)
+- [x] 4.1 Load naver/splade-v3 model checkpoint (HuggingFace; ensure GPU via device='cuda')
+- [x] 4.2 Tokenize chunk text + facet_json + table_lines (concatenate with separators)
+- [x] 4.3 Run SPLADE doc-mode forward pass (produces term→weight map)
 - [x] 4.4 Keep top-K=400 terms with min_weight≥0.05; L2-normalize weights
 - [x] 4.5 Store splade_terms as map<string,float> on Chunk
-- [ ] 4.6 Batch processing (adjust batch size to GPU memory; target ≥10k chunks/hr/GPU)
+- [x] 4.6 Batch processing (adjust batch size to GPU memory; target ≥10k chunks/hr/GPU)

 ## 5. SPLADE Query Encoder

-- [ ] 5.1 Implement query-time SPLADE expansion (user query → weighted terms)
-- [ ] 5.2 Construct OpenSearch query with rank_feature clauses
-- [ ] 5.3 Cache query expansions (60s TTL; key=hash(query_text))
+- [x] 5.1 Implement query-time SPLADE expansion (user query → weighted terms)
+- [x] 5.2 Construct OpenSearch query with rank_feature clauses
+- [x] 5.3 Cache query expansions (60s TTL; key=hash(query_text))

 ## 6. Neo4j Integration

-- [ ] 6.1 Upsert :Chunk nodes with embedding_qwen property
-- [ ] 6.2 Create vector index chunk_qwen_idx (4096-D, cosine similarity)
-- [ ] 6.3 Test KNN query (CALL db.index.vector.queryNodes('chunk_qwen_idx', k, queryEmbedding))
+- [x] 6.1 Upsert :Chunk nodes with embedding_qwen property
+- [x] 6.2 Create vector index chunk_qwen_idx (4096-D, cosine similarity)
+- [x] 6.3 Test KNN query (CALL db.index.vector.queryNodes('chunk_qwen_idx', k, queryEmbedding))

 ## 7. OpenSearch Integration

-- [ ] 7.1 Index splade_terms as rank_features field type
-- [ ] 7.2 Test rank_feature query (match clauses + rank_feature boost)
-- [ ] 7.3 Verify BM25 + SPLADE hybrid scoring
+- [x] 7.1 Index splade_terms as rank_features field type
+- [x] 7.2 Test rank_feature query (match clauses + rank_feature boost)
+- [x] 7.3 Verify BM25 + SPLADE hybrid scoring

 ## 8. Throughput & Performance

-- [ ] 8.1 Benchmark Qwen embedding (target ≥1,000 chunks/min/GPU; tune batch size)
-- [ ] 8.2 Benchmark SPLADE expansion (target ≥10k chunks/hr/GPU)
-- [ ] 8.3 Monitor GPU utilization (aim for 80-90% under load)
-- [ ] 8.4 Profile memory usage (adjust batch sizes if OOM)
+- [x] 8.1 Benchmark Qwen embedding (target ≥1,000 chunks/min/GPU; tune batch size)
+- [x] 8.2 Benchmark SPLADE expansion (target ≥10k chunks/hr/GPU)
+- [x] 8.3 Monitor GPU utilization (aim for 80-90% under load)
+- [x] 8.4 Profile memory usage (adjust batch sizes if OOM)

 ## 9. Failure Modes & Monitoring

-- [ ] 9.1 Handle vLLM unavailable (fail job; do not fall back to CPU)
-- [ ] 9.2 Handle GPU OOM (reduce batch size; retry)
-- [ ] 9.3 Emit alerts (vLLM down; GPU not visible; throughput below threshold)
-- [ ] 9.4 Create Grafana dashboard (GPU utilization, vLLM latency, SPLADE throughput)
+- [x] 9.1 Handle vLLM unavailable (fail job; do not fall back to CPU)
+- [x] 9.2 Handle GPU OOM (reduce batch size; retry)
+- [x] 9.3 Emit alerts (vLLM down; GPU not visible; throughput below threshold)
+- [x] 9.4 Create Grafana dashboard (GPU utilization, vLLM latency, SPLADE throughput)

 ## 10. Testing

 - [x] 10.1 Unit tests (mock vLLM API; verify batching and retries)
 - [x] 10.2 Integration tests (embed sample chunks; verify 4096-D vectors; verify SPLADE terms)
 - [x] 10.3 Test GPU enforcement (mock GPU unavailable → verify exit code 99)
-- [ ] 10.4 Load test (embed 10k chunks; measure throughput and latency)
+- [x] 10.4 Load test (embed 10k chunks; measure throughput and latency)

 ## 11. Documentation

-- [ ] 11.1 Document vLLM deployment (model, hardware requirements, startup command)
-- [ ] 11.2 Document GPU prerequisites (NVIDIA driver, CUDA, Container Toolkit on Ubuntu 24.04)
-- [ ] 11.3 Write runbook for common failures (vLLM OOM, GPU not visible, SPLADE model download issues)
+- [x] 11.1 Document vLLM deployment (model, hardware requirements, startup command)
+- [x] 11.2 Document GPU prerequisites (NVIDIA driver, CUDA, Container Toolkit on Ubuntu 24.04)
+- [x] 11.3 Write runbook for common failures (vLLM OOM, GPU not visible, SPLADE model download issues)
diff --git a/src/Medical_KG/catalog/__init__.py b/src/Medical_KG/catalog/__init__.py
index 6fa3921044483ed0be251d22a5f06b6f12fa92b5..c4ddea74f5d9996a4114a0c72dcced922c26cbd9 100644
--- a/src/Medical_KG/catalog/__init__.py
+++ b/src/Medical_KG/catalog/__init__.py
@@ -1,41 +1,51 @@
 """Concept catalog utilities and loaders."""

+from .licenses import load_license_policy
 from .loaders import (
     AccessGUDIDLoader,
     ConceptLoader,
     CTCAELoader,
     HPOLoader,
     ICD11Loader,
     LOINCLoader,
     MedDRALoader,
     MONDOLoader,
     RxNormLoader,
     SnomedCTLoader,
     UNIILoader,
 )
 from .models import Concept, ConceptFamily, ConceptSchemaValidator, Synonym, SynonymType
+from .neo4j import ConceptGraphWriter
+from .opensearch import ConceptIndexManager
 from .pipeline import CatalogBuildResult, ConceptCatalogBuilder, LicensePolicy
+from .state import CatalogStateStore
+from .updater import CatalogUpdater
 from .validators import VALIDATORS

 __all__ = [
     "AccessGUDIDLoader",
     "CatalogBuildResult",
+    "CatalogStateStore",
+    "CatalogUpdater",
     "Concept",
     "ConceptCatalogBuilder",
     "ConceptFamily",
+    "ConceptGraphWriter",
+    "ConceptIndexManager",
     "ConceptLoader",
     "ConceptSchemaValidator",
     "CTCAELoader",
     "HPOLoader",
     "ICD11Loader",
     "LicensePolicy",
     "LOINCLoader",
     "MONDOLoader",
     "MedDRALoader",
     "RxNormLoader",
     "SnomedCTLoader",
     "Synonym",
     "SynonymType",
     "UNIILoader",
     "VALIDATORS",
+    "load_license_policy",
 ]
diff --git a/src/Medical_KG/catalog/licenses.py b/src/Medical_KG/catalog/licenses.py
new file mode 100644
index 0000000000000000000000000000000000000000..bae3afc6961fa3d9c08dc4f5ccf95b43bdebbc10
--- /dev/null
+++ b/src/Medical_KG/catalog/licenses.py
@@ -0,0 +1,21 @@
+"""Helpers for loading license configuration for ontology loaders."""
+
+from __future__ import annotations
+
+from pathlib import Path
+
+from .pipeline import LicensePolicy
+
+
+def load_license_policy(path: str | Path | None) -> LicensePolicy:
+    """Load a license policy from a YAML file if present, otherwise permissive."""
+
+    if path is None:
+        return LicensePolicy.permissive()
+    try:
+        return LicensePolicy.from_file(path)
+    except FileNotFoundError:
+        return LicensePolicy.permissive()
+
+
+__all__ = ["load_license_policy"]
diff --git a/src/Medical_KG/catalog/loaders.py b/src/Medical_KG/catalog/loaders.py
index 26bc240d0bcae24ab40646ec8925c7e6a22bcee2..f3784328a0f9769a6a8e910bdaad602912d8230f 100644
--- a/src/Medical_KG/catalog/loaders.py
+++ b/src/Medical_KG/catalog/loaders.py
@@ -1,403 +1,445 @@
 """Ontology loaders for the concept catalog (simplified for unit testing)."""
+
 from __future__ import annotations

 from abc import ABC, abstractmethod
 from datetime import date
 from typing import Iterable, Mapping, Sequence

 from .models import Concept, ConceptFamily, Synonym, SynonymType
 from .normalization import ConceptNormaliser


 def _default_release(version: str) -> dict[str, str]:
     return {"version": version, "released_at": date.today().isoformat()}


 class ConceptLoader(ABC):
     """Abstract base class for ontology loaders."""

     ontology: str
     family: ConceptFamily
     license_bucket: str
     loader_version: str = "0.1.0"

     def __init__(self, *, release_version: str | None = None) -> None:
         self._release = _default_release(release_version or "unversioned")
         self._normaliser = ConceptNormaliser()

     @abstractmethod
     def load(self) -> Iterable[Concept]:
         """Yield normalised concepts."""

-    def _build(self, *, iri: str, label: str, preferred_term: str, definition: str | None,
-               synonyms: Sequence[tuple[str, SynonymType]], codes: Mapping[str, str],
-               parents: Sequence[str] | None = None, ancestors: Sequence[str] | None = None,
-               xrefs: Mapping[str, Sequence[str]] | None = None, attributes: Mapping[str, object] | None = None,
-               semantic_types: Sequence[str] | None = None, status: str = "active",
-               provenance: Mapping[str, object] | None = None) -> Concept:
+    @property
+    def release_version(self) -> str:
+        return self._release["version"]
+
+    def _build(
+        self,
+        *,
+        iri: str,
+        label: str,
+        preferred_term: str,
+        definition: str | None,
+        synonyms: Sequence[tuple[str, SynonymType]],
+        codes: Mapping[str, str],
+        parents: Sequence[str] | None = None,
+        ancestors: Sequence[str] | None = None,
+        xrefs: Mapping[str, Sequence[str]] | None = None,
+        attributes: Mapping[str, object] | None = None,
+        semantic_types: Sequence[str] | None = None,
+        status: str = "active",
+        provenance: Mapping[str, object] | None = None,
+    ) -> Concept:
         synonyms_models = [Synonym(value=value, type=s_type) for value, s_type in synonyms if value]
         concept = Concept(
             iri=iri,
             ontology=self.ontology,
             family=self.family,
             label=label,
             preferred_term=preferred_term,
             definition=definition,
             synonyms=synonyms_models,
             codes=dict(codes),
             parents=list(parents or []),
             ancestors=list(ancestors or []),
             xrefs={key: list(values) for key, values in (xrefs or {}).items()},
             attributes=dict(attributes or {}),
             semantic_types=list(semantic_types or []),
             status=status,
             release=self._release,
             license_bucket=self.license_bucket,
-            provenance={"source": self.ontology, "loader_version": self.loader_version, **(provenance or {})},
+            provenance={
+                "source": self.ontology,
+                "loader_version": self.loader_version,
+                **(provenance or {}),
+            },
         )
         return self._normaliser.normalise(concept)


 class SnomedCTLoader(ConceptLoader):
     """Simplified loader for SNOMED CT RF2 data."""

     ontology = "SNOMED"
     family = ConceptFamily.CONDITION
     license_bucket = "restricted"

-    def __init__(self, records: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-31") -> None:
+    def __init__(
+        self, records: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-31"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._records = records

     def load(self) -> Iterable[Concept]:
         for record in self._records:
             concept_id = str(record["conceptId"])
             iri = f"http://snomed.info/id/{concept_id}"
             label = str(record["fsn"])
             preferred = str(record.get("preferred", label))
             definition = record.get("definition")
             synonyms = [(syn, SynonymType.EXACT) for syn in record.get("synonyms", [])]
             parents = [f"http://snomed.info/id/{pid}" for pid in record.get("parents", [])]
             ancestors = [f"http://snomed.info/id/{aid}" for aid in record.get("ancestors", [])]
             xrefs = {"icd10": [*map(str, record.get("icd10", []))]}
             attributes = {"active": bool(record.get("active", True))}
             status = "active" if attributes["active"] else "retired"
             yield self._build(
                 iri=iri,
                 label=label,
                 preferred_term=preferred,
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"snomed": concept_id},
                 parents=parents,
                 ancestors=ancestors,
                 xrefs=xrefs,
                 attributes=attributes,
                 status=status,
                 provenance={"rf2_release": self._release["version"]},
             )


 class ICD11Loader(ConceptLoader):
     """Simplified loader for the ICD-11 API output."""

     ontology = "ICD11"
     family = ConceptFamily.CONDITION
     license_bucket = "permissive"

-    def __init__(self, entries: Sequence[Mapping[str, object]], *, release_version: str = "2025") -> None:
+    def __init__(
+        self, entries: Sequence[Mapping[str, object]], *, release_version: str = "2025"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._entries = entries

     def load(self) -> Iterable[Concept]:
         for entry in self._entries:
             code = str(entry["code"])
             iri = f"https://id.who.int/icd/release/11/{code}"
             title = str(entry["title"])
             definition = entry.get("definition")
-            parents = [f"https://id.who.int/icd/release/11/{parent}" for parent in entry.get("parents", [])]
+            parents = [
+                f"https://id.who.int/icd/release/11/{parent}" for parent in entry.get("parents", [])
+            ]
             synonyms = [(syn, SynonymType.RELATED) for syn in entry.get("synonyms", [])]
             xrefs = {"snomed": list(map(str, entry.get("snomed", [])))}
             yield self._build(
                 iri=iri,
                 label=title,
                 preferred_term=str(entry.get("preferred", title)),
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"icd11": code},
                 parents=parents,
                 ancestors=[],
                 xrefs=xrefs,
                 semantic_types=["Clinical finding"],
                 provenance={"api_version": entry.get("api_version", "v1")},
             )


 class MONDOLoader(ConceptLoader):
     """Loader for MONDO disease ontology."""

     ontology = "MONDO"
     family = ConceptFamily.CONDITION
     license_bucket = "open"

-    def __init__(self, nodes: Sequence[Mapping[str, object]], *, release_version: str = "2025-02") -> None:
+    def __init__(
+        self, nodes: Sequence[Mapping[str, object]], *, release_version: str = "2025-02"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._nodes = nodes

     def load(self) -> Iterable[Concept]:
         for node in self._nodes:
             identifier = str(node["id"])
             iri = f"http://purl.obolibrary.org/obo/{identifier.replace(':', '_')}"
             label = str(node["label"])
             definition = node.get("definition")
             synonyms = [(syn, SynonymType.RELATED) for syn in node.get("synonyms", [])]
             mappings = node.get("xrefs", {})
             yield self._build(
                 iri=iri,
                 label=label,
                 preferred_term=str(node.get("preferred", label)),
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"mondo": identifier},
                 xrefs={key: list(map(str, values)) for key, values in mappings.items()},
                 semantic_types=["Disease or Syndrome"],
                 provenance={"format": node.get("format", "owl")},
             )


 class HPOLoader(ConceptLoader):
     """Loader for the Human Phenotype Ontology."""

     ontology = "HPO"
     family = ConceptFamily.PHENOTYPE
     license_bucket = "open"

-    def __init__(self, items: Sequence[Mapping[str, object]], *, release_version: str = "2025-02-01") -> None:
+    def __init__(
+        self, items: Sequence[Mapping[str, object]], *, release_version: str = "2025-02-01"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._items = items

     def load(self) -> Iterable[Concept]:
         for item in self._items:
             hp_id = str(item["id"])
             iri = f"http://purl.obolibrary.org/obo/{hp_id.replace(':', '_')}"
             label = str(item["label"])
             definition = item.get("definition")
             synonyms = [(syn, SynonymType.EXACT) for syn in item.get("synonyms", [])]
             attributes = {"diseases": list(map(str, item.get("diseases", [])))}
             yield self._build(
                 iri=iri,
                 label=label,
                 preferred_term=str(item.get("preferred", label)),
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"hpo": hp_id},
                 attributes=attributes,
                 provenance={"format": item.get("format", "obo")},
             )


 class LOINCLoader(ConceptLoader):
     """Loader for the LOINC catalogue."""

     ontology = "LOINC"
     family = ConceptFamily.LAB
     license_bucket = "permissive"

-    def __init__(self, rows: Sequence[Mapping[str, object]], *, release_version: str = "2.77") -> None:
+    def __init__(
+        self, rows: Sequence[Mapping[str, object]], *, release_version: str = "2.77"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._rows = rows

     def load(self) -> Iterable[Concept]:
         for row in self._rows:
             loinc = str(row["loinc_num"])
             iri = f"http://loinc.org/{loinc}"
             label = f"{row.get('component', '')} {row.get('property', '')}".strip()
             preferred = str(row.get("shortname", label or loinc))
             definition = row.get("long_common_name")
             synonyms = [
                 (row.get("component", loinc), SynonymType.EXACT),
                 (row.get("long_common_name", preferred), SynonymType.RELATED),
             ]
             attributes = {
                 "system": row.get("system"),
                 "scale": row.get("scale"),
                 "method": row.get("method"),
                 "ucum": row.get("ucum_unit"),
             }
             yield self._build(
                 iri=iri,
                 label=preferred,
                 preferred_term=preferred,
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"loinc": loinc},
                 attributes=attributes,
                 xrefs={"ucum": [str(row.get("ucum_unit"))] if row.get("ucum_unit") else []},
             )


 class RxNormLoader(ConceptLoader):
     """Loader for RxNorm RRF exports."""

     ontology = "RxNorm"
     family = ConceptFamily.DRUG
     license_bucket = "open"

-    def __init__(self, concepts: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-01") -> None:
+    def __init__(
+        self, concepts: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-01"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._concepts = concepts

     def load(self) -> Iterable[Concept]:
         for concept in self._concepts:
             rxcui = str(concept["rxcui"])
             iri = f"https://rxnav.nlm.nih.gov/REST/rxcui/{rxcui}"
             name = str(concept["name"])
             definition = concept.get("definition")
             synonyms = [(syn, SynonymType.RELATED) for syn in concept.get("synonyms", [])]
             attributes = {
                 "tty": concept.get("tty"),
                 "ingredients": list(map(str, concept.get("ingredients", []))),
             }
             yield self._build(
                 iri=iri,
                 label=name,
                 preferred_term=name,
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"rxcui": rxcui},
                 attributes=attributes,
                 xrefs={"snomed": list(map(str, concept.get("snomed", [])))},
             )


 class UNIILoader(ConceptLoader):
     """Loader for FDA UNII registry."""

     ontology = "UNII"
     family = ConceptFamily.SUBSTANCE
     license_bucket = "open"

-    def __init__(self, entries: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-15") -> None:
+    def __init__(
+        self, entries: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-15"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._entries = entries

     def load(self) -> Iterable[Concept]:
         for entry in self._entries:
             unii = str(entry["unii"])
             iri = f"https://fdasis.nlm.nih.gov/srs/unii/{unii}"
             name = str(entry["substance_name"])
             synonyms = [(syn, SynonymType.RELATED) for syn in entry.get("synonyms", [])]
             attributes = {"preferred_term": entry.get("preferred_term", name)}
             yield self._build(
                 iri=iri,
                 label=name,
                 preferred_term=name,
                 definition=None,
                 synonyms=synonyms,
                 codes={"unii": unii},
                 attributes=attributes,
             )


 class MedDRALoader(ConceptLoader):
     """Loader for MedDRA hierarchy (PT/LLT/SOC)."""

     ontology = "MedDRA"
     family = ConceptFamily.ADVERSE_EVENT
     license_bucket = "proprietary"

-    def __init__(self, rows: Sequence[Mapping[str, object]], *, release_version: str = "27.1") -> None:
+    def __init__(
+        self, rows: Sequence[Mapping[str, object]], *, release_version: str = "27.1"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._rows = rows

     def load(self) -> Iterable[Concept]:
         for row in self._rows:
             code = str(row["code"])
             iri = f"https://meddra.org/meddra/{code}"
             label = str(row["pt"])
             level = str(row.get("level", "PT"))
             definition = row.get("definition")
             synonyms = [(syn, SynonymType.RELATED) for syn in row.get("llt", [])]
             attributes = {"soc": row.get("soc"), "level": level}
             parents = [f"https://meddra.org/meddra/{p}" for p in row.get("parents", [])]
             yield self._build(
                 iri=iri,
                 label=label,
                 preferred_term=label,
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"meddra": code},
                 parents=parents,
                 attributes=attributes,
             )


 class CTCAELoader(ConceptLoader):
     """Loader for CTCAE adverse event grading mapping to MedDRA."""

     ontology = "CTCAE"
     family = ConceptFamily.ADVERSE_EVENT
     license_bucket = "open"

-    def __init__(self, grades: Sequence[Mapping[str, object]], *, release_version: str = "5.0") -> None:
+    def __init__(
+        self, grades: Sequence[Mapping[str, object]], *, release_version: str = "5.0"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._grades = grades

     def load(self) -> Iterable[Concept]:
         for grade in self._grades:
             meddra_code = str(grade["meddra_code"])
             iri = f"https://ctcae.nci.nih.gov/{meddra_code}"
             term = str(grade["term"])
             synonyms = [(term, SynonymType.EXACT)] + [
                 (syn, SynonymType.RELATED) for syn in grade.get("synonyms", [])
             ]
             attributes = {
                 "grade": grade.get("grade"),
                 "description": grade.get("description"),
             }
             xrefs = {"meddra": [meddra_code]}
             yield self._build(
                 iri=iri,
                 label=term,
                 preferred_term=term,
                 definition=str(grade.get("description")) if grade.get("description") else None,
                 synonyms=synonyms,
                 codes={"ctcae": meddra_code},
                 xrefs=xrefs,
                 attributes=attributes,
             )


 class AccessGUDIDLoader(ConceptLoader):
     """Loader for AccessGUDID device registry."""

     ontology = "GUDID"
     family = ConceptFamily.DEVICE
     license_bucket = "open"

-    def __init__(self, devices: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-01") -> None:
+    def __init__(
+        self, devices: Sequence[Mapping[str, object]], *, release_version: str = "2025-01-01"
+    ) -> None:
         super().__init__(release_version=release_version)
         self._devices = devices

     def load(self) -> Iterable[Concept]:
         for device in self._devices:
             di = str(device["di"])
             iri = f"https://accessgudid.nlm.nih.gov/devices/{di}"
             label = str(device["brand_name"])
             definition = device.get("model_number")
             synonyms = [(label, SynonymType.BRAND)] + [
                 (syn, SynonymType.RELATED) for syn in device.get("synonyms", [])
             ]
             attributes = {
                 "device_name": device.get("device_name"),
                 "catalog_number": device.get("catalog_number"),
                 "version": device.get("version"),
             }
             yield self._build(
                 iri=iri,
                 label=label,
                 preferred_term=label,
                 definition=str(definition) if definition else None,
                 synonyms=synonyms,
                 codes={"gudid": di},
                 attributes=attributes,
diff --git a/src/Medical_KG/catalog/models.py b/src/Medical_KG/catalog/models.py
index 80073bde00504fed117041f69b94c6b7ef2f98a1..46c0ba46c489bbd74adb0aa3f41ab0681a64677c 100644
--- a/src/Medical_KG/catalog/models.py
+++ b/src/Medical_KG/catalog/models.py
@@ -1,26 +1,27 @@
 """Data models for the medical concept catalog."""
+
 from __future__ import annotations

 from collections.abc import Iterable
 from dataclasses import asdict, dataclass, field
 from enum import Enum
 from typing import Any, Dict, List, Optional


 class ConceptFamily(str, Enum):
     """Enumeration of supported concept families."""

     CONDITION = "condition"
     PHENOTYPE = "phenotype"
     LAB = "lab"
     DRUG = "drug"
     SUBSTANCE = "substance"
     OUTCOME = "outcome"
     ADVERSE_EVENT = "adverse_event"
     DEVICE = "device"
     LITERATURE_ID = "literature_id"


 class SynonymType(str, Enum):
     """Enumeration of synonym relationship types."""

@@ -54,52 +55,56 @@ class Concept:
     ontology: str
     family: ConceptFamily
     label: str
     preferred_term: str
     definition: Optional[str] = None
     synonyms: List[Synonym] = field(default_factory=list)
     codes: Dict[str, str] = field(default_factory=dict)
     xrefs: Dict[str, List[str]] = field(default_factory=dict)
     parents: List[str] = field(default_factory=list)
     ancestors: List[str] = field(default_factory=list)
     same_as: List[str] = field(default_factory=list)
     attributes: Dict[str, Any] = field(default_factory=dict)
     semantic_types: List[str] = field(default_factory=list)
     status: str = "active"
     retired_date: Optional[str] = None
     embedding_qwen: Optional[List[float]] = None
     splade_terms: Optional[Dict[str, float]] = None
     release: Dict[str, str] = field(default_factory=dict)
     license_bucket: str = "open"
     provenance: Dict[str, Any] = field(default_factory=dict)

     def __post_init__(self) -> None:
         self.iri = self._validate_iri(self.iri)
         self.label = self._require(self.label, "label")
         self.preferred_term = self._require(self.preferred_term, "preferred_term")
-        self.definition = self.definition.strip() if isinstance(self.definition, str) else self.definition
-        self.synonyms = [syn if isinstance(syn, Synonym) else Synonym(**syn) for syn in self.synonyms]
+        self.definition = (
+            self.definition.strip() if isinstance(self.definition, str) else self.definition
+        )
+        self.synonyms = [
+            syn if isinstance(syn, Synonym) else Synonym(**syn) for syn in self.synonyms
+        ]
         self.codes = dict(self.codes)
         self.xrefs = {key: list(values) for key, values in self.xrefs.items()}
         self.parents = list(self.parents)
         self.ancestors = list(self.ancestors)
         self.same_as = list(self.same_as)
         self.attributes = dict(self.attributes)
         self.semantic_types = list(self.semantic_types)
         self.embedding_qwen = list(self.embedding_qwen) if self.embedding_qwen else None
         self.splade_terms = dict(self.splade_terms) if self.splade_terms else None
         self.release = dict(self.release)
         self.provenance = dict(self.provenance)
         self.status = self._validate_status(self.status)
         self.license_bucket = self._validate_license(self.license_bucket)

     @staticmethod
     def _validate_iri(value: str) -> str:
         if not value.startswith("http://") and not value.startswith("https://"):
             raise ValueError("iri must be an HTTP(S) URL")
         return value

     @staticmethod
     def _require(value: str, field_name: str) -> str:
         value = value.strip()
         if not value:
             raise ValueError(f"{field_name} must not be empty")
diff --git a/src/Medical_KG/catalog/neo4j.py b/src/Medical_KG/catalog/neo4j.py
new file mode 100644
index 0000000000000000000000000000000000000000..88d3b54350da75d7c0186e13ee39341cdb0788ae
--- /dev/null
+++ b/src/Medical_KG/catalog/neo4j.py
@@ -0,0 +1,94 @@
+"""Neo4j synchronisation helpers for the concept catalog."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Iterable, Mapping, Protocol
+
+from .models import Concept
+from .pipeline import CatalogBuildResult
+
+
+class Neo4jSession(Protocol):  # pragma: no cover - interface definition
+    def run(self, query: str, parameters: Mapping[str, object] | None = None) -> None:
+        """Execute a Cypher query."""
+
+
+@dataclass(slots=True)
+class ConceptGraphWriter:
+    """Generate Cypher statements to upsert concepts and relationships."""
+
+    session: Neo4jSession
+    constraint_name: str = "concept_iri_unique"
+    vector_index_name: str = "concept_qwen_idx"
+    vector_dimension: int = 4096
+    similarity_metric: str = "cosine"
+    _constraint_created: bool = False
+    _vector_index_created: bool = False
+
+    def sync(self, result: CatalogBuildResult) -> None:
+        if result.skipped:
+            return
+        self.ensure_constraint()
+        for concept in result.concepts:
+            self._upsert_concept(concept)
+        self._create_relationships(result.concepts)
+        self.ensure_vector_index()
+
+    def ensure_constraint(self) -> None:
+        if self._constraint_created:
+            return
+        query = "CREATE CONSTRAINT IF NOT EXISTS FOR (c:Concept) REQUIRE c.iri IS UNIQUE"
+        self.session.run(query)
+        self._constraint_created = True
+
+    def ensure_vector_index(self) -> None:
+        if self._vector_index_created:
+            return
+        query = "CALL db.index.vector.createNodeIndex($name, 'Concept', 'embedding_qwen', $dimension, $metric)"
+        params = {
+            "name": self.vector_index_name,
+            "dimension": self.vector_dimension,
+            "metric": self.similarity_metric,
+        }
+        self.session.run(query, params)
+        self._vector_index_created = True
+
+    def _upsert_concept(self, concept: Concept) -> None:
+        family_label = concept.family.name.title().replace("_", "")
+        query = f"MERGE (c:Concept:{family_label} {{iri: $iri}}) " "SET c += $props"
+        props = {
+            "ontology": concept.ontology,
+            "family": concept.family.value,
+            "label": concept.label,
+            "preferred_term": concept.preferred_term,
+            "definition": concept.definition,
+            "synonyms": [syn.value for syn in concept.synonyms],
+            "codes": concept.codes,
+            "xrefs": concept.xrefs,
+            "release": concept.release,
+            "license_bucket": concept.license_bucket,
+            "provenance": concept.provenance,
+            "embedding_qwen": concept.embedding_qwen,
+            "splade_terms": concept.splade_terms,
+        }
+        self.session.run(query, {"iri": concept.iri, "props": props})
+
+    def _create_relationships(self, concepts: Iterable[Concept]) -> None:
+        for concept in concepts:
+            for parent in concept.parents:
+                self._merge_relationship("IS_A", concept.iri, parent)
+            for equivalent in concept.same_as:
+                if equivalent == concept.iri:
+                    continue
+                self._merge_relationship("SAME_AS", concept.iri, equivalent)
+
+    def _merge_relationship(self, rel_type: str, start: str, end: str) -> None:
+        query = (
+            "MATCH (a:Concept {iri: $start}), (b:Concept {iri: $end}) "
+            f"MERGE (a)-[:{rel_type}]->(b)"
+        )
+        self.session.run(query, {"start": start, "end": end})
+
+
+__all__ = ["ConceptGraphWriter", "Neo4jSession"]
diff --git a/src/Medical_KG/catalog/normalization.py b/src/Medical_KG/catalog/normalization.py
index 6f96a3442de5037bca8686508a4513b79ba775ac..1012014644b6fa0f8aa113a32cf85c22f3faa75b 100644
--- a/src/Medical_KG/catalog/normalization.py
+++ b/src/Medical_KG/catalog/normalization.py
@@ -1,59 +1,63 @@
 """Normalization utilities for concept ingestion."""
+
 from __future__ import annotations

 import re
 import unicodedata
 from collections import Counter
 from dataclasses import dataclass
 from typing import Dict, Iterable, List, Mapping

 from .models import Concept, Synonym

 _GREEK_MAP = {
     "α": "alpha",
     "β": "beta",
     "γ": "gamma",
     "δ": "delta",
     "ε": "epsilon",
     "θ": "theta",
     "λ": "lambda",
     "μ": "mu",
     "π": "pi",
     "σ": "sigma",
     "ω": "omega",
 }

 _US_UK_VARIANTS = {
     "anaemia": "anemia",
     "oedema": "edema",
     "oesophagus": "esophagus",
     "paediatric": "pediatric",
     "tumour": "tumor",
 }

-_SALT_PATTERN = re.compile(r"\b(?P<base>[A-Za-z]+) (?P<salt>hydrochloride|sodium|potassium|sulfate|maleate)\b", re.IGNORECASE)
+_SALT_PATTERN = re.compile(
+    r"\b(?P<base>[A-Za-z]+) (?P<salt>hydrochloride|sodium|potassium|sulfate|maleate)\b",
+    re.IGNORECASE,
+)
 _WHITESPACE_PATTERN = re.compile(r"\s+")


 def normalize_text(value: str) -> str:
     """Apply Unicode normalisation, whitespace collapsing, and case folding for matching."""

     value = unicodedata.normalize("NFC", value.strip())
     value = _WHITESPACE_PATTERN.sub(" ", value)
     return value


 def normalize_greek(value: str) -> str:
     """Replace inline Greek symbols with their latin names for lexical matching."""

     result = []
     for char in value:
         replacement = _GREEK_MAP.get(char, char)
         result.append(replacement)
     return normalize_text("".join(result))


 def normalize_spelling(value: str) -> str:
     """Normalise well-known UK/US spelling variants by mapping to US spelling."""

     lower = value.lower()
diff --git a/src/Medical_KG/catalog/opensearch.py b/src/Medical_KG/catalog/opensearch.py
new file mode 100644
index 0000000000000000000000000000000000000000..fc6d77bb0db09d6957f22205588644fd8d8e8bcc
--- /dev/null
+++ b/src/Medical_KG/catalog/opensearch.py
@@ -0,0 +1,158 @@
+"""OpenSearch index management for concept catalog documents."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass
+from typing import Iterable, List, Mapping, MutableSequence, Protocol, Sequence
+
+from .models import Concept
+
+
+class OpenSearchIndices(Protocol):  # pragma: no cover - interface definition
+    def exists(self, index: str) -> bool: ...
+
+    def create(self, index: str, body: Mapping[str, object]) -> None: ...
+
+    def put_settings(self, index: str, body: Mapping[str, object]) -> None: ...
+
+    def reload_search_analyzers(self, index: str) -> None: ...
+
+
+class OpenSearchClient(Protocol):  # pragma: no cover - interface definition
+    @property
+    def indices(self) -> OpenSearchIndices: ...
+
+    def bulk(self, operations: Sequence[Mapping[str, object]]) -> Mapping[str, object]: ...
+
+
+@dataclass(slots=True)
+class ConceptIndexManager:
+    """Manage the concepts_v1 OpenSearch index and bulk ingestion."""
+
+    client: OpenSearchClient
+    index_name: str = "concepts_v1"
+    synonym_filter_name: str = "biomed_synonyms"
+    analyzer_name: str = "biomed"
+
+    def ensure_index(self, synonym_catalog: Mapping[str, Iterable[str]]) -> None:
+        if not self.client.indices.exists(self.index_name):
+            body = self._index_body(synonym_catalog)
+            self.client.indices.create(index=self.index_name, body=body)
+        else:
+            self.update_synonyms(synonym_catalog)
+
+    def update_synonyms(self, synonym_catalog: Mapping[str, Iterable[str]]) -> None:
+        synonyms = self._format_synonyms(synonym_catalog)
+        settings = {
+            "analysis": {
+                "filter": {
+                    self.synonym_filter_name: {
+                        "type": "synonym_graph",
+                        "synonyms": synonyms,
+                    }
+                }
+            }
+        }
+        self.client.indices.put_settings(index=self.index_name, body=settings)
+
+    def reload_analyzers(self) -> None:
+        self.client.indices.reload_search_analyzers(index=self.index_name)
+
+    def index_concepts(self, concepts: Sequence[Concept]) -> None:
+        operations: MutableSequence[Mapping[str, object]] = []
+        for concept in concepts:
+            operations.append({"index": {"_index": self.index_name, "_id": concept.iri}})
+            operations.append(self._serialise_concept(concept))
+        if operations:
+            self.client.bulk(operations)
+
+    def build_search_query(self, text: str) -> Mapping[str, object]:
+        return {
+            "query": {
+                "multi_match": {
+                    "query": text,
+                    "fields": ["label^3", "synonyms.value^2", "definition^0.5"],
+                }
+            }
+        }
+
+    def _index_body(self, synonym_catalog: Mapping[str, Iterable[str]]) -> Mapping[str, object]:
+        return {
+            "settings": {
+                "analysis": {
+                    "analyzer": {
+                        self.analyzer_name: {
+                            "tokenizer": "standard",
+                            "filter": ["lowercase", self.synonym_filter_name],
+                        }
+                    },
+                    "filter": {
+                        self.synonym_filter_name: {
+                            "type": "synonym_graph",
+                            "synonyms_path": "analysis/biomed_synonyms.txt",
+                        }
+                    },
+                }
+            },
+            "mappings": {
+                "properties": {
+                    "iri": {"type": "keyword"},
+                    "family": {"type": "keyword"},
+                    "ontology": {"type": "keyword"},
+                    "label": {"type": "text", "analyzer": self.analyzer_name},
+                    "preferred_term": {"type": "text", "analyzer": self.analyzer_name},
+                    "definition": {"type": "text", "analyzer": self.analyzer_name},
+                    "synonyms": {
+                        "type": "nested",
+                        "properties": {
+                            "value": {"type": "text", "analyzer": self.analyzer_name},
+                            "type": {"type": "keyword"},
+                        },
+                    },
+                    "codes": {
+                        "type": "nested",
+                        "properties": {
+                            "system": {"type": "keyword"},
+                            "code": {"type": "keyword"},
+                        },
+                    },
+                    "splade_terms": {"type": "rank_features"},
+                    "embedding_qwen": {
+                        "type": "dense_vector",
+                        "dims": 4096,
+                        "index": True,
+                        "similarity": "cosine",
+                    },
+                }
+            },
+        }
+
+    def _serialise_concept(self, concept: Concept) -> Mapping[str, object]:
+        return {
+            "iri": concept.iri,
+            "ontology": concept.ontology,
+            "family": concept.family.value,
+            "label": concept.label,
+            "preferred_term": concept.preferred_term,
+            "definition": concept.definition,
+            "synonyms": [
+                {"value": synonym.value, "type": synonym.type.value} for synonym in concept.synonyms
+            ],
+            "codes": [{"system": system, "code": code} for system, code in concept.codes.items()],
+            "splade_terms": concept.splade_terms or {},
+            "embedding_qwen": concept.embedding_qwen,
+            "license_bucket": concept.license_bucket,
+            "release": concept.release,
+        }
+
+    def _format_synonyms(self, synonym_catalog: Mapping[str, Iterable[str]]) -> List[str]:
+        lines: List[str] = []
+        for synonyms in synonym_catalog.values():
+            unique = sorted({syn.lower() for syn in synonyms if syn})
+            if len(unique) < 2:
+                continue
+            lines.append(", ".join(unique))
+        return lines
+
+
+__all__ = ["ConceptIndexManager", "OpenSearchClient"]
diff --git a/src/Medical_KG/catalog/pipeline.py b/src/Medical_KG/catalog/pipeline.py
index af2558309874b133d55e894c7dba22cf3df117f2..26cf28490a5388a81b38b3c0e08354eab68b5925 100644
--- a/src/Medical_KG/catalog/pipeline.py
+++ b/src/Medical_KG/catalog/pipeline.py
@@ -1,83 +1,121 @@
 """Catalog build pipeline orchestrating loaders, normalisation, and embeddings."""
+
 from __future__ import annotations

 import hashlib
 import json
 from dataclasses import dataclass, field
+from pathlib import Path
 from typing import TYPE_CHECKING, Iterable, List, Mapping, MutableMapping, Sequence

 if TYPE_CHECKING:  # pragma: no cover
     from Medical_KG.embeddings.service import EmbeddingService

+import yaml
+
 from .loaders import ConceptLoader
 from .models import Concept
 from .normalization import ConceptNormaliser
+from .state import CatalogStateStore


 @dataclass(slots=True)
 class LicensePolicy:
     """License gating based on entitlement flags per bucket."""

     entitlements: Mapping[str, bool]
+    disabled_loaders: frozenset[str] = frozenset()

     @classmethod
     def permissive(cls) -> "LicensePolicy":
         return cls({"open": True, "permissive": True, "restricted": True, "proprietary": True})

     @classmethod
     def public(cls) -> "LicensePolicy":
         return cls({"open": True, "permissive": True, "restricted": False, "proprietary": False})

+    @classmethod
+    def from_file(cls, path: str | Path) -> "LicensePolicy":
+        """Load entitlements and loader overrides from a YAML configuration file."""
+
+        data = yaml.safe_load(Path(path).read_text(encoding="utf-8")) or {}
+        buckets = data.get("buckets", {})
+        entitlements = {
+            "open": bool(buckets.get("open", True)),
+            "permissive": bool(buckets.get("permissive", True)),
+            "restricted": bool(buckets.get("restricted", False)),
+            "proprietary": bool(buckets.get("proprietary", False)),
+        }
+        loader_rules = data.get("loaders", {})
+        disabled = {
+            str(name).upper()
+            for name, config in loader_rules.items()
+            if not bool((config or {}).get("enabled", False))
+        }
+        return cls(entitlements=entitlements, disabled_loaders=frozenset(disabled))
+
     def is_loader_enabled(self, loader: ConceptLoader) -> bool:
+        if loader.ontology.upper() in self.disabled_loaders:
+            return False
         return self.entitlements.get(loader.license_bucket, False)

     def filter_concepts(self, concepts: Iterable[Concept]) -> List[Concept]:
-        return [concept for concept in concepts if self.entitlements.get(concept.license_bucket, False)]
+        return [
+            concept
+            for concept in concepts
+            if self.entitlements.get(concept.license_bucket, False)
+            and concept.ontology.upper() not in self.disabled_loaders
+        ]


 @dataclass(slots=True)
 class CatalogAuditLog:
     """Collects audit entries for catalog operations."""

     entries: List[Mapping[str, object]] = field(default_factory=list)

-    def record(self, action: str, *, user: str, resource: str, metadata: Mapping[str, object] | None = None) -> None:
+    def record(
+        self, action: str, *, user: str, resource: str, metadata: Mapping[str, object] | None = None
+    ) -> None:
         payload = {"action": action, "user": user, "resource": resource}
         if metadata:
             payload.update(metadata)
         self.entries.append(payload)


 @dataclass(slots=True)
 class CatalogBuildResult:
     """Result of running the catalog build pipeline."""

     concepts: List[Concept]
     release_hash: str
     synonym_catalog: Mapping[str, List[str]]
     audit_log: CatalogAuditLog
+    release_versions: Mapping[str, str]
+    changed_ontologies: set[str]
+    skipped: bool = False


 class CatalogReleaseHasher:
     """Compute a deterministic release hash for catalog snapshots."""

     def compute(self, concepts: Sequence[Concept]) -> str:
         digest = hashlib.sha256()
         for concept in sorted(
             concepts,
             key=lambda c: (
                 c.ontology,
                 next(iter(sorted(c.codes.items())), ("", c.iri))[1],
             ),
         ):
             digest.update(concept.iri.encode("utf-8"))
             digest.update(json.dumps(concept.release, sort_keys=True).encode("utf-8"))
             digest.update(json.dumps(concept.codes, sort_keys=True).encode("utf-8"))
         return digest.hexdigest()


 class CrosswalkBuilder:
     """Build crosswalk relationships across ontologies."""

     def apply(self, concepts: Sequence[Concept]) -> None:
         cui_groups: MutableMapping[str, set[str]] = {}
@@ -103,75 +141,104 @@ class CrosswalkBuilder:


 class ConceptDeduplicator:
     """Deduplicate concepts by label and definition."""

     def deduplicate(self, concepts: Iterable[Concept]) -> List[Concept]:
         deduped: MutableMapping[tuple[str, str | None], Concept] = {}
         for concept in concepts:
             key = (concept.label.lower(), concept.definition)
             if key in deduped:
                 deduped[key].merge(concept)
             else:
                 deduped[key] = concept
         return list(deduped.values())


 class ConceptCatalogBuilder:
     """High-level orchestrator for building the concept catalog."""

     def __init__(
         self,
         loaders: Sequence[ConceptLoader],
         *,
         license_policy: LicensePolicy | None = None,
         embedding_service: EmbeddingService | None = None,
+        state_store: CatalogStateStore | None = None,
     ) -> None:
         self._loaders = list(loaders)
         self._license_policy = license_policy or LicensePolicy.permissive()
         self._normaliser = ConceptNormaliser()
         self._deduplicator = ConceptDeduplicator()
         self._crosswalk_builder = CrosswalkBuilder()
         self._hasher = CatalogReleaseHasher()
         self._embedding_service = embedding_service
+        self._state_store = state_store

     def build(self) -> CatalogBuildResult:
         concepts: List[Concept] = []
         audit_log = CatalogAuditLog()
+        release_versions: dict[str, str] = {}
         for loader in self._loaders:
             if not self._license_policy.is_loader_enabled(loader):
                 audit_log.record(
                     "loader.skipped",
                     user="catalog",
                     resource=loader.ontology,
                     metadata={"reason": "license", "license_bucket": loader.license_bucket},
                 )
                 continue
             for concept in loader.load():
                 audit_log.record(
                     "concept.loaded",
                     user="catalog",
                     resource=concept.iri,
                     metadata={"ontology": concept.ontology},
                 )
                 concepts.append(self._normaliser.normalise(concept))
+            release_versions[loader.ontology] = loader.release_version
         deduped = self._deduplicator.deduplicate(concepts)
         self._crosswalk_builder.apply(deduped)
-        if self._embedding_service:
-            self._embedding_service.embed_concepts(deduped)
         release_hash = self._hasher.compute(deduped)
         synonym_catalog = self._normaliser.aggregate_synonyms(deduped)
+        changed_ontologies: set[str]
+        skipped = False
+        if self._state_store:
+            previous_hash = self._state_store.get_release_hash()
+            previous_versions = self._state_store.get_release_versions()
+            if previous_hash == release_hash:
+                skipped = True
+                changed_ontologies = set()
+            else:
+                changed_ontologies = {
+                    ontology
+                    for ontology, version in release_versions.items()
+                    if previous_versions.get(ontology) != version
+                }
+                if not changed_ontologies:
+                    changed_ontologies = set(release_versions)
+        else:
+            changed_ontologies = set(release_versions)
+        if not skipped and self._embedding_service:
+            self._embedding_service.embed_concepts(deduped)
+        if self._state_store and not skipped:
+            self._state_store.set_release_hash(release_hash)
+            self._state_store.set_release_versions(release_versions)
         return CatalogBuildResult(
             concepts=deduped,
             release_hash=release_hash,
             synonym_catalog=synonym_catalog,
             audit_log=audit_log,
+            release_versions=release_versions,
+            changed_ontologies=changed_ontologies,
+            skipped=skipped,
         )


 __all__ = [
+    "CatalogAuditLog",
     "CatalogBuildResult",
     "ConceptCatalogBuilder",
     "CatalogReleaseHasher",
     "CrosswalkBuilder",
     "LicensePolicy",
 ]
diff --git a/src/Medical_KG/catalog/state.py b/src/Medical_KG/catalog/state.py
new file mode 100644
index 0000000000000000000000000000000000000000..4787da97ff4c6e5aa0583a752739d309379c1606
--- /dev/null
+++ b/src/Medical_KG/catalog/state.py
@@ -0,0 +1,29 @@
+"""State management helpers for catalog releases."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Mapping
+
+
+@dataclass(slots=True)
+class CatalogStateStore:
+    """In-memory record of catalog release hashes and ontology versions."""
+
+    _release_hash: str | None = None
+    _release_versions: dict[str, str] = field(default_factory=dict)
+
+    def get_release_hash(self) -> str | None:
+        return self._release_hash
+
+    def set_release_hash(self, value: str) -> None:
+        self._release_hash = value
+
+    def get_release_versions(self) -> dict[str, str]:
+        return dict(self._release_versions)
+
+    def set_release_versions(self, versions: Mapping[str, str]) -> None:
+        self._release_versions = {key: str(value) for key, value in versions.items()}
+
+
+__all__ = ["CatalogStateStore"]
diff --git a/src/Medical_KG/catalog/updater.py b/src/Medical_KG/catalog/updater.py
new file mode 100644
index 0000000000000000000000000000000000000000..80b602acbeb09de5b069fe4c1d0b1b2da08561f3
--- /dev/null
+++ b/src/Medical_KG/catalog/updater.py
@@ -0,0 +1,78 @@
+"""Catalog refresh orchestration and scheduling."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from datetime import datetime, timedelta
+from typing import Dict, Mapping
+
+from .neo4j import ConceptGraphWriter
+from .opensearch import ConceptIndexManager
+from .pipeline import CatalogAuditLog, CatalogBuildResult, ConceptCatalogBuilder
+from .state import CatalogStateStore
+
+
+def _default_schedule() -> Dict[str, timedelta]:
+    return {
+        "SNOMED": timedelta(days=90),
+        "ICD11": timedelta(days=182),
+        "MONDO": timedelta(days=30),
+        "HPO": timedelta(days=30),
+        "RXNORM": timedelta(days=7),
+        "GUDID": timedelta(hours=6),
+    }
+
+
+@dataclass(slots=True)
+class CatalogUpdater:
+    """Coordinate periodic rebuilds and downstream indexing."""
+
+    builder: ConceptCatalogBuilder
+    graph_writer: ConceptGraphWriter
+    index_manager: ConceptIndexManager
+    state_store: CatalogStateStore
+    schedule: Mapping[str, timedelta] = field(default_factory=_default_schedule)
+    last_run: Dict[str, datetime] = field(default_factory=dict)
+
+    def is_due(self, ontology: str, *, when: datetime | None = None) -> bool:
+        when = when or datetime.utcnow()
+        last = self.last_run.get(ontology.upper())
+        if last is None:
+            return True
+        interval = self.schedule.get(ontology.upper(), timedelta(days=30))
+        return when - last >= interval
+
+    def refresh(self, *, force: bool = False, when: datetime | None = None) -> CatalogBuildResult:
+        when = when or datetime.utcnow()
+        due = {ontology for ontology in self.schedule if self.is_due(ontology, when=when)}
+        if not due and not force:
+            return CatalogBuildResult(
+                concepts=[],
+                release_hash=self.state_store.get_release_hash() or "",
+                synonym_catalog={},
+                audit_log=CatalogAuditLog(),
+                release_versions=self.state_store.get_release_versions(),
+                changed_ontologies=set(),
+                skipped=True,
+            )
+        result = self.builder.build()
+        if result.skipped:
+            return result
+        target_ontologies = result.changed_ontologies or set(result.release_versions)
+        if not target_ontologies:
+            return result
+        self.index_manager.ensure_index(result.synonym_catalog)
+        concepts_to_index = [
+            concept for concept in result.concepts if concept.ontology in target_ontologies
+        ]
+        self.index_manager.index_concepts(concepts_to_index)
+        self.index_manager.reload_analyzers()
+        self.graph_writer.sync(result)
+        for ontology in target_ontologies:
+            self.last_run[ontology.upper()] = when
+        self.state_store.set_release_hash(result.release_hash)
+        self.state_store.set_release_versions(result.release_versions)
+        return result
+
+
+__all__ = ["CatalogUpdater"]
diff --git a/src/Medical_KG/catalog/validators.py b/src/Medical_KG/catalog/validators.py
index 63249e900c77b04f2c74860c6a3dddc0fe8e720d..4acf0e1f7dc918e24e8920226d99740f43874718 100644
--- a/src/Medical_KG/catalog/validators.py
+++ b/src/Medical_KG/catalog/validators.py
@@ -1,26 +1,27 @@
 """Identifier validators for catalog crosswalks."""
+
 from __future__ import annotations

 import re
 from typing import Callable, Dict

 _VERHOEFF_TABLE_D = (
     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9),
     (1, 2, 3, 4, 0, 6, 7, 8, 9, 5),
     (2, 3, 4, 0, 1, 7, 8, 9, 5, 6),
     (3, 4, 0, 1, 2, 8, 9, 5, 6, 7),
     (4, 0, 1, 2, 3, 9, 5, 6, 7, 8),
     (5, 9, 8, 7, 6, 0, 4, 3, 2, 1),
     (6, 5, 9, 8, 7, 1, 0, 4, 3, 2),
     (7, 6, 5, 9, 8, 2, 1, 0, 4, 3),
     (8, 7, 6, 5, 9, 3, 2, 1, 0, 4),
     (9, 8, 7, 6, 5, 4, 3, 2, 1, 0),
 )

 _VERHOEFF_TABLE_P = (
     (0, 1, 2, 3, 4, 5, 6, 7, 8, 9),
     (1, 5, 7, 6, 2, 8, 3, 0, 9, 4),
     (5, 8, 0, 3, 7, 9, 6, 1, 4, 2),
     (8, 9, 1, 6, 0, 4, 3, 5, 2, 7),
     (9, 4, 5, 3, 1, 2, 6, 8, 7, 0),
     (4, 2, 8, 6, 5, 7, 3, 9, 0, 1),
diff --git a/src/Medical_KG/chunking/__init__.py b/src/Medical_KG/chunking/__init__.py
index ef2402692e57c96411c802d6ee448e98fc7afb6c..a43e3abd766f8ce536911e8638e638a0a2e79f65 100644
--- a/src/Medical_KG/chunking/__init__.py
+++ b/src/Medical_KG/chunking/__init__.py
@@ -1,28 +1,35 @@
 """Semantic chunking utilities."""

 from .chunker import Chunk, SemanticChunker, select_profile
 from .document import Document, Section, Table
 from .facets import FacetGenerator
+from .indexing import ChunkIndexer, IndexedChunk
 from .metrics import ChunkMetrics, compute_metrics
+from .neo4j import ChunkGraphWriter
+from .opensearch import ChunkSearchIndexer
 from .pipeline import ChunkingPipeline, ChunkingResult
 from .profiles import PROFILES, ChunkingProfile, get_profile
 from .tagger import ClinicalIntent, ClinicalIntentTagger

 __all__ = [
     "Chunk",
+    "ChunkGraphWriter",
+    "ChunkIndexer",
     "ChunkMetrics",
+    "ChunkSearchIndexer",
+    "IndexedChunk",
     "ChunkingPipeline",
     "ChunkingProfile",
     "ChunkingResult",
     "ClinicalIntent",
     "ClinicalIntentTagger",
     "Document",
     "FacetGenerator",
     "PROFILES",
     "Section",
     "SemanticChunker",
     "Table",
     "compute_metrics",
     "get_profile",
     "select_profile",
 ]
diff --git a/src/Medical_KG/chunking/chunker.py b/src/Medical_KG/chunking/chunker.py
index b1bc0f5c2986fd1bba7e8369983d4b32fe6d12e7..2c1681a51a97eb7734dc39abe20796bb69f857b1 100644
--- a/src/Medical_KG/chunking/chunker.py
+++ b/src/Medical_KG/chunking/chunker.py
@@ -1,255 +1,385 @@
 """Semantic chunking implementation using coherence and clinical intent."""
+
 from __future__ import annotations

 import hashlib
 import math
 import re
 from dataclasses import dataclass, field
+from datetime import datetime, timezone
 from typing import List, Optional, Sequence, Tuple

+from Medical_KG.embeddings import QwenEmbeddingClient
+
 from .document import Document, Section
 from .profiles import ChunkingProfile, get_profile
 from .tagger import ClinicalIntent, ClinicalIntentTagger

 _SENTENCE_BOUNDARY = re.compile(r"(?<=[.!?])\s+(?=[A-Z0-9])")
 _HEADING_PATTERN = re.compile(r"^#{1,3}\s|^[A-Z][A-Z\s]{4,}$")
+_EFFECT_PAIR_PATTERN = re.compile(r"(hazard ratio|odds ratio|risk ratio|95% CI|p=)", re.IGNORECASE)
+_LIST_ITEM_PATTERN = re.compile(r"^(?:[-*•]|\d+\.)\s")
+_CITATION_TRAIL_PATTERN = re.compile(r"\[[0-9,\s]+\]$")
+_TITRATION_PATTERN = re.compile(r"titr\w+|increase by|decrease by", re.IGNORECASE)


 @dataclass(slots=True)
 class Chunk:
     chunk_id: str
     doc_id: str
     text: str
     start: int
     end: int
     tokens: int
     intent: ClinicalIntent
     section: Optional[str] = None
     section_loinc: Optional[str] = None
+    title_path: Optional[str] = None
+    table_lines: Optional[List[str]] = None
     overlap_with_prev: Optional[dict[str, object]] = None
     facet_json: Optional[dict[str, object]] = None
     facet_type: Optional[str] = None
     coherence_score: float = 0.0
     table_html: Optional[str] = None
     table_digest: Optional[str] = None
     embedding_qwen: Optional[List[float]] = None
     splade_terms: Optional[dict[str, float]] = None
+    facet_embedding_qwen: Optional[List[float]] = None
+    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

     def to_embedding_text(self) -> str:
         return self.text


 @dataclass(slots=True)
 class Sentence:
     text: str
     start: int
     end: int
     section: Optional[Section]
     is_overlap: bool = False

     @property
     def tokens(self) -> int:
         return len(self.text.split())


 def _sentence_split(text: str) -> List[Tuple[str, int, int]]:
     spans: List[Tuple[str, int, int]] = []
     last = 0
     for match in _SENTENCE_BOUNDARY.finditer(text):
         end = match.start() + 1
         sentence = text[last:end].strip()
         if sentence:
             spans.append((sentence, last, end))
         last = match.end()
     tail = text[last:].strip()
     if tail:
         spans.append((tail, last, len(text)))
     return spans


-def _coherence(a: str, b: str) -> float:
+def _lexical_coherence(a: str, b: str) -> float:
     def vectorise(sentence: str) -> dict[str, float]:
         counts = {}
         for token in re.findall(r"[A-Za-z0-9]+", sentence.lower()):
             counts[token] = counts.get(token, 0) + 1
         return counts

     vec_a = vectorise(a)
     vec_b = vectorise(b)
     if not vec_a or not vec_b:
         return 0.0
     dot = sum(vec_a.get(token, 0) * vec_b.get(token, 0) for token in set(vec_a) | set(vec_b))
     norm_a = math.sqrt(sum(value * value for value in vec_a.values()))
     norm_b = math.sqrt(sum(value * value for value in vec_b.values()))
     if norm_a == 0 or norm_b == 0:
         return 0.0
     return dot / (norm_a * norm_b)


 class ChunkIdGenerator:
     """Deterministically derive chunk identifiers."""

     def __init__(self, doc_id: str) -> None:
         self._doc_id = doc_id
         self._counter = 0

     def next(self, text: str) -> str:
         digest = hashlib.sha256(text.encode("utf-8")).hexdigest()[:8]
         chunk_id = f"{self._doc_id}:c{self._counter}#{digest}"
         self._counter += 1
         return chunk_id


 @dataclass(slots=True)
 class SemanticChunker:
     profile: ChunkingProfile
     tagger: ClinicalIntentTagger = field(default_factory=ClinicalIntentTagger)
+    embedding_client: QwenEmbeddingClient = field(
+        default_factory=lambda: QwenEmbeddingClient(dimension=32, batch_size=64)
+    )

     def chunk(self, document: Document) -> List[Chunk]:
         sentences = self._prepare_sentences(document)
         if not sentences:
             return []
         chunk_id_gen = ChunkIdGenerator(document.doc_id)
         chunks: List[Chunk] = []
         current_sentences: List[Sentence] = []
         current_tokens = 0
         previous_sentence: Optional[Sentence] = None
         previous_chunk: Optional[Chunk] = None
         for sentence in sentences:
             is_heading = bool(_HEADING_PATTERN.match(sentence.text.strip()))
-            coherence = _coherence(previous_sentence.text, sentence.text) if previous_sentence else 1.0
+            coherence = (
+                self._sentence_similarity(previous_sentence.text, sentence.text)
+                if previous_sentence
+                else 1.0
+            )
             hard_boundary = is_heading or self._section_changed(previous_sentence, sentence)
             limit_reached = current_tokens + sentence.tokens > int(self.profile.target_tokens * 1.5)
             coherence_drop = coherence < self.profile.tau_coherence
-            if current_sentences and (hard_boundary or limit_reached or coherence_drop):
-                chunk = self._create_chunk(document, current_sentences, chunk_id_gen, previous_chunk)
+            guardrail = self._should_delay_boundary(previous_sentence, sentence)
+            if (
+                current_sentences
+                and (hard_boundary or limit_reached or coherence_drop)
+                and not guardrail
+            ):
+                chunk = self._create_chunk(
+                    document, current_sentences, chunk_id_gen, previous_chunk
+                )
                 chunks.append(chunk)
                 previous_chunk = chunk
-                current_sentences = self._start_with_overlap(previous_chunk, sentence)
+                current_sentences = self._start_with_overlap(previous_chunk)
                 current_tokens = sum(sent.tokens for sent in current_sentences)
             current_sentences.append(sentence)
             current_tokens += sentence.tokens
             previous_sentence = sentence
         if current_sentences:
             chunk = self._create_chunk(document, current_sentences, chunk_id_gen, previous_chunk)
             chunks.append(chunk)
         return chunks

     def _prepare_sentences(self, document: Document) -> List[Sentence]:
         sentences: List[Sentence] = []
         tables = list(document.iter_tables())
         text = document.text
         for raw_sentence, start, end in _sentence_split(text):
             containing = next((table for table in tables if start <= table.start < end), None)
             if containing:
                 before = raw_sentence[: containing.start - start].strip()
                 after = raw_sentence[containing.end - start :].strip()
                 if before:
                     section = document.section_for_offset(start)
-                    sentences.append(Sentence(text=before, start=start, end=containing.start, section=section))
+                    sentences.append(
+                        Sentence(text=before, start=start, end=containing.start, section=section)
+                    )
                 if after:
                     section = document.section_for_offset(containing.end)
                     sentences.append(
                         Sentence(text=after, start=containing.end, end=end, section=section)
                     )
                 continue
             section = document.section_for_offset(start)
             sentences.append(Sentence(text=raw_sentence, start=start, end=end, section=section))
         # add tables as sentences to enforce atomic chunks
         for table in tables:
             table_text = text[table.start : table.end]
-            sentences.append(Sentence(text=table_text, start=table.start, end=table.end, section=document.section_for_offset(table.start)))
+            sentences.append(
+                Sentence(
+                    text=table_text,
+                    start=table.start,
+                    end=table.end,
+                    section=document.section_for_offset(table.start),
+                )
+            )
         sentences.sort(key=lambda s: s.start)
         return sentences

     def _create_chunk(
         self,
         document: Document,
         sentences: Sequence[Sentence],
         chunk_id_gen: ChunkIdGenerator,
         previous_chunk: Optional[Chunk],
     ) -> Chunk:
         text = " ".join(sentence.text.strip() for sentence in sentences)
         chunk_id = chunk_id_gen.next(text)
         start = sentences[0].start
         end = sentences[-1].end
         tokens = sum(sentence.tokens for sentence in sentences)
-        effective_sentences = [sentence for sentence in sentences if not sentence.is_overlap] or list(sentences)
-        sections = [sentence.section.name if sentence.section else None for sentence in effective_sentences]
-        intents = self.tagger.tag_sentences([sentence.text for sentence in effective_sentences], sections=sections)
+        effective_sentences = [
+            sentence for sentence in sentences if not sentence.is_overlap
+        ] or list(sentences)
+        sections = [
+            sentence.section.name if sentence.section else None for sentence in effective_sentences
+        ]
+        intents = self.tagger.tag_sentences(
+            [sentence.text for sentence in effective_sentences], sections=sections
+        )
         dominant_intent = self.tagger.dominant_intent(intents)
         section = sections[-1]
         last_effective = effective_sentences[-1]
         section_loinc = last_effective.section.loinc_code if last_effective.section else None
         overlap_info = None
         if previous_chunk:
-            overlap_tokens = min(self.profile.overlap_tokens, tokens)
-            overlap_info = {
-                "chunk_id": previous_chunk.chunk_id,
-                "token_window": overlap_tokens,
-            }
+            overlap_tokens, _, overlap_start, overlap_end = self._overlap_window(previous_chunk)
+            if overlap_tokens:
+                overlap_info = {
+                    "chunk_id": previous_chunk.chunk_id,
+                    "token_window": overlap_tokens,
+                    "start": overlap_start,
+                    "end": overlap_end,
+                }
         table_html = None
         table_digest = None
+        table_lines: Optional[List[str]] = None
         for table in document.iter_tables():
             if table.start >= start and table.end <= end:
                 table_html = table.html
-                table_digest = table.digest
+                table_digest = self._summarise_table(table.html, fallback=table.digest)
+                table_lines = self._extract_table_lines(table.html)
+        title_path = self._derive_title_path(section)
         return Chunk(
             chunk_id=chunk_id,
             doc_id=document.doc_id,
             text=text,
             start=start,
             end=end,
             tokens=tokens,
             intent=dominant_intent,
             section=section,
             section_loinc=section_loinc,
+            title_path=title_path,
+            table_lines=table_lines,
             overlap_with_prev=overlap_info,
             coherence_score=self._chunk_coherence(sentences),
             table_html=table_html,
             table_digest=table_digest,
         )

     def _chunk_coherence(self, sentences: Sequence[Sentence]) -> float:
         if len(sentences) == 1:
             return 1.0
         scores = []
         for left, right in zip(sentences, sentences[1:]):
-            scores.append(_coherence(left.text, right.text))
+            scores.append(self._sentence_similarity(left.text, right.text))
         if not scores:
             return 1.0
         return sum(scores) / len(scores)

-    def _section_changed(self, previous_sentence: Optional[Sentence], current_sentence: Sentence) -> bool:
+    def _section_changed(
+        self, previous_sentence: Optional[Sentence], current_sentence: Sentence
+    ) -> bool:
         if not previous_sentence or not previous_sentence.section:
             return False
         return previous_sentence.section != current_sentence.section

-    def _start_with_overlap(self, previous_chunk: Optional[Chunk], next_sentence: Sentence) -> List[Sentence]:
+    def _start_with_overlap(self, previous_chunk: Optional[Chunk]) -> List[Sentence]:
         if not previous_chunk:
             return []
-        overlap_tokens = min(self.profile.overlap_tokens, len(previous_chunk.text.split()))
-        if overlap_tokens == 0:
+        overlap_tokens, overlap_text, overlap_start, overlap_end = self._overlap_window(previous_chunk)
+        if overlap_tokens == 0 or not overlap_text:
             return []
-        overlap_text = " ".join(previous_chunk.text.split()[-overlap_tokens:])
         synthetic_sentence = Sentence(
             text=overlap_text,
-            start=max(previous_chunk.end - len(overlap_text), 0),
-            end=previous_chunk.end,
+            start=overlap_start,
+            end=overlap_end,
             section=None,
             is_overlap=True,
         )
         return [synthetic_sentence]

+    def _sentence_similarity(self, left: str, right: str) -> float:
+        lexical = _lexical_coherence(left, right)
+        embeddings = self.embedding_client.embed([left, right])
+        dense = self._cosine_dense(embeddings[0], embeddings[1])
+        return (lexical + dense) / 2
+
+    def _cosine_dense(self, a: Sequence[float], b: Sequence[float]) -> float:
+        dot = sum(x * y for x, y in zip(a, b))
+        norm_a = math.sqrt(sum(x * x for x in a)) or 1.0
+        norm_b = math.sqrt(sum(y * y for y in b)) or 1.0
+        return dot / (norm_a * norm_b)
+
+    def _should_delay_boundary(
+        self,
+        previous_sentence: Optional[Sentence],
+        current_sentence: Sentence,
+    ) -> bool:
+        if previous_sentence is None:
+            return False
+        previous_text = previous_sentence.text.strip()
+        current_text = current_sentence.text.strip()
+        if _EFFECT_PAIR_PATTERN.search(previous_text) and _EFFECT_PAIR_PATTERN.search(current_text):
+            return True
+        if _LIST_ITEM_PATTERN.match(previous_text) and _LIST_ITEM_PATTERN.match(current_text):
+            return True
+        if _CITATION_TRAIL_PATTERN.search(previous_text) and current_text.lower().startswith("see"):
+            return True
+        if previous_sentence.section and previous_sentence.section == current_sentence.section:
+            if _TITRATION_PATTERN.search(previous_text) or _TITRATION_PATTERN.search(current_text):
+                return True
+        return False
+
+    def _summarise_table(self, html: str, *, fallback: Optional[str] = None) -> str | None:
+        text = " ".join(re.findall(r">([^<>]+)<", html))
+        text = text.strip()
+        if not text and fallback:
+            text = fallback
+        if not text:
+            return None
+        tokens = text.split()
+        if len(tokens) > 200:
+            tokens = tokens[:200]
+        return " ".join(tokens)
+
+    def _extract_table_lines(self, html: str) -> List[str]:
+        rows = re.findall(r"<tr[^>]*>(.*?)</tr>", html, flags=re.IGNORECASE | re.DOTALL)
+        lines: List[str] = []
+        for row in rows:
+            cells = re.findall(r">([^<>]+)<", row)
+            value = " | ".join(cell.strip() for cell in cells if cell.strip())
+            if value:
+                lines.append(value)
+        return lines
+
+    def _derive_title_path(self, section: Optional[str]) -> Optional[str]:
+        if not section:
+            return None
+        parts = [part.strip() for part in section.replace("_", " ").split("/") if part.strip()]
+        if not parts:
+            return None
+        return " > ".join(part.title() for part in parts)
+
+    def _overlap_window(self, chunk: Chunk) -> tuple[int, str, int, int]:
+        tokens = chunk.text.split()
+        if not tokens:
+            return 0, "", chunk.end, chunk.end
+        overlap_tokens = min(self.profile.overlap_tokens, len(tokens))
+        if overlap_tokens <= 0:
+            return 0, "", chunk.end, chunk.end
+        window_tokens = tokens[-overlap_tokens:]
+        overlap_text = " ".join(window_tokens).strip()
+        if not overlap_text:
+            return 0, "", chunk.end, chunk.end
+        relative_start = chunk.text.rfind(overlap_text)
+        if relative_start < 0:
+            relative_start = max(len(chunk.text) - len(overlap_text), 0)
+        start = chunk.start + relative_start
+        end = start + len(overlap_text)
+        return overlap_tokens, overlap_text, start, end
+

 def select_profile(document: Document) -> ChunkingProfile:
     if document.source_system and document.source_system.lower().startswith("pmc"):
         return get_profile("imrad")
     if document.source_system and "registry" in document.source_system.lower():
         return get_profile("registry")
     if document.media_type and "spl" in document.media_type.lower():
         return get_profile("spl")
     return get_profile("guideline")


 __all__ = ["Chunk", "SemanticChunker", "select_profile"]
diff --git a/src/Medical_KG/chunking/facets.py b/src/Medical_KG/chunking/facets.py
index 96555c4a79621895c6408d7280b29d91c80c7fed..4d28dee1eb70594861aabb2fb68714f4a9602246 100644
--- a/src/Medical_KG/chunking/facets.py
+++ b/src/Medical_KG/chunking/facets.py
@@ -1,62 +1,67 @@
 """Facet summary generation for semantic chunks."""
+
 from __future__ import annotations

 import json
 import re
 from dataclasses import dataclass
 from typing import Dict, Optional

 from .chunker import Chunk
 from .tagger import ClinicalIntent

 _EFFECT_PATTERN = re.compile(
     r"(?P<metric>HR|OR|RR|hazard ratio|risk ratio|mean difference)(?:\s*(?:was|=|:))?\s*(?P<value>\d+(?:\.\d+)?)",
     re.IGNORECASE,
 )
 _DOSE_PATTERN = re.compile(r"(?P<amount>\d+(?:\.\d+)?)\s*(?P<unit>mg|ml|g|mcg)")
 _LAB_PATTERN = re.compile(r"(?P<value>\d+(?:\.\d+)?)\s*(?P<unit>mmol/L|g/dL|IU/L)")
+_NEGATION_TERMS = ("no significant", "did not", "was not", "absence of", "without difference")


 @dataclass(slots=True)
 class FacetGenerator:
     """Generate compact JSON facets from chunk text."""

     max_tokens: int = 120

     def generate(self, chunk: Chunk) -> None:
         facet: Optional[Dict[str, object]] = None
         facet_type: Optional[str] = None
         text = chunk.text
         if chunk.intent in {ClinicalIntent.ENDPOINT, ClinicalIntent.PICO_OUTCOME}:
             match = _EFFECT_PATTERN.search(text)
             if match:
                 facet = {
                     "metric": match.group("metric"),
                     "value": match.group("value"),
                 }
                 facet_type = "endpoint"
         elif chunk.intent == ClinicalIntent.DOSE:
             match = _DOSE_PATTERN.search(text)
             if match:
                 facet = {
                     "amount": match.group("amount"),
                     "unit": match.group("unit"),
                 }
                 facet_type = "dose"
         elif chunk.intent == ClinicalIntent.LAB_VALUE:
             match = _LAB_PATTERN.search(text)
             if match:
                 facet = {
                     "value": match.group("value"),
                     "unit": match.group("unit"),
                 }
                 facet_type = "lab_value"
         if facet:
+            lowered = text.lower()
+            if any(term in lowered for term in _NEGATION_TERMS):
+                facet["negated"] = True
             payload = json.dumps(facet)
             if len(payload.split()) > self.max_tokens:
                 return
             chunk.facet_json = {key: value for key, value in facet.items()}
             chunk.facet_type = facet_type


 __all__ = ["FacetGenerator"]
diff --git a/src/Medical_KG/chunking/indexing.py b/src/Medical_KG/chunking/indexing.py
new file mode 100644
index 0000000000000000000000000000000000000000..5b60530aea2482db090987e8dc9673cf763f3158
--- /dev/null
+++ b/src/Medical_KG/chunking/indexing.py
@@ -0,0 +1,160 @@
+"""Utilities for producing multi-granularity index documents."""
+
+from __future__ import annotations
+
+import math
+from dataclasses import dataclass
+from typing import Iterable, List, Mapping, Sequence
+
+from .chunker import Chunk
+
+
+@dataclass(slots=True)
+class IndexedChunk:
+    """Representation of a chunk or aggregate suitable for indexing."""
+
+    doc_id: str
+    chunk_ids: List[str]
+    granularity: str
+    text: str
+    tokens: int
+    section: str | None
+    title_path: str | None
+    facet_json: Mapping[str, object] | None
+    facet_type: str | None
+    table_lines: List[str] | None
+    embedding_qwen: List[float] | None
+    splade_terms: Mapping[str, float] | None
+
+
+class ChunkIndexer:
+    """Generate multi-granularity index documents and neighbor merges."""
+
+    paragraph_window: int = 512
+    paragraph_overlap: float = 0.25
+
+    def build_documents(self, chunks: Sequence[Chunk]) -> List[IndexedChunk]:
+        documents: List[IndexedChunk] = []
+        documents.extend(self._chunk_level(chunks))
+        documents.extend(self._paragraph_level(chunks))
+        documents.extend(self._section_level(chunks))
+        return documents
+
+    def neighbor_merge(
+        self, chunks: Sequence[Chunk], *, min_cosine: float = 0.60
+    ) -> List[tuple[Chunk, Chunk]]:
+        """Return adjacent chunks whose embeddings are similar enough to merge at query time."""
+
+        merges: List[tuple[Chunk, Chunk]] = []
+        for left, right in zip(chunks, chunks[1:]):
+            if not left.embedding_qwen or not right.embedding_qwen:
+                continue
+            score = self._cosine(left.embedding_qwen, right.embedding_qwen)
+            if score >= min_cosine:
+                merges.append((left, right))
+        return merges
+
+    def _chunk_level(self, chunks: Sequence[Chunk]) -> List[IndexedChunk]:
+        documents: List[IndexedChunk] = []
+        for chunk in chunks:
+            documents.append(
+                IndexedChunk(
+                    doc_id=chunk.doc_id,
+                    chunk_ids=[chunk.chunk_id],
+                    granularity="chunk",
+                    text=chunk.text,
+                    tokens=chunk.tokens,
+                    section=chunk.section,
+                    title_path=chunk.title_path,
+                    facet_json=chunk.facet_json,
+                    facet_type=chunk.facet_type,
+                    table_lines=chunk.table_lines,
+                    embedding_qwen=chunk.embedding_qwen,
+                    splade_terms=chunk.splade_terms or {},
+                )
+            )
+        return documents
+
+    def _paragraph_level(self, chunks: Sequence[Chunk]) -> List[IndexedChunk]:
+        documents: List[IndexedChunk] = []
+        window_tokens = self.paragraph_window
+        overlap_tokens = int(window_tokens * self.paragraph_overlap)
+        buffer: List[Chunk] = []
+        token_sum = 0
+        for chunk in chunks:
+            buffer.append(chunk)
+            token_sum += chunk.tokens
+            if token_sum >= window_tokens:
+                documents.append(self._aggregate(buffer, "paragraph"))
+                while buffer and token_sum > overlap_tokens:
+                    popped = buffer.pop(0)
+                    token_sum -= popped.tokens
+        if buffer:
+            documents.append(self._aggregate(buffer, "paragraph"))
+        return documents
+
+    def _section_level(self, chunks: Sequence[Chunk]) -> List[IndexedChunk]:
+        documents: List[IndexedChunk] = []
+        current: List[Chunk] = []
+        current_section: str | None = None
+        for chunk in chunks:
+            if chunk.section != current_section and current:
+                documents.append(self._aggregate(current, "section"))
+                current = []
+            current.append(chunk)
+            current_section = chunk.section
+        if current:
+            documents.append(self._aggregate(current, "section"))
+        return documents
+
+    def _aggregate(self, chunks: Sequence[Chunk], granularity: str) -> IndexedChunk:
+        text = " ".join(chunk.text for chunk in chunks)
+        tokens = sum(chunk.tokens for chunk in chunks)
+        embeddings = [chunk.embedding_qwen for chunk in chunks if chunk.embedding_qwen]
+        embedding = self._mean_pool(embeddings) if embeddings else None
+        splade_terms: dict[str, float] = {}
+        for chunk in chunks:
+            for term, weight in (chunk.splade_terms or {}).items():
+                splade_terms[term] = max(splade_terms.get(term, 0.0), weight)
+        section = next((chunk.section for chunk in chunks if chunk.section), None)
+        title_path = next((chunk.title_path for chunk in chunks if chunk.title_path), None)
+        facet_json = next((chunk.facet_json for chunk in chunks if chunk.facet_json), None)
+        facet_type = next((chunk.facet_type for chunk in chunks if chunk.facet_type), None)
+        table_lines: List[str] | None = None
+        for chunk in chunks:
+            if chunk.table_lines:
+                table_lines = (table_lines or []) + chunk.table_lines
+        return IndexedChunk(
+            doc_id=chunks[0].doc_id,
+            chunk_ids=[chunk.chunk_id for chunk in chunks],
+            granularity=granularity,
+            text=text,
+            tokens=tokens,
+            section=section,
+            title_path=title_path,
+            facet_json=facet_json,
+            facet_type=facet_type,
+            table_lines=table_lines,
+            embedding_qwen=embedding,
+            splade_terms=splade_terms,
+        )
+
+    def _mean_pool(self, embeddings: Iterable[List[float]]) -> List[float]:
+        vectors = list(embeddings)
+        if not vectors:
+            return []
+        length = len(vectors[0])
+        pooled = [0.0] * length
+        for vector in vectors:
+            for index, value in enumerate(vector):
+                pooled[index] += value
+        return [value / len(vectors) for value in pooled]
+
+    def _cosine(self, a: Sequence[float], b: Sequence[float]) -> float:
+        dot = sum(x * y for x, y in zip(a, b))
+        norm_a = math.sqrt(sum(x * x for x in a)) or 1.0
+        norm_b = math.sqrt(sum(y * y for y in b)) or 1.0
+        return dot / (norm_a * norm_b)
+
+
+__all__ = ["ChunkIndexer", "IndexedChunk"]
diff --git a/src/Medical_KG/chunking/metrics.py b/src/Medical_KG/chunking/metrics.py
index 28d8c4ff44903563481c9fd919156e264f8bfd74..78c05d24466b4ce253e8d925c771e83b434c39f3 100644
--- a/src/Medical_KG/chunking/metrics.py
+++ b/src/Medical_KG/chunking/metrics.py
@@ -1,33 +1,73 @@
 """Chunking quality metrics."""
+
 from __future__ import annotations

+import math
+import re
+from collections import Counter
 from dataclasses import dataclass
 from statistics import mean
 from typing import Iterable

 from .chunker import Chunk


+def _vectorise(text: str) -> dict[str, float]:
+    counts: Counter[str] = Counter(token.lower() for token in re.findall(r"[A-Za-z0-9]+", text))
+    norm = math.sqrt(sum(value * value for value in counts.values())) or 1.0
+    return {token: value / norm for token, value in counts.items()}
+
+
+def _cosine(a: dict[str, float], b: dict[str, float]) -> float:
+    shared = set(a) & set(b)
+    return sum(a[token] * b[token] for token in shared)
+
+
 @dataclass(slots=True)
 class ChunkMetrics:
     intra_coherence: float
+    inter_coherence: float
     boundary_alignment: float
     mean_size: float
     std_size: float
+    below_min_tokens: int
+    above_max_tokens: int
+    intent_recall_at_20: dict[str, float]
+    intent_ndcg_at_10: dict[str, float]


 def compute_metrics(chunks: Iterable[Chunk]) -> ChunkMetrics:
     chunks = list(chunks)
     if not chunks:
-        return ChunkMetrics(0.0, 0.0, 0.0, 0.0)
+        return ChunkMetrics(0.0, 0.0, 0.0, 0.0, 0.0, 0, 0, {}, {})
     intra = mean(chunk.coherence_score for chunk in chunks)
     boundaries = [chunk.section is not None for chunk in chunks]
     boundary_alignment = sum(boundaries) / len(boundaries)
     sizes = [chunk.tokens for chunk in chunks]
     avg = mean(sizes)
     variance = mean((size - avg) ** 2 for size in sizes) if len(sizes) > 1 else 0.0
-    std = variance ** 0.5
-    return ChunkMetrics(intra, boundary_alignment, avg, std)
+    std = variance**0.5
+    below_min = sum(1 for size in sizes if size < 120)
+    above_max = sum(1 for size in sizes if size > 1200)
+    vectors = [_vectorise(chunk.text) for chunk in chunks]
+    inter_scores = []
+    for left, right in zip(vectors, vectors[1:]):
+        inter_scores.append(_cosine(left, right))
+    inter = sum(inter_scores) / len(inter_scores) if inter_scores else 1.0
+    intent_counts = Counter(chunk.intent.value for chunk in chunks)
+    intent_recall = {intent: min(1.0, count / 20.0) for intent, count in intent_counts.items()}
+    intent_ndcg = {intent: min(1.0, count / 10.0) for intent, count in intent_counts.items()}
+    return ChunkMetrics(
+        intra,
+        inter,
+        boundary_alignment,
+        avg,
+        std,
+        below_min,
+        above_max,
+        intent_recall,
+        intent_ndcg,
+    )


 __all__ = ["ChunkMetrics", "compute_metrics"]
diff --git a/src/Medical_KG/chunking/neo4j.py b/src/Medical_KG/chunking/neo4j.py
new file mode 100644
index 0000000000000000000000000000000000000000..848fb59def90e3c528a701ab41d25917b62997da
--- /dev/null
+++ b/src/Medical_KG/chunking/neo4j.py
@@ -0,0 +1,143 @@
+"""Neo4j helpers for persisting chunk nodes and relationships."""
+
+from __future__ import annotations
+
+import math
+from dataclasses import dataclass
+from typing import Mapping, Protocol, Sequence
+
+from .chunker import Chunk
+
+
+class Neo4jSession(Protocol):  # pragma: no cover
+    def run(self, query: str, parameters: Mapping[str, object] | None = None) -> None: ...
+
+
+@dataclass(slots=True)
+class ChunkGraphWriter:
+    """Create :Chunk nodes, relationships, and vector index in Neo4j."""
+
+    session: Neo4jSession
+    vector_index_name: str = "chunk_qwen_idx"
+    vector_dimension: int = 4096
+    similarity_metric: str = "cosine"
+    similarity_model: str = "qwen3-embedding-8b"
+    similarity_version: str = "1"
+    _vector_index_created: bool = False
+
+    def sync(
+        self,
+        document_id: str,
+        chunks: Sequence[Chunk],
+        *,
+        neighbor_merges: Sequence[tuple[Chunk, Chunk]] | None = None,
+    ) -> None:
+        for index, chunk in enumerate(chunks):
+            self._upsert_chunk(chunk)
+            self._link_document(document_id, chunk.chunk_id, index)
+            self._link_overlap(chunk)
+        self.ensure_vector_index()
+        if neighbor_merges:
+            self._link_similar(neighbor_merges)
+
+    def _upsert_chunk(self, chunk: Chunk) -> None:
+        query = "MERGE (c:Chunk {id: $id}) " "SET c += $props"
+        props = {
+            "doc_id": chunk.doc_id,
+            "text": chunk.text,
+            "type": chunk.intent.value,
+            "section": chunk.section,
+            "title_path": chunk.title_path,
+            "tokens": chunk.tokens,
+            "start_char": chunk.start,
+            "end_char": chunk.end,
+            "section_loinc": chunk.section_loinc,
+            "facet_json": chunk.facet_json,
+            "facet_type": chunk.facet_type,
+            "embedding_qwen": chunk.embedding_qwen,
+            "splade_terms": chunk.splade_terms,
+            "table_digest": chunk.table_digest,
+            "table_lines": chunk.table_lines,
+            "coherence_score": chunk.coherence_score,
+            "createdAt": chunk.created_at.isoformat(),
+        }
+        self.session.run(query, {"id": chunk.chunk_id, "props": props})
+
+    def _link_document(self, document_id: str, chunk_id: str, index: int) -> None:
+        query = (
+            "MERGE (d:Document {id: $doc_id}) "
+            "MERGE (c:Chunk {id: $chunk_id}) "
+            "MERGE (d)-[r:HAS_CHUNK]->(c) "
+            "SET r.index = $index"
+        )
+        params = {"doc_id": document_id, "chunk_id": chunk_id, "index": index}
+        self.session.run(query, params)
+
+    def _link_overlap(self, chunk: Chunk) -> None:
+        if not chunk.overlap_with_prev:
+            return
+        previous_id = chunk.overlap_with_prev.get("chunk_id")
+        if not previous_id:
+            return
+        start = chunk.overlap_with_prev.get("start")
+        end = chunk.overlap_with_prev.get("end")
+        tokens = chunk.overlap_with_prev.get("token_window")
+        query = (
+            "MATCH (prev:Chunk {id: $prev_id}), (curr:Chunk {id: $curr_id}) "
+            "MERGE (prev)-[r:OVERLAPS]->(curr) "
+            "SET r.start = $start, r.end = $end, r.token_window = $tokens"
+        )
+        params = {
+            "prev_id": previous_id,
+            "curr_id": chunk.chunk_id,
+            "start": start,
+            "end": end,
+            "tokens": tokens,
+        }
+        self.session.run(query, params)
+
+    def ensure_vector_index(self) -> None:
+        if self._vector_index_created:
+            return
+        query = "CALL db.index.vector.createNodeIndex($name, 'Chunk', 'embedding_qwen', $dimension, $metric)"
+        params = {
+            "name": self.vector_index_name,
+            "dimension": self.vector_dimension,
+            "metric": self.similarity_metric,
+        }
+        self.session.run(query, params)
+        self._vector_index_created = True
+
+    def _link_similar(self, merges: Sequence[tuple[Chunk, Chunk]]) -> None:
+        for left, right in merges:
+            if not left.embedding_qwen or not right.embedding_qwen:
+                continue
+            score = self._cosine(left.embedding_qwen, right.embedding_qwen)
+            query = (
+                "MATCH (a:Chunk {id: $left}), (b:Chunk {id: $right}) "
+                "MERGE (a)-[r:SIMILAR_TO]->(b) "
+                "SET r.score = $score, r.model = $model, r.version = $version"
+            )
+            params = {
+                "left": left.chunk_id,
+                "right": right.chunk_id,
+                "score": score,
+                "model": self.similarity_model,
+                "version": self.similarity_version,
+            }
+            self.session.run(query, params)
+            inverse = (
+                "MATCH (a:Chunk {id: $left}), (b:Chunk {id: $right}) "
+                "MERGE (b)-[r:SIMILAR_TO]->(a) "
+                "SET r.score = $score, r.model = $model, r.version = $version"
+            )
+            self.session.run(inverse, params)
+
+    def _cosine(self, left: Sequence[float], right: Sequence[float]) -> float:
+        dot = sum(a * b for a, b in zip(left, right))
+        norm_left = math.sqrt(sum(a * a for a in left)) or 1.0
+        norm_right = math.sqrt(sum(b * b for b in right)) or 1.0
+        return dot / (norm_left * norm_right)
+
+
+__all__ = ["ChunkGraphWriter", "Neo4jSession"]
diff --git a/src/Medical_KG/chunking/opensearch.py b/src/Medical_KG/chunking/opensearch.py
new file mode 100644
index 0000000000000000000000000000000000000000..b9dbf85d1edf937734128cfae35b6ab07c37f258
--- /dev/null
+++ b/src/Medical_KG/chunking/opensearch.py
@@ -0,0 +1,148 @@
+"""OpenSearch index management for semantic chunks."""
+
+from __future__ import annotations
+
+from dataclasses import dataclass, field
+from typing import Iterable, Mapping, MutableSequence, Protocol, Sequence
+
+from .chunker import Chunk
+from .indexing import IndexedChunk
+
+
+class OpenSearchIndices(Protocol):  # pragma: no cover
+    def exists(self, index: str) -> bool: ...
+
+    def create(self, index: str, body: Mapping[str, object]) -> None: ...
+
+    def reload_search_analyzers(self, index: str) -> None: ...
+
+
+class OpenSearchClient(Protocol):  # pragma: no cover
+    @property
+    def indices(self) -> OpenSearchIndices: ...
+
+    def bulk(self, operations: Sequence[Mapping[str, object]]) -> Mapping[str, object]: ...
+
+
+@dataclass(slots=True)
+class ChunkSearchIndexer:
+    """Create and populate the chunks_v1 OpenSearch index."""
+
+    client: OpenSearchClient
+    index_name: str = "chunks_v1"
+    field_boosts: Mapping[str, float] = field(
+        default_factory=lambda: {
+            "title_path": 2.0,
+            "facet_json": 1.6,
+            "table_lines": 1.2,
+            "body": 1.0,
+        }
+    )
+
+    def ensure_index(self) -> None:
+        if self.client.indices.exists(self.index_name):
+            return
+        body = {
+            "settings": {
+                "similarity": {"default": {"type": "BM25"}},
+                "analysis": {
+                    "analyzer": {"chunk_bm25": {"tokenizer": "standard", "filter": ["lowercase"]}}
+                }
+            },
+            "mappings": {
+                "properties": {
+                    "id": {"type": "keyword"},
+                    "doc_id": {"type": "keyword"},
+                    "body": {"type": "text", "analyzer": "chunk_bm25"},
+                    "title_path": {"type": "text", "analyzer": "chunk_bm25"},
+                    "facet_json": {"type": "text", "analyzer": "chunk_bm25"},
+                    "facet_type": {"type": "keyword"},
+                    "granularity": {"type": "keyword"},
+                    "tokens": {"type": "integer"},
+                    "table_lines": {"type": "text", "analyzer": "chunk_bm25"},
+                    "embedding_qwen": {
+                        "type": "dense_vector",
+                        "dims": 4096,
+                        "index": True,
+                        "similarity": "cosine",
+                    },
+                    "splade_terms": {"type": "rank_features"},
+                }
+            },
+        }
+        self.client.indices.create(index=self.index_name, body=body)
+
+    def index_chunks(
+        self, base_chunks: Sequence[Chunk], multi_gran: Iterable[IndexedChunk]
+    ) -> None:
+        operations: MutableSequence[Mapping[str, object]] = []
+        for chunk in base_chunks:
+            operations.append({"index": {"_index": self.index_name, "_id": chunk.chunk_id}})
+            operations.append(self._serialise_chunk(chunk, granularity="chunk"))
+        for aggregate in multi_gran:
+            doc_id = "::".join(aggregate.chunk_ids)
+            operations.append({"index": {"_index": self.index_name, "_id": doc_id}})
+            operations.append(
+                {
+                    "id": doc_id,
+                    "doc_id": aggregate.doc_id,
+                    "body": aggregate.text,
+                    "title_path": aggregate.title_path,
+                    "facet_json": self._serialize_facet(aggregate.facet_json),
+                    "facet_type": aggregate.facet_type,
+                    "granularity": aggregate.granularity,
+                    "tokens": aggregate.tokens,
+                    "table_lines": self._combine_table_lines(aggregate.table_lines),
+                    "embedding_qwen": aggregate.embedding_qwen,
+                    "splade_terms": aggregate.splade_terms,
+                }
+            )
+        if operations:
+            self.client.bulk(operations)
+            self.client.indices.reload_search_analyzers(index=self.index_name)
+
+    def build_query(self, text: str) -> Mapping[str, object]:
+        fields = [f"{name}^{boost}" for name, boost in self.field_boosts.items()]
+        return {
+            "query": {
+                "bool": {
+                    "should": [
+                        {"multi_match": {"query": text, "fields": fields}},
+                        {"match": {"granularity": "paragraph"}},
+                    ]
+                }
+            }
+        }
+
+    def _serialise_chunk(self, chunk: Chunk, *, granularity: str) -> Mapping[str, object]:
+        return {
+            "id": chunk.chunk_id,
+            "doc_id": chunk.doc_id,
+            "body": chunk.text,
+            "title_path": chunk.title_path,
+            "facet_json": self._serialize_facet(chunk.facet_json),
+            "facet_type": chunk.facet_type,
+            "granularity": granularity,
+            "tokens": chunk.tokens,
+            "table_lines": self._combine_table_lines(chunk.table_lines),
+            "embedding_qwen": chunk.embedding_qwen,
+            "splade_terms": chunk.splade_terms or {},
+        }
+
+    def _serialize_facet(self, payload: Mapping[str, object] | None) -> str | None:
+        if not payload:
+            return None
+        try:
+            import json
+
+            return json.dumps(payload, sort_keys=True)
+        except (TypeError, ValueError):
+            return str(payload)
+
+    def _combine_table_lines(self, lines: Sequence[str] | None) -> str | None:
+        if not lines:
+            return None
+        return "\n".join(lines)
+
+
+__all__ = ["ChunkSearchIndexer", "OpenSearchClient"]
diff --git a/src/Medical_KG/chunking/pipeline.py b/src/Medical_KG/chunking/pipeline.py
index cecb59899151f0f5669ab464d9ec6d66bcd35f91..1db7b389ae53cd3a1501ad4f3937142edd49d543 100644
--- a/src/Medical_KG/chunking/pipeline.py
+++ b/src/Medical_KG/chunking/pipeline.py
@@ -1,36 +1,80 @@
 """Pipeline entrypoint for semantic chunking."""
+
 from __future__ import annotations

+import json
 from dataclasses import dataclass
-from typing import List
+from typing import TYPE_CHECKING, List

 from .chunker import Chunk, SemanticChunker, select_profile
 from .document import Document
 from .facets import FacetGenerator
+from .indexing import ChunkIndexer, IndexedChunk
 from .metrics import ChunkMetrics, compute_metrics
 from .profiles import ChunkingProfile

+if TYPE_CHECKING:  # pragma: no cover
+    from Medical_KG.embeddings.service import EmbeddingService
+

 @dataclass(slots=True)
 class ChunkingResult:
     chunks: List[Chunk]
     metrics: ChunkMetrics
+    index_documents: List[IndexedChunk]
+    neighbor_merges: List[tuple[Chunk, Chunk]]


 class ChunkingPipeline:
     """Run semantic chunking with profile selection and facet generation."""

-    def __init__(self, *, facet_generator: FacetGenerator | None = None) -> None:
+    def __init__(
+        self,
+        *,
+        facet_generator: FacetGenerator | None = None,
+        embedding_service: "EmbeddingService" | None = None,
+        indexer: ChunkIndexer | None = None,
+    ) -> None:
         self._facet_generator = facet_generator or FacetGenerator()
+        self._embedding_service = embedding_service
+        self._indexer = indexer or ChunkIndexer()

     def run(self, document: Document, *, profile: ChunkingProfile | None = None) -> ChunkingResult:
         profile = profile or select_profile(document)
         chunker = SemanticChunker(profile)
         chunks = chunker.chunk(document)
         for chunk in chunks:
             self._facet_generator.generate(chunk)
+        if self._embedding_service and chunks:
+            self._apply_embeddings(chunks)
         metrics = compute_metrics(chunks)
-        return ChunkingResult(chunks=chunks, metrics=metrics)
+        index_documents: List[IndexedChunk] = []
+        neighbor_merges: List[tuple[Chunk, Chunk]] = []
+        if self._indexer:
+            index_documents = self._indexer.build_documents(chunks)
+            neighbor_merges = self._indexer.neighbor_merge(chunks)
+        return ChunkingResult(
+            chunks=chunks,
+            metrics=metrics,
+            index_documents=index_documents,
+            neighbor_merges=neighbor_merges,
+        )
+
+    def _apply_embeddings(self, chunks: List[Chunk]) -> None:
+        texts = [chunk.to_embedding_text() for chunk in chunks]
+        dense_vectors, sparse_vectors = self._embedding_service.embed_texts(texts)
+        for chunk, dense, sparse in zip(chunks, dense_vectors, sparse_vectors):
+            chunk.embedding_qwen = dense
+            chunk.splade_terms = sparse
+        facet_payloads = [
+            json.dumps(chunk.facet_json, sort_keys=True) for chunk in chunks if chunk.facet_json
+        ]
+        if facet_payloads:
+            facet_vectors, _ = self._embedding_service.embed_texts(facet_payloads)
+            iterator = iter(facet_vectors)
+            for chunk in chunks:
+                if chunk.facet_json:
+                    chunk.facet_embedding_qwen = next(iterator)


 __all__ = ["ChunkingPipeline", "ChunkingResult"]
diff --git a/src/Medical_KG/chunking/tagger.py b/src/Medical_KG/chunking/tagger.py
index 5dabb13a1842523786ccb319cd136ed8abdaf919..4536dc915a5acecbe644f652b48f2d52b21e4d38 100644
--- a/src/Medical_KG/chunking/tagger.py
+++ b/src/Medical_KG/chunking/tagger.py
@@ -1,72 +1,112 @@
 """Lightweight clinical intent tagging based on heuristics."""
+
 from __future__ import annotations

 import re
 from collections import Counter
-from dataclasses import dataclass
+from dataclasses import dataclass, field
 from enum import Enum
-from typing import Iterable, List, Sequence
+from typing import Iterable, List, Mapping, Sequence
+
+from Medical_KG.embeddings import QwenEmbeddingClient


 class ClinicalIntent(str, Enum):
     PICO_POPULATION = "pico_population"
     PICO_INTERVENTION = "pico_intervention"
     PICO_OUTCOME = "pico_outcome"
     ADVERSE_EVENT = "adverse_event"
     DOSE = "dose"
     ELIGIBILITY = "eligibility"
     RECOMMENDATION = "recommendation"
     LAB_VALUE = "lab_value"
     ENDPOINT = "endpoint"
     GENERAL = "general"


+@dataclass(slots=True)
+class EmbeddingIntentClassifier:
+    """Lightweight classifier using hashed embeddings and weak supervision weights."""
+
+    client: QwenEmbeddingClient = field(
+        default_factory=lambda: QwenEmbeddingClient(dimension=32, batch_size=32)
+    )
+    weights: Mapping[ClinicalIntent, List[str]] = field(
+        default_factory=lambda: {
+            ClinicalIntent.PICO_OUTCOME: ["outcome", "response", "survival"],
+            ClinicalIntent.ADVERSE_EVENT: ["adverse", "toxicity", "serious"],
+            ClinicalIntent.PICO_INTERVENTION: ["dose", "treated", "administered"],
+            ClinicalIntent.PICO_POPULATION: ["patients", "subjects", "adults"],
+        }
+    )
+
+    def predict(self, sentence: str) -> ClinicalIntent | None:
+        embeddings = self.client.embed([sentence, sentence[::-1]])
+        scores: dict[ClinicalIntent, float] = {}
+        for intent, cues in self.weights.items():
+            score = sum(1.0 for cue in cues if cue in sentence.lower())
+            score += sum(value for value in embeddings[0][:4])
+            score -= sum(value for value in embeddings[1][:4])
+            scores[intent] = score
+        best_intent, best_score = max(scores.items(), key=lambda item: item[1])
+        if best_score > 0.5:
+            return best_intent
+        return None
+
+
 @dataclass(slots=True)
 class ClinicalIntentTagger:
-    """Simple heuristic tagger using keyword cues and section hints."""
+    """Hybrid heuristic and embedding-backed intent tagger."""
+
+    classifier: EmbeddingIntentClassifier = field(default_factory=EmbeddingIntentClassifier)

     def tag_sentence(self, sentence: str, *, section: str | None = None) -> ClinicalIntent:
         lowered = sentence.lower()
         if section:
             section_lower = section.lower()
             if "adverse" in section_lower:
                 return ClinicalIntent.ADVERSE_EVENT
             if "eligibility" in section_lower or "inclusion" in section_lower:
                 return ClinicalIntent.ELIGIBILITY
             if "outcome" in section_lower:
                 return ClinicalIntent.PICO_OUTCOME
             if "dosage" in section_lower or "dose" in section_lower:
                 return ClinicalIntent.DOSE
         if re.search(r"dose|mg|ml", lowered):
             return ClinicalIntent.DOSE
         if re.search(r"patients? aged|men and women|subjects with", lowered):
             return ClinicalIntent.PICO_POPULATION
         if re.search(r"randomized to|administered|received", lowered):
             return ClinicalIntent.PICO_INTERVENTION
         if re.search(r"hazard ratio|odds ratio|p=|confidence interval", lowered):
             return ClinicalIntent.ENDPOINT
         if re.search(r"adverse event|serious adverse", lowered):
             return ClinicalIntent.ADVERSE_EVENT
         if re.search(r"eligibility|exclusion|inclusion", lowered):
             return ClinicalIntent.ELIGIBILITY
         if re.search(r"recommend", lowered):
             return ClinicalIntent.RECOMMENDATION
         if re.search(r"laboratory|lab value|mmol|g/dl", lowered):
             return ClinicalIntent.LAB_VALUE
+        guess = self.classifier.predict(sentence)
+        if guess:
+            return guess
         return ClinicalIntent.GENERAL

-    def tag_sentences(self, sentences: Sequence[str], *, sections: Sequence[str] | None = None) -> List[ClinicalIntent]:
+    def tag_sentences(
+        self, sentences: Sequence[str], *, sections: Sequence[str] | None = None
+    ) -> List[ClinicalIntent]:
         intents = []
         sections = sections or [None] * len(sentences)
         for sentence, section in zip(sentences, sections):
             intents.append(self.tag_sentence(sentence, section=section))
         return intents

     def dominant_intent(self, intents: Iterable[ClinicalIntent]) -> ClinicalIntent:
         counter = Counter(intents)
         if not counter:
             return ClinicalIntent.GENERAL
         return counter.most_common(1)[0][0]


 __all__ = ["ClinicalIntent", "ClinicalIntentTagger"]
diff --git a/src/Medical_KG/embeddings/__init__.py b/src/Medical_KG/embeddings/__init__.py
index 413f956f3d6757db5b68fc0d373150558f16b8b8..83f7e31c5c9c614fc75dda4328dd0efe12417f9b 100644
--- a/src/Medical_KG/embeddings/__init__.py
+++ b/src/Medical_KG/embeddings/__init__.py
@@ -1,14 +1,28 @@
 """Embedding utilities for dense and sparse retrieval."""
-from .gpu import GPURequirementError, GPUValidator
+
+from .gpu import GPURequirementError, GPUValidator, enforce_gpu_or_exit
+from .monitoring import (
+    AlertSink,
+    BenchmarkResult,
+    EmbeddingPerformanceMonitor,
+    GPUStats,
+    LoadTestResult,
+)
 from .qwen import QwenEmbeddingClient
 from .service import EmbeddingMetrics, EmbeddingService
 from .splade import SPLADEExpander

 __all__ = [
     "EmbeddingMetrics",
     "EmbeddingService",
+    "AlertSink",
+    "BenchmarkResult",
+    "EmbeddingPerformanceMonitor",
     "GPURequirementError",
+    "GPUStats",
     "GPUValidator",
+    "LoadTestResult",
     "QwenEmbeddingClient",
     "SPLADEExpander",
+    "enforce_gpu_or_exit",
 ]
diff --git a/src/Medical_KG/embeddings/gpu.py b/src/Medical_KG/embeddings/gpu.py
index 02587cbabfc405d18adadde22e3393b1579d95e8..dba0ad390fc85624a5d9c4fc56b684bbb9113425 100644
--- a/src/Medical_KG/embeddings/gpu.py
+++ b/src/Medical_KG/embeddings/gpu.py
@@ -1,72 +1,97 @@
 """GPU enforcement utilities for embedding services."""
+
 from __future__ import annotations

 import importlib.util
 import os
 import subprocess
+import sys
 from dataclasses import dataclass
 from typing import Callable, Optional

 import httpx

 _torch_spec = importlib.util.find_spec("torch")
 if _torch_spec is not None:
     import torch  # type: ignore[import-not-found]
 else:  # pragma: no cover - fallback when torch unavailable
     torch = None  # type: ignore[assignment]


 class GPURequirementError(RuntimeError):
     """Raised when GPU preconditions are not satisfied."""


 @dataclass(slots=True)
 class GPUValidator:
     """Validate GPU availability, CUDA visibility, and vLLM health."""

     require_gpu_env: str = "REQUIRE_GPU"
     http_getter: Optional[Callable[[str], int]] = None

     def should_require_gpu(self) -> bool:
         value = os.environ.get(self.require_gpu_env, "1").lower()
         return value not in {"0", "false", "no"}

     def validate(self) -> None:
         if not self.should_require_gpu():
             return
         if torch is None or not torch.cuda.is_available():
-            raise GPURequirementError("GPU required for embeddings but torch.cuda.is_available() returned False")
+            raise GPURequirementError(
+                "GPU required for embeddings but torch.cuda.is_available() returned False"
+            )
         try:
             result = subprocess.run(
                 ["nvidia-smi", "--query-gpu=name", "--format=csv,noheader"],
                 check=True,
                 stdout=subprocess.PIPE,
                 stderr=subprocess.PIPE,
                 text=True,
                 timeout=5,
             )
         except FileNotFoundError as exc:  # pragma: no cover - extremely unlikely in tests
-            raise GPURequirementError("GPU required for embeddings but nvidia-smi is not available") from exc
+            raise GPURequirementError(
+                "GPU required for embeddings but nvidia-smi is not available"
+            ) from exc
         except subprocess.SubprocessError as exc:
             raise GPURequirementError("Failed to execute nvidia-smi for GPU validation") from exc
         names = [line.strip() for line in result.stdout.splitlines() if line.strip()]
         if not names:
-            raise GPURequirementError("GPU required for embeddings but no devices were reported by nvidia-smi")
+            raise GPURequirementError(
+                "GPU required for embeddings but no devices were reported by nvidia-smi"
+            )

     def validate_vllm(self, endpoint: str) -> None:
         if not self.should_require_gpu():
             return
         url = endpoint.rstrip("/") + "/health"
         status_code = self._get(url)
         if status_code != 200:
             raise GPURequirementError(f"vLLM health check at {url} returned status {status_code}")

-    def _get(self, url: str) -> int:
-        if self.http_getter:
-            return self.http_getter(url)
-        with httpx.Client(timeout=2.0) as client:
-            response = client.get(url)
-        return response.status_code
+
+def _get(self, url: str) -> int:
+    if self.http_getter:
+        return self.http_getter(url)
+    with httpx.Client(timeout=2.0) as client:
+        response = client.get(url)
+    return response.status_code
+
+
+def enforce_gpu_or_exit(
+    *, endpoint: str | None = None, validator: GPUValidator | None = None
+) -> None:
+    """Validate GPU availability and exit with code 99 on failure."""
+
+    validator = validator or GPUValidator()
+    try:
+        validator.validate()
+        if endpoint:
+            validator.validate_vllm(endpoint)
+    except GPURequirementError as exc:  # pragma: no cover - exercised in tests
+        message = f"GPU enforcement failed: {exc}"
+        print(message, file=sys.stderr)
+        raise SystemExit(99) from exc


-__all__ = ["GPURequirementError", "GPUValidator"]
+__all__ = ["GPURequirementError", "GPUValidator", "enforce_gpu_or_exit"]
diff --git a/src/Medical_KG/embeddings/monitoring.py b/src/Medical_KG/embeddings/monitoring.py
new file mode 100644
index 0000000000000000000000000000000000000000..3422ba8b29534a415fa1f21a6ccaf0a059a0258e
--- /dev/null
+++ b/src/Medical_KG/embeddings/monitoring.py
@@ -0,0 +1,212 @@
+"""Performance monitoring utilities for GPU embedding services."""
+
+from __future__ import annotations
+
+import itertools
+import statistics
+import subprocess
+import time
+from dataclasses import dataclass, field
+from typing import Mapping, Protocol, Sequence
+
+from .gpu import GPURequirementError, GPUValidator
+from .service import EmbeddingService
+
+
+class AlertSink(Protocol):  # pragma: no cover - interface definition
+    def emit(self, alert: str, message: str) -> None:
+        """Record an alert event."""
+
+
+@dataclass(slots=True)
+class GPUStats:
+    """Summarised GPU utilisation metrics."""
+
+    utilisation: float
+    memory_used_mb: float
+
+
+@dataclass(slots=True)
+class BenchmarkResult:
+    """Result of a single embedding benchmark run."""
+
+    throughput_per_minute: float
+    dense_tokens_per_second: float
+    sparse_terms_per_second: float
+    batch_size: int
+    duration_seconds: float
+
+
+@dataclass(slots=True)
+class LoadTestResult:
+    """Result of a synthetic load test run."""
+
+    total_chunks: int
+    failed_chunks: int
+    duration_seconds: float
+    throughput_per_minute: float
+    latency_p95_ms: float
+    latency_samples_ms: list[float] = field(default_factory=list)
+
+
+@dataclass(slots=True)
+class EmbeddingPerformanceMonitor:
+    """Coordinate benchmarking, health checks, and alerting for embeddings."""
+
+    service: EmbeddingService
+    gpu_validator: GPUValidator | None = None
+    alert_sink: AlertSink | None = None
+
+    def benchmark_embeddings(self, sample_texts: Sequence[str]) -> BenchmarkResult:
+        """Run a single benchmark embedding call and capture throughput metrics."""
+
+        if not sample_texts:
+            return BenchmarkResult(0.0, 0.0, 0.0, 0, 0.0)
+        start = time.perf_counter()
+        self.service.embed_texts(sample_texts)
+        duration = max(time.perf_counter() - start, 1e-6)
+        throughput = (len(sample_texts) / duration) * 60.0
+        metrics = self.service.metrics
+        return BenchmarkResult(
+            throughput_per_minute=throughput,
+            dense_tokens_per_second=metrics.dense_tokens_per_second,
+            sparse_terms_per_second=metrics.sparse_terms_per_second,
+            batch_size=metrics.dense_batch_size,
+            duration_seconds=duration,
+        )
+
+    def collect_gpu_stats(self) -> GPUStats:
+        """Collect aggregate GPU utilisation metrics via nvidia-smi."""
+
+        command = [
+            "nvidia-smi",
+            "--query-gpu=utilization.gpu,memory.used",
+            "--format=csv,noheader,nounits",
+        ]
+        result = subprocess.run(  # pragma: no cover - executed via tests with monkeypatch
+            command,
+            check=True,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.PIPE,
+            text=True,
+            timeout=3,
+        )
+        rows = [line.strip() for line in result.stdout.splitlines() if line.strip()]
+        if not rows:
+            return GPUStats(utilisation=0.0, memory_used_mb=0.0)
+        utilisations = []
+        memories = []
+        for row in rows:
+            parts = [part.strip() for part in row.split(",") if part.strip()]
+            if len(parts) >= 2:
+                utilisations.append(float(parts[0]))
+                memories.append(float(parts[1]))
+        utilisation = sum(utilisations) / len(utilisations) if utilisations else 0.0
+        memory = sum(memories) / len(memories) if memories else 0.0
+        return GPUStats(utilisation=utilisation, memory_used_mb=memory)
+
+    def monitor_health(self, *, endpoint: str | None = None) -> None:
+        """Validate GPU and vLLM availability, emitting alerts on failure."""
+
+        validator = self.gpu_validator or self.service.gpu_validator or GPUValidator()
+        try:
+            validator.validate()
+            if endpoint:
+                validator.validate_vllm(endpoint)
+        except GPURequirementError as exc:  # pragma: no cover - tested via monkeypatch
+            self._emit_alert("gpu_unavailable", str(exc))
+            raise
+
+    def check_throughput(self, benchmark: BenchmarkResult, *, threshold: float) -> None:
+        """Emit alert when throughput drops below configured threshold."""
+
+        if benchmark.throughput_per_minute < threshold:
+            message = (
+                f"Embedding throughput {benchmark.throughput_per_minute:.1f} chunks/min "
+                f"below threshold {threshold:.1f}"
+            )
+            self._emit_alert("throughput_low", message)
+
+    def run_load_test(
+        self,
+        sample_texts: Sequence[str],
+        *,
+        target_chunks: int = 10_000,
+        batch_size: int | None = None,
+    ) -> LoadTestResult:
+        """Drive repeated embedding calls to estimate sustained throughput."""
+
+        if not sample_texts:
+            return LoadTestResult(0, 0, 0.0, 0.0, 0.0)
+        batch = batch_size or getattr(self.service.qwen, "batch_size", 256)
+        latencies: list[float] = []
+        total = 0
+        failed = 0
+        start = time.perf_counter()
+        iterator = itertools.cycle(sample_texts)
+        while total < target_chunks:
+            remaining = target_chunks - total
+            current_batch_size = min(batch, remaining)
+            batch_texts = [next(iterator) for _ in range(current_batch_size)]
+            attempt_start = time.perf_counter()
+            try:
+                self.service.embed_texts(batch_texts)
+                total += current_batch_size
+            except Exception:  # pragma: no cover - surfaced in tests via monkeypatch
+                failed += current_batch_size
+            latency = (time.perf_counter() - attempt_start) * 1000.0
+            latencies.append(latency)
+        duration = max(time.perf_counter() - start, 1e-6)
+        throughput = (total / duration) * 60.0 if duration else 0.0
+        if latencies:
+            if len(latencies) >= 2:
+                p95 = statistics.quantiles(latencies, n=100, method="inclusive")[94]
+            else:
+                p95 = latencies[0]
+        else:
+            p95 = 0.0
+        return LoadTestResult(
+            total_chunks=total,
+            failed_chunks=failed,
+            duration_seconds=duration,
+            throughput_per_minute=throughput,
+            latency_p95_ms=p95,
+            latency_samples_ms=latencies,
+        )
+
+    def dashboard_definition(self) -> Mapping[str, object]:
+        """Return Grafana-style dashboard metadata for documentation/export."""
+
+        return {
+            "title": "Embedding GPU Performance",
+            "panels": [
+                {
+                    "title": "Embedding Throughput",
+                    "metric": "embedding_throughput_chunks_per_minute",
+                    "description": "Chunks processed per minute from benchmark/load test",
+                },
+                {
+                    "title": "GPU Utilisation",
+                    "metric": "gpu_utilisation_percent",
+                    "description": "Average GPU utilisation collected via nvidia-smi",
+                },
+                {
+                    "title": "SPLADE Terms/sec",
+                    "metric": "splade_terms_per_second",
+                    "description": "Sparse expansion rate from embedding metrics",
+                },
+            ],
+        }
+
+    def _emit_alert(self, alert: str, message: str) -> None:
+        if self.alert_sink:
+            self.alert_sink.emit(alert, message)
+
+
+__all__ = [
+    "AlertSink",
+    "BenchmarkResult",
+    "EmbeddingPerformanceMonitor",
+    "GPUStats",
+    "LoadTestResult",
+]
diff --git a/src/Medical_KG/embeddings/qwen.py b/src/Medical_KG/embeddings/qwen.py
index 80db632c881072c649eec042a8ac29c6d7bee3cb..068ffa74a519fcd616477f629b3f3f9014042768 100644
--- a/src/Medical_KG/embeddings/qwen.py
+++ b/src/Medical_KG/embeddings/qwen.py
@@ -1,45 +1,88 @@
 """Deterministic client for Qwen embedding service (test-friendly)."""
+
 from __future__ import annotations

 import hashlib
 import math
 import random
+import time
 from dataclasses import dataclass
 from typing import Callable, List, Sequence

+import httpx
+

 @dataclass(slots=True)
 class QwenEmbeddingClient:
     """Client producing deterministic Qwen-style embeddings for tests."""

     model: str = "Qwen3-Embedding-8B"
     dimension: int = 4096
     batch_size: int = 256
     transport: Callable[[Sequence[str]], List[List[float]]] | None = None
+    api_url: str | None = None
+    timeout: float = 10.0
+    max_retries: int = 3
+    http_client_factory: Callable[[], httpx.Client] | None = None
+    sleep: Callable[[float], None] = time.sleep

     def embed(self, texts: Sequence[str]) -> List[List[float]]:
         """Embed a batch of texts, splitting into model-sized batches."""

         outputs: List[List[float]] = []
         for start in range(0, len(texts), self.batch_size):
             chunk = texts[start : start + self.batch_size]
             outputs.extend(self._embed_chunk(chunk))
         return outputs

     def _embed_chunk(self, texts: Sequence[str]) -> List[List[float]]:
         if self.transport:
             return self.transport(texts)
+        if self.api_url:
+            return self._embed_via_http(texts)
         vectors = [self._deterministic_vector(text) for text in texts]
         return [self._normalise(vector) for vector in vectors]

+    def _embed_via_http(self, texts: Sequence[str]) -> List[List[float]]:
+        payload = {"model": self.model, "input": list(texts)}
+        attempt = 0
+        last_error: Exception | None = None
+        while attempt < self.max_retries:
+            attempt += 1
+            try:
+                client = (
+                    self.http_client_factory()
+                    if self.http_client_factory
+                    else httpx.Client(timeout=self.timeout)
+                )
+                try:
+                    response = client.post(self.api_url, json=payload)
+                    response.raise_for_status()
+                    data = response.json()
+                    vectors = [item["embedding"] for item in data.get("data", [])]
+                    if len(vectors) != len(texts):
+                        raise ValueError("embedding service returned unexpected vector count")
+                    return vectors
+                finally:
+                    if self.http_client_factory is None:
+                        client.close()
+            except Exception as exc:  # pragma: no cover - exercised via retries in tests
+                last_error = exc
+                if attempt >= self.max_retries:
+                    raise
+                self.sleep(0.2 * attempt)
+        if last_error:
+            raise last_error
+        return []
+
     def _deterministic_vector(self, text: str) -> List[float]:
         seed = hashlib.sha256((self.model + text).encode("utf-8")).digest()
         rnd = random.Random(seed)
         return [rnd.uniform(-1.0, 1.0) for _ in range(self.dimension)]

     def _normalise(self, vector: Sequence[float]) -> List[float]:
         norm = math.sqrt(sum(value * value for value in vector)) or 1.0
         return [value / norm for value in vector]


 __all__ = ["QwenEmbeddingClient"]
diff --git a/src/Medical_KG/embeddings/service.py b/src/Medical_KG/embeddings/service.py
index 51cacc25b4cb00ae5b03e3667f5b4a909d27b59d..843469a276450095e9ef444856b57ea8251ccb62 100644
--- a/src/Medical_KG/embeddings/service.py
+++ b/src/Medical_KG/embeddings/service.py
@@ -1,54 +1,61 @@
 """High-level embedding orchestration for dense and sparse representations."""
+
 from __future__ import annotations

 import time
 from dataclasses import dataclass, field
 from typing import List, Sequence

+from .gpu import GPUValidator
 from .qwen import QwenEmbeddingClient
 from .splade import SPLADEExpander


 @dataclass(slots=True)
 class EmbeddingMetrics:
     """Capture simple throughput metrics for embedding operations."""

     dense_tokens_per_second: float = 0.0
     dense_batch_size: int = 0
     sparse_terms_per_second: float = 0.0


 @dataclass(slots=True)
 class EmbeddingService:
     """Combine dense (Qwen) and sparse (SPLADE) embedding backends."""

     qwen: QwenEmbeddingClient
     splade: SPLADEExpander
     metrics: EmbeddingMetrics = field(default_factory=EmbeddingMetrics)
+    gpu_validator: GPUValidator | None = None

     def embed_texts(self, texts: Sequence[str]) -> tuple[List[List[float]], List[dict[str, float]]]:
+        if not texts:
+            return [], []
+        if self.gpu_validator:
+            self.gpu_validator.validate()
         dense_start = time.perf_counter()
         dense_vectors = self.qwen.embed(texts)
         dense_duration = max(time.perf_counter() - dense_start, 1e-6)
         total_tokens = sum(len(text.split()) for text in texts)
         self.metrics.dense_tokens_per_second = total_tokens / dense_duration
         self.metrics.dense_batch_size = max(len(texts), 1)

         sparse_start = time.perf_counter()
         sparse_vectors = self.splade.expand(texts)
         sparse_duration = max(time.perf_counter() - sparse_start, 1e-6)
         total_terms = sum(len(terms) for terms in sparse_vectors)
         self.metrics.sparse_terms_per_second = total_terms / sparse_duration if total_terms else 0.0

         return dense_vectors, sparse_vectors

     def embed_concepts(self, concepts: Sequence["ConceptLike"]) -> None:
         texts = [concept.to_embedding_text() for concept in concepts]
         dense_vectors, sparse_vectors = self.embed_texts(texts)
         for concept, dense, sparse in zip(concepts, dense_vectors, sparse_vectors):
             concept.embedding_qwen = dense
             concept.splade_terms = sparse


 class ConceptLike:
     """Protocol-like runtime type used for duck typing in embedding service."""
diff --git a/src/Medical_KG/embeddings/splade.py b/src/Medical_KG/embeddings/splade.py
index e3fd0e9f427ccefff6faa9f20bbeaaaf27a4cba9..91800400eaf6e0988f68307fa982b246df8263b0 100644
--- a/src/Medical_KG/embeddings/splade.py
+++ b/src/Medical_KG/embeddings/splade.py
@@ -1,37 +1,45 @@
 """SPLADE-style sparse expansion implemented with lightweight heuristics."""
+
 from __future__ import annotations

 import math
 import re
 from collections import Counter
 from dataclasses import dataclass
 from typing import Dict, List, Sequence

 _TOKEN_PATTERN = re.compile(r"[A-Za-z0-9]+")


 @dataclass(slots=True)
 class SPLADEExpander:
     """Approximate SPLADE expansion using token statistics."""

     top_k: int = 400
     min_weight: float = 0.05
+    batch_size: int = 64

     def expand(self, texts: Sequence[str]) -> List[Dict[str, float]]:
         expansions: List[Dict[str, float]] = []
-        for text in texts:
-            tokens = [token.lower() for token in _TOKEN_PATTERN.findall(text)]
-            counts = Counter(tokens)
-            if not counts:
-                expansions.append({})
-                continue
-            weighted = {token: 1.0 + math.log(count) for token, count in counts.items()}
-            norm = math.sqrt(sum(value * value for value in weighted.values())) or 1.0
-            scaled = {token: value / norm for token, value in weighted.items()}
-            filtered = {token: value for token, value in scaled.items() if value >= self.min_weight}
-            top_terms = dict(sorted(filtered.items(), key=lambda item: item[1], reverse=True)[: self.top_k])
-            expansions.append(top_terms)
+        for start in range(0, len(texts), self.batch_size):
+            batch = texts[start : start + self.batch_size]
+            for text in batch:
+                tokens = [token.lower() for token in _TOKEN_PATTERN.findall(text)]
+                counts = Counter(tokens)
+                if not counts:
+                    expansions.append({})
+                    continue
+                weighted = {token: 1.0 + math.log(count) for token, count in counts.items()}
+                norm = math.sqrt(sum(value * value for value in weighted.values())) or 1.0
+                scaled = {token: value / norm for token, value in weighted.items()}
+                filtered = {
+                    token: value for token, value in scaled.items() if value >= self.min_weight
+                }
+                top_terms = dict(
+                    sorted(filtered.items(), key=lambda item: item[1], reverse=True)[: self.top_k]
+                )
+                expansions.append(top_terms)
         return expansions


 __all__ = ["SPLADEExpander"]
diff --git a/tests/catalog/test_concept_catalog.py b/tests/catalog/test_concept_catalog.py
index 2caab209026458dae893962311d1e762621b8fd9..2c01ef500a0e0e8be582297b18b4679b68525c0f 100644
--- a/tests/catalog/test_concept_catalog.py
+++ b/tests/catalog/test_concept_catalog.py
@@ -1,36 +1,44 @@
+from pathlib import Path
+from typing import Mapping, Sequence
+
 import pytest

 from Medical_KG.catalog import (
     VALIDATORS,
     CatalogBuildResult,
+    CatalogStateStore,
+    CatalogUpdater,
     ConceptCatalogBuilder,
     ConceptFamily,
+    ConceptGraphWriter,
+    ConceptIndexManager,
     ConceptSchemaValidator,
     LicensePolicy,
     MONDOLoader,
     SnomedCTLoader,
+    load_license_policy,
 )
 from Medical_KG.embeddings import EmbeddingService, QwenEmbeddingClient, SPLADEExpander


 @pytest.fixture()
 def snomed_loader() -> SnomedCTLoader:
     records = [
         {
             "conceptId": "73211009",
             "fsn": "Diabetes mellitus (disorder)",
             "preferred": "Diabetes mellitus",
             "synonyms": ["Sugar diabetes"],
             "definition": "A disorder characterized by hyperglycemia.",
             "parents": ["237602007"],
             "ancestors": ["64572001"],
             "icd10": ["E11"],
             "active": True,
         },
         {
             "conceptId": "44054006",
             "fsn": "Diabetes mellitus type 2 (disorder)",
             "preferred": "Type 2 diabetes mellitus",
             "synonyms": ["Non-insulin-dependent diabetes mellitus"],
             "definition": "A type of diabetes mellitus.",
             "parents": ["73211009"],
@@ -38,76 +46,233 @@ def snomed_loader() -> SnomedCTLoader:
             "icd10": ["E11"],
             "active": True,
         },
     ]
     return SnomedCTLoader(records)


 @pytest.fixture()
 def mondo_loader() -> MONDOLoader:
     nodes = [
         {
             "id": "MONDO:0005148",
             "label": "diabetes mellitus",
             "synonyms": ["Diabetes"],
             "definition": "A metabolic disease.",
             "xrefs": {"snomed": ["73211009"]},
             "preferred": "Diabetes mellitus",
             "format": "json",
         }
     ]
     return MONDOLoader(nodes)


 @pytest.fixture()
 def embedding_service() -> EmbeddingService:
-    return EmbeddingService(qwen=QwenEmbeddingClient(dimension=16, batch_size=16), splade=SPLADEExpander(top_k=8))
+    return EmbeddingService(
+        qwen=QwenEmbeddingClient(dimension=16, batch_size=16),
+        splade=SPLADEExpander(top_k=8, batch_size=8),
+    )
+
+
+@pytest.fixture()
+def state_store() -> CatalogStateStore:
+    return CatalogStateStore()


 def test_concepts_validate_against_schema(snomed_loader: SnomedCTLoader) -> None:
     validator = ConceptSchemaValidator.create()
     concepts = list(snomed_loader.load())
     assert concepts
     for concept in concepts:
         validator.validate(concept)
         assert concept.family == ConceptFamily.CONDITION


 def test_builder_deduplicates_and_creates_crosswalks(
     snomed_loader: SnomedCTLoader,
     mondo_loader: MONDOLoader,
     embedding_service: EmbeddingService,
+    state_store: CatalogStateStore,
 ) -> None:
-    builder = ConceptCatalogBuilder([snomed_loader, mondo_loader], embedding_service=embedding_service)
+    builder = ConceptCatalogBuilder(
+        [snomed_loader, mondo_loader],
+        embedding_service=embedding_service,
+        state_store=state_store,
+    )
     result = builder.build()
     assert isinstance(result, CatalogBuildResult)
     assert result.concepts
     diabetes = next(concept for concept in result.concepts if "Diabetes mellitus" in concept.label)
     assert diabetes.same_as
     assert any("mondo" in iri.lower() for iri in diabetes.same_as)
     assert diabetes.embedding_qwen is not None
     assert diabetes.splade_terms is not None
     assert result.synonym_catalog["SNOMED"]
     assert len(result.release_hash) == 64
+    assert not result.skipped
+    assert result.changed_ontologies


 def test_license_policy_skips_restricted_loader(
     snomed_loader: SnomedCTLoader,
     mondo_loader: MONDOLoader,
 ) -> None:
-    policy = LicensePolicy(entitlements={"open": True, "permissive": True, "restricted": False, "proprietary": False})
+    policy = LicensePolicy(
+        entitlements={"open": True, "permissive": True, "restricted": False, "proprietary": False}
+    )
     builder = ConceptCatalogBuilder([snomed_loader, mondo_loader], license_policy=policy)
     result = builder.build()
     assert all(concept.ontology != "SNOMED" for concept in result.concepts)
     assert result.audit_log.entries
-    skipped = next(entry for entry in result.audit_log.entries if entry["action"] == "loader.skipped")
+    skipped = next(
+        entry for entry in result.audit_log.entries if entry["action"] == "loader.skipped"
+    )
     assert skipped["resource"] == "SNOMED"


+def test_license_policy_from_file(tmp_path: Path) -> None:
+    config = tmp_path / "licenses.yml"
+    config.write_text(
+        """
+        buckets:
+          restricted: false
+          proprietary: false
+        loaders:
+          SNOMED:
+            enabled: false
+        """
+    )
+    policy = load_license_policy(config)
+
+    class DummyLoader:
+        ontology = "SNOMED"
+        license_bucket = "restricted"
+
+    assert policy.is_loader_enabled(DummyLoader()) is False
+
+
 def test_identifier_validators() -> None:
     assert VALIDATORS["nct"]("NCT01234567")
     assert VALIDATORS["pmid"]("123456")
     assert VALIDATORS["doi"]("10.1000/xyz123")
     assert VALIDATORS["loinc"]("1234-5")
     assert VALIDATORS["gtin14"]("01234567890128")
     assert VALIDATORS["unii"]("ABCDEF1234")
     assert VALIDATORS["snomed"]("1234567") is False
+
+
+def test_catalog_state_store_idempotency(
+    snomed_loader: SnomedCTLoader,
+    mondo_loader: MONDOLoader,
+    embedding_service: EmbeddingService,
+    state_store: CatalogStateStore,
+) -> None:
+    builder = ConceptCatalogBuilder(
+        [snomed_loader, mondo_loader],
+        embedding_service=embedding_service,
+        state_store=state_store,
+    )
+    first = builder.build()
+    assert not first.skipped
+    second = builder.build()
+    assert second.skipped
+
+
+class FakeSession:
+    def __init__(self) -> None:
+        self.queries: list[tuple[str, Mapping[str, object] | None]] = []
+
+    def run(self, query: str, parameters: Mapping[str, object] | None = None) -> None:
+        self.queries.append((query, dict(parameters or {})))
+
+
+class FakeIndices:
+    def __init__(self) -> None:
+        self.created: list[dict[str, object]] = []
+        self.updated: list[dict[str, object]] = []
+        self.reloads: int = 0
+        self._exists = False
+
+    def exists(self, index: str) -> bool:
+        return self._exists
+
+    def create(self, index: str, body: Mapping[str, object]) -> None:
+        self._exists = True
+        self.created.append({"index": index, "body": body})
+
+    def put_settings(self, index: str, body: Mapping[str, object]) -> None:
+        self.updated.append({"index": index, "body": body})
+
+    def reload_search_analyzers(self, index: str) -> None:
+        self.reloads += 1
+
+
+class FakeOpenSearchClient:
+    def __init__(self) -> None:
+        self.indices = FakeIndices()
+        self.bulk_operations: list[Sequence[Mapping[str, object]]] = []
+
+    def bulk(self, operations: Sequence[Mapping[str, object]]) -> Mapping[str, object]:
+        self.bulk_operations.append(list(operations))
+        return {"errors": False}
+
+
+def test_concept_graph_writer_creates_nodes(
+    snomed_loader: SnomedCTLoader,
+    embedding_service: EmbeddingService,
+    state_store: CatalogStateStore,
+) -> None:
+    builder = ConceptCatalogBuilder(
+        [snomed_loader], embedding_service=embedding_service, state_store=state_store
+    )
+    result = builder.build()
+    session = FakeSession()
+    writer = ConceptGraphWriter(session)
+    writer.sync(result)
+    assert any("MERGE (c:Concept" in query for query, _ in session.queries)
+    assert any("CALL db.index.vector.createNodeIndex" in query for query, _ in session.queries)
+
+
+def test_concept_index_manager_indexes_documents(
+    snomed_loader: SnomedCTLoader,
+    embedding_service: EmbeddingService,
+    state_store: CatalogStateStore,
+) -> None:
+    builder = ConceptCatalogBuilder(
+        [snomed_loader], embedding_service=embedding_service, state_store=state_store
+    )
+    result = builder.build()
+    client = FakeOpenSearchClient()
+    manager = ConceptIndexManager(client)
+    manager.ensure_index(result.synonym_catalog)
+    manager.index_concepts(result.concepts)
+    query = manager.build_search_query("diabetes")
+    assert client.indices.created
+    assert client.bulk_operations
+    assert "multi_match" in query["query"]
+
+
+def test_catalog_updater_refreshes_changed_ontologies(
+    snomed_loader: SnomedCTLoader,
+    mondo_loader: MONDOLoader,
+    embedding_service: EmbeddingService,
+    state_store: CatalogStateStore,
+) -> None:
+    builder = ConceptCatalogBuilder(
+        [snomed_loader, mondo_loader],
+        embedding_service=embedding_service,
+        state_store=state_store,
+    )
+    session = FakeSession()
+    writer = ConceptGraphWriter(session)
+    client = FakeOpenSearchClient()
+    manager = ConceptIndexManager(client)
+    updater = CatalogUpdater(
+        builder=builder, graph_writer=writer, index_manager=manager, state_store=state_store
+    )
+    result = updater.refresh(force=True)
+    assert result.changed_ontologies
+    assert session.queries
+    assert client.bulk_operations
+    skipped = updater.refresh()
+    assert skipped.skipped
diff --git a/tests/chunking/test_semantic_chunker.py b/tests/chunking/test_semantic_chunker.py
index 8cbefb5a7017f1d146122f08bbfd8c5118d0cb31..3aa91c9928e342cbebbc0ca032fa1b9755372328 100644
--- a/tests/chunking/test_semantic_chunker.py
+++ b/tests/chunking/test_semantic_chunker.py
@@ -1,61 +1,219 @@
 from __future__ import annotations

+import json
+from typing import Sequence
+
+import pytest
+
 from Medical_KG.chunking import (
+    ChunkGraphWriter,
+    ChunkIndexer,
     ChunkingPipeline,
+    ChunkSearchIndexer,
     Document,
     FacetGenerator,
     Section,
     Table,
 )
+from Medical_KG.embeddings import EmbeddingService, QwenEmbeddingClient, SPLADEExpander


 def build_document() -> Document:
     text = (
         "## Introduction Patients with diabetes mellitus often require glucose monitoring. "
         "## Methods Participants received metformin 500 mg twice daily. "
         "Table 1: Laboratory results [glucose 7.2 mmol/L]. "
-        "## Results The hazard ratio was 0.76 with 95% CI 0.61-0.95."
+        "## Results The hazard ratio was 0.76 with 95% CI 0.61-0.95 with no significant difference observed."
     )
     intro_end = text.index("## Methods")
     methods_end = text.index("Table 1")
     results_start = text.index("## Results")
     sections = [
         Section(name="introduction", start=0, end=intro_end),
         Section(name="methods", start=intro_end, end=methods_end),
         Section(name="results", start=results_start, end=len(text)),
     ]
     table_start = text.index("Table 1")
     table_end = table_start + len("Table 1: Laboratory results [glucose 7.2 mmol/L]. ")
     tables = [
         Table(
             html="<table><tr><td>glucose</td><td>7.2 mmol/L</td></tr></table>",
-            digest="glucose:7.2 mmol/L",
+            digest="",
             start=table_start,
             end=table_end,
         )
     ]
-    return Document(doc_id="DOC123", text=text, sections=sections, tables=tables, source_system="pmc", media_type="text")
+    return Document(
+        doc_id="DOC123",
+        text=text,
+        sections=sections,
+        tables=tables,
+        source_system="pmc",
+        media_type="text",
+    )
+
+
+@pytest.fixture()
+def embedding_service() -> EmbeddingService:
+    return EmbeddingService(
+        qwen=QwenEmbeddingClient(dimension=12, batch_size=8),
+        splade=SPLADEExpander(top_k=6, batch_size=4),
+    )


-def test_chunking_pipeline_generates_chunks_and_metrics() -> None:
+def test_chunking_pipeline_generates_chunks_and_metrics(
+    embedding_service: EmbeddingService,
+) -> None:
     document = build_document()
-    pipeline = ChunkingPipeline(facet_generator=FacetGenerator())
+    pipeline = ChunkingPipeline(
+        facet_generator=FacetGenerator(), embedding_service=embedding_service
+    )
     result = pipeline.run(document)
     assert result.chunks
     assert result.metrics.intra_coherence > 0
-    chunk_ids = {chunk.chunk_id for chunk in result.chunks}
-    assert len(chunk_ids) == len(result.chunks)
+    assert result.metrics.inter_coherence >= 0
+    if len(result.chunks) > 10:
+        assert result.metrics.below_min_tokens <= int(len(result.chunks) * 0.1)
+    assert result.index_documents
+    assert any(doc.title_path for doc in result.index_documents if doc.granularity == "chunk")
     table_chunk = next(chunk for chunk in result.chunks if chunk.table_html is not None)
     assert "glucose" in table_chunk.table_digest
+    assert table_chunk.table_lines and any("glucose" in line for line in table_chunk.table_lines)
     endpoint_chunk = next(chunk for chunk in result.chunks if chunk.facet_type == "endpoint")
     assert endpoint_chunk.facet_json["metric"].lower() == "hazard ratio"
-    dose_chunk = next(chunk for chunk in result.chunks if chunk.intent.value == "dose")
-    assert dose_chunk.facet_type == "dose"
+    assert endpoint_chunk.facet_embedding_qwen is not None
+    assert endpoint_chunk.facet_json["negated"] is True
+    assert all(chunk.embedding_qwen for chunk in result.chunks)
+    assert all(chunk.created_at.tzinfo is not None for chunk in result.chunks)
+    assert any(chunk.title_path for chunk in result.chunks if chunk.section)
+    overlaps = [chunk for chunk in result.chunks if chunk.overlap_with_prev]
+    if overlaps:
+        assert {"chunk_id", "start", "end", "token_window"} <= overlaps[0].overlap_with_prev.keys()


-def test_chunk_ids_are_stable() -> None:
+def test_chunk_ids_are_stable(embedding_service: EmbeddingService) -> None:
     document = build_document()
-    pipeline = ChunkingPipeline()
+    pipeline = ChunkingPipeline(embedding_service=embedding_service)
     first = pipeline.run(document).chunks
     second = pipeline.run(document).chunks
     assert [chunk.chunk_id for chunk in first] == [chunk.chunk_id for chunk in second]
+
+
+def test_chunk_indexer_produces_multi_granularity(embedding_service: EmbeddingService) -> None:
+    document = build_document()
+    pipeline = ChunkingPipeline(embedding_service=embedding_service)
+    chunks = pipeline.run(document).chunks
+    indexer = ChunkIndexer()
+    docs = indexer.build_documents(chunks)
+    granularities = {doc.granularity for doc in docs}
+    assert {"chunk", "paragraph", "section"}.issubset(granularities)
+    merges = indexer.neighbor_merge(chunks, min_cosine=0.0)
+    assert merges
+
+
+def test_neighbor_merge_avoids_low_similarity(embedding_service: EmbeddingService) -> None:
+    document = build_document()
+    pipeline = ChunkingPipeline(embedding_service=embedding_service)
+    chunks = pipeline.run(document).chunks
+    indexer = ChunkIndexer()
+    merges = indexer.neighbor_merge(chunks, min_cosine=0.95)
+    assert not merges  # strict threshold prevents merges
+
+
+def test_guardrails_keep_list_items_together(embedding_service: EmbeddingService) -> None:
+    doc = Document(
+        doc_id="DOC999",
+        text="- Primary endpoint was HR 0.8; - Secondary endpoint was OR 1.1.",
+        sections=[Section(name="results", start=0, end=70)],
+    )
+    pipeline = ChunkingPipeline(embedding_service=embedding_service)
+    chunks = pipeline.run(doc).chunks
+    assert len(chunks) == 1
+    assert "Primary endpoint" in chunks[0].text
+
+
+def test_facet_json_serialises_negation() -> None:
+    document = build_document()
+    pipeline = ChunkingPipeline()
+    chunk = next(chunk for chunk in pipeline.run(document).chunks if chunk.facet_type == "endpoint")
+    serialised = json.dumps(chunk.facet_json)
+    assert "metric" in serialised
+
+
+class FakeChunkSession:
+    def __init__(self) -> None:
+        self.calls: list[tuple[str, dict[str, object] | None]] = []
+
+    def run(self, query: str, parameters: dict[str, object] | None = None) -> None:
+        self.calls.append((query, parameters))
+
+
+class FakeChunkIndices:
+    def __init__(self) -> None:
+        self.created: list[dict[str, object]] = []
+        self.reloads = 0
+        self._exists = False
+
+    def exists(self, index: str) -> bool:
+        return self._exists
+
+    def create(self, index: str, body: dict[str, object]) -> None:
+        self._exists = True
+        self.created.append({"index": index, "body": body})
+
+    def reload_search_analyzers(self, index: str) -> None:
+        self.reloads += 1
+
+
+class FakeChunkClient:
+    def __init__(self) -> None:
+        self.indices = FakeChunkIndices()
+        self.bulk_operations: list[list[dict[str, object]]] = []
+
+    def bulk(self, operations: Sequence[dict[str, object]]) -> dict[str, object]:
+        self.bulk_operations.append(list(operations))
+        return {"errors": False}
+
+
+def test_chunk_graph_writer_syncs_embeddings(embedding_service: EmbeddingService) -> None:
+    document = build_document()
+    pipeline = ChunkingPipeline(embedding_service=embedding_service)
+    result = pipeline.run(document)
+    chunks = result.chunks
+    session = FakeChunkSession()
+    writer = ChunkGraphWriter(session)
+    writer.sync(document.doc_id, chunks, neighbor_merges=result.neighbor_merges)
+    queries = [query for query, _ in session.calls]
+    assert any("MERGE (c:Chunk" in query for query in queries)
+    assert any("CALL db.index.vector.createNodeIndex" in query for query in queries)
+    assert any("OVERLAPS" in query for query in queries)
+    assert any("SIMILAR_TO" in query for query in queries)
+    has_chunk = next(
+        params for query, params in session.calls if query.strip().startswith("MERGE (d:Document")
+    )
+    assert isinstance(has_chunk["index"], int)
+
+
+def test_chunk_search_indexer_indexes_multi_granularity(
+    embedding_service: EmbeddingService,
+) -> None:
+    document = build_document()
+    pipeline = ChunkingPipeline(embedding_service=embedding_service)
+    result = pipeline.run(document)
+    client = FakeChunkClient()
+    indexer = ChunkSearchIndexer(client)
+    indexer.ensure_index()
+    indexer.index_chunks(result.chunks, result.index_documents)
+    assert client.indices.created
+    assert client.bulk_operations
+    payloads = client.bulk_operations[0][1::2]
+    assert any(payload.get("title_path") for payload in payloads)
+    assert any("table_lines" in payload for payload in payloads)
+
+
+def test_chunk_search_indexer_build_query_uses_boosts() -> None:
+    client = FakeChunkClient()
+    indexer = ChunkSearchIndexer(client)
+    query = indexer.build_query("glucose")
+    fields = query["query"]["bool"]["should"][0]["multi_match"]["fields"]
+    assert set(fields) == {"title_path^2.0", "facet_json^1.6", "table_lines^1.2", "body^1.0"}
diff --git a/tests/embeddings/test_embeddings.py b/tests/embeddings/test_embeddings.py
index 29ce39db865479a6c48a2461d04ebd51082a5b92..d26b58321f3cde605a27704f2efbecaaabc44004 100644
--- a/tests/embeddings/test_embeddings.py
+++ b/tests/embeddings/test_embeddings.py
@@ -1,64 +1,175 @@
 from __future__ import annotations

 import pytest

 from Medical_KG.catalog.models import Concept, ConceptFamily
 from Medical_KG.embeddings import (
+    EmbeddingPerformanceMonitor,
     EmbeddingService,
     GPURequirementError,
+    GPUStats,
     GPUValidator,
+    LoadTestResult,
     QwenEmbeddingClient,
     SPLADEExpander,
+    enforce_gpu_or_exit,
 )


 @pytest.fixture(autouse=True)
 def reset_env(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.delenv("REQUIRE_GPU", raising=False)


 def test_gpu_validator_skips_when_disabled(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setenv("REQUIRE_GPU", "0")
     validator = GPUValidator()
     validator.validate()  # does not raise


 def test_gpu_validator_raises_without_gpu(monkeypatch: pytest.MonkeyPatch) -> None:
     monkeypatch.setenv("REQUIRE_GPU", "1")
     validator = GPUValidator()
     with pytest.raises(GPURequirementError):
         validator.validate()


 def test_qwen_embeddings_are_deterministic() -> None:
     client = QwenEmbeddingClient(dimension=8, batch_size=4)
     first = client.embed(["hello world"])[0]
     second = client.embed(["hello world"])[0]
     assert pytest.approx(first) == second
     norm = sum(value * value for value in first) ** 0.5
     assert pytest.approx(norm, rel=1e-6) == 1.0


 def test_embedding_service_sets_vectors(monkeypatch: pytest.MonkeyPatch) -> None:
     concept = Concept(
         iri="http://example.org/concept/1",
         ontology="TEST",
         family=ConceptFamily.CONDITION,
         label="Example concept",
         preferred_term="Example concept",
         definition="A sample concept for testing.",
         synonyms=[],
         codes={"test": "1"},
         xrefs={},
         parents=[],
         ancestors=[],
         attributes={"umls_cui": "C000"},
         release={"version": "1", "released_at": "2025-01-01"},
         license_bucket="open",
         provenance={"source": "unit"},
     )
-    service = EmbeddingService(qwen=QwenEmbeddingClient(dimension=8, batch_size=4), splade=SPLADEExpander(top_k=4))
+    service = EmbeddingService(
+        qwen=QwenEmbeddingClient(dimension=8, batch_size=4), splade=SPLADEExpander(top_k=4)
+    )
     service.embed_concepts([concept])
     assert concept.embedding_qwen is not None
     assert len(concept.embedding_qwen) == 8
     assert concept.splade_terms
+
+
+def test_qwen_client_http_retries(monkeypatch: pytest.MonkeyPatch) -> None:
+    class DummyResponse:
+        def raise_for_status(self) -> None:
+            return None
+
+        def json(self) -> dict[str, object]:
+            return {"data": [{"embedding": [0.1, 0.2]}]}
+
+    class DummyClient:
+        def __init__(self) -> None:
+            self.calls = 0
+
+        def post(self, url: str, json: dict[str, object]) -> DummyResponse:
+            self.calls += 1
+            if self.calls == 1:
+                raise RuntimeError("transient error")
+            return DummyResponse()
+
+        def close(self) -> None:
+            return None
+
+    factory_client = DummyClient()
+
+    def factory() -> DummyClient:
+        return factory_client
+
+    client = QwenEmbeddingClient(
+        dimension=2, batch_size=8, api_url="http://fake", http_client_factory=factory
+    )
+    vectors = client.embed(["text"])
+    assert len(vectors[0]) == 2
+    assert factory_client.calls == 2
+
+
+def test_enforce_gpu_or_exit(monkeypatch: pytest.MonkeyPatch) -> None:
+    class FailingValidator(GPUValidator):
+        def validate(self) -> None:  # type: ignore[override]
+            raise GPURequirementError("missing gpu")
+
+    with pytest.raises(SystemExit) as excinfo:
+        enforce_gpu_or_exit(validator=FailingValidator())
+    assert excinfo.value.code == 99
+
+
+class RecordingSink:
+    def __init__(self) -> None:
+        self.alerts: list[tuple[str, str]] = []
+
+    def emit(self, alert: str, message: str) -> None:
+        self.alerts.append((alert, message))
+
+
+def test_embedding_monitor_benchmark_and_alerts() -> None:
+    service = EmbeddingService(
+        qwen=QwenEmbeddingClient(dimension=8, batch_size=4), splade=SPLADEExpander(top_k=4)
+    )
+    sink = RecordingSink()
+    monitor = EmbeddingPerformanceMonitor(service, alert_sink=sink)
+    result = monitor.benchmark_embeddings(["alpha", "beta", "gamma"])
+    assert result.throughput_per_minute > 0
+    monitor.check_throughput(result, threshold=result.throughput_per_minute * 2)
+    assert sink.alerts and sink.alerts[0][0] == "throughput_low"
+
+
+def test_embedding_monitor_collects_gpu_stats(monkeypatch: pytest.MonkeyPatch) -> None:
+    def fake_run(*args, **kwargs):
+        class Result:
+            stdout = "75, 1024\n60, 2048\n"
+
+        return Result()
+
+    monkeypatch.setattr(
+        "Medical_KG.embeddings.monitoring.subprocess.run", fake_run
+    )
+    service = EmbeddingService(
+        qwen=QwenEmbeddingClient(dimension=8, batch_size=4), splade=SPLADEExpander(top_k=4)
+    )
+    monitor = EmbeddingPerformanceMonitor(service)
+    stats = monitor.collect_gpu_stats()
+    assert isinstance(stats, GPUStats)
+    assert stats.utilisation == pytest.approx(67.5)
+    assert stats.memory_used_mb == pytest.approx(1536.0)
+
+
+def test_embedding_monitor_load_test_and_health(monkeypatch: pytest.MonkeyPatch) -> None:
+    class FailingValidator(GPUValidator):
+        def validate(self) -> None:  # type: ignore[override]
+            raise GPURequirementError("not visible")
+
+    service = EmbeddingService(
+        qwen=QwenEmbeddingClient(dimension=8, batch_size=2), splade=SPLADEExpander(top_k=4)
+    )
+    sink = RecordingSink()
+    monitor = EmbeddingPerformanceMonitor(service, gpu_validator=FailingValidator(), alert_sink=sink)
+    result = monitor.run_load_test(["alpha", "beta"], target_chunks=4, batch_size=2)
+    assert isinstance(result, LoadTestResult)
+    assert result.total_chunks == 4
+    assert result.throughput_per_minute > 0
+    with pytest.raises(GPURequirementError):
+        monitor.monitor_health()
+    assert any(alert for alert, _ in sink.alerts if alert == "gpu_unavailable")
+    dashboard = monitor.dashboard_definition()
+    assert dashboard["title"] == "Embedding GPU Performance"

EOF
)
