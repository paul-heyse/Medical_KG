 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF'
diff --git a/openspec/changes/02-add-data-ingestion-core/tasks.md b/openspec/changes/02-add-data-ingestion-core/tasks.md
index e56c8f9a9985171d5f6007448451ef0b30bd321a..5861f4230fcc3ab37ad100b89bb381232aff012f 100644
--- a/openspec/changes/02-add-data-ingestion-core/tasks.md
+++ b/openspec/changes/02-add-data-ingestion-core/tasks.md
@@ -1,42 +1,42 @@
 # Implementation Tasks

 ## 1. Foundation

 - [x] 1.1 Create HTTP client with retries, rate limiting, per-host throttling (`connectors/http_client.py`)
 - [x] 1.2 Implement ledger system (states: pdf_downloaded, auto_inflight, auto_done, *_failed) with JSONL persistence
 - [x] 1.3 Create base Adapter interface (`fetch`, `parse`, `validate`, `write`)
 - [x] 1.4 Implement content hashing (SHA256) and doc_id generation (`{source}:{id}#{version}:{hash12}`)
 - [x] 1.5 Create normalization utilities (UTF-8, NFKC, whitespace, de-hyphenation, language detection)
-- [ ] 1.6 Add test fixtures for each source (sample responses)
+- [x] 1.6 Add test fixtures for each source (sample responses)

 ## 2. Literature & Preprints

-- [ ] 2.1 Implement PubMed E-utilities adapter (ESearch, EFetch, ESummary with usehistory paging)
-- [ ] 2.2 Implement PMC OAI-PMH adapter (ListRecords with resumptionToken)
-- [ ] 2.3 Implement medRxiv adapter (details endpoint with cursor paging)
-- [ ] 2.4 Add rate limit handling (3 rps → 10 rps with API key for NCBI)
+- [x] 2.1 Implement PubMed E-utilities adapter (ESearch, EFetch, ESummary with usehistory paging)
+- [x] 2.2 Implement PMC OAI-PMH adapter (ListRecords with resumptionToken)
+- [x] 2.3 Implement medRxiv adapter (details endpoint with cursor paging)
+- [x] 2.4 Add rate limit handling (3 rps → 10 rps with API key for NCBI)

 ## 3. Clinical Trials

 - [ ] 3.1 Implement ClinicalTrials.gov v2 adapter (search_studies, get_study with pageToken)
 - [ ] 3.2 Parse protocol sections (eligibility, outcome measures, arms, results, AEs)
 - [ ] 3.3 Store record_version for change tracking
 - [ ] 3.4 Add validation for NCT ID format

 ## 4. Drug & Device Safety

 - [ ] 4.1 Implement openFDA adapter (FAERS /drug/event, MAUDE /device/event, Labels, NDC)
 - [ ] 4.2 Handle Elasticsearch-style search params and pagination
 - [ ] 4.3 Add API key support (240 rpm with key vs 1k/day without)
 - [ ] 4.4 Implement DailyMed SPL adapter (by setid/NDC; parse LOINC-coded sections)
 - [ ] 4.5 Implement RxNav/RxNorm adapter (rxcui, ndcproperties endpoints)

 ## 5. Clinical Terminologies

 - [ ] 5.1 Implement MeSH RDF adapter (lookup/descriptor endpoint + SPARQL queries)
 - [ ] 5.2 Implement UMLS adapter (UTS API key authentication; CUI lookups)
 - [ ] 5.3 Implement LOINC FHIR adapter (CodeSystem/$lookup, ValueSet/$expand with basic auth)
 - [ ] 5.4 Implement ICD-11 API adapter (OAuth2 client credentials flow; /mms/{code} endpoint)
 - [ ] 5.5 Implement SNOMED CT Snowstorm adapter (FHIR CodeSystem/$lookup, ValueSet/$expand)

 ## 6. Guidelines & Practice Data
diff --git a/src/Medical_KG/ingestion/adapters/literature.py b/src/Medical_KG/ingestion/adapters/literature.py
index ea4f6ae57ce812553f3db05e1f22077e5bed9c09..7a2368b923381cbddb490bdb6708709086c9d450 100644
--- a/src/Medical_KG/ingestion/adapters/literature.py
+++ b/src/Medical_KG/ingestion/adapters/literature.py
@@ -1,133 +1,419 @@
 from __future__ import annotations

 import re
 import xml.etree.ElementTree as ET
 from collections.abc import AsyncIterator
-from typing import Any
+from typing import Any, Iterable
+from urllib.parse import urlparse

 from Medical_KG.ingestion.adapters.base import AdapterContext
 from Medical_KG.ingestion.adapters.http import HttpAdapter
-from Medical_KG.ingestion.http_client import AsyncHttpClient
+from Medical_KG.ingestion.http_client import AsyncHttpClient, RateLimit
 from Medical_KG.ingestion.models import Document
 from Medical_KG.ingestion.utils import canonical_json, normalize_text

 PUBMED_SEARCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
 PUBMED_SUMMARY_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
+PUBMED_FETCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"
 PMC_LIST_URL = "https://www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi"
 MEDRXIV_URL = "https://api.medrxiv.org/details/medrxiv"

 PMID_RE = re.compile(r"^\d{4,}")
 PMCID_RE = re.compile(r"^PMC\d+")


 class PubMedAdapter(HttpAdapter):
     source = "pubmed"

     def __init__(self, context: AdapterContext, client: AsyncHttpClient, *, api_key: str | None = None) -> None:
         super().__init__(context, client)
         self.api_key = api_key
+        host = urlparse(PUBMED_SEARCH_URL).netloc
+        rate = RateLimit(rate=10 if api_key else 3, per=1.0)
+        self.client.set_rate_limit(host, rate)

-    async def fetch(self, term: str, retmax: int = 100) -> AsyncIterator[Any]:
-        params = {"db": "pubmed", "retmode": "json", "retmax": retmax, "term": term}
+    async def fetch(self, term: str, retmax: int = 1000) -> AsyncIterator[Any]:
+        retmax = min(retmax, 10000)
+        params = {
+            "db": "pubmed",
+            "retmode": "json",
+            "retmax": retmax,
+            "term": term,
+            "usehistory": "y",
+        }
         if self.api_key:
             params["api_key"] = self.api_key
         search = await self.fetch_json(PUBMED_SEARCH_URL, params=params)
-        ids = search["esearchresult"].get("idlist", [])
-        if not ids:
+        search_result = search.get("esearchresult", {})
+        webenv = search_result.get("webenv")
+        query_key = search_result.get("querykey")
+        count = int(search_result.get("count", len(search_result.get("idlist", [])) or 0))
+        if not (webenv and query_key and count):
+            id_list = search_result.get("idlist", [])
+            if not id_list:
+                return
+            summary_params = {"db": "pubmed", "retmode": "json", "id": ",".join(id_list)}
+            if self.api_key:
+                summary_params["api_key"] = self.api_key
+            summary = await self.fetch_json(PUBMED_SUMMARY_URL, params=summary_params)
+            fetch_params = {"db": "pubmed", "retmode": "xml", "id": ",".join(id_list), "rettype": "abstract"}
+            if self.api_key:
+                fetch_params["api_key"] = self.api_key
+            fetch_xml = await self.fetch_text(PUBMED_FETCH_URL, params=fetch_params)
+            details = self._parse_fetch_xml(fetch_xml)
+            summary_result = summary.get("result", {})
+            for uid in summary_result.get("uids", []):
+                combined = dict(details.get(uid, {}))
+                combined.update(summary_result.get(uid, {}))
+                if combined:
+                    yield combined
             return
-        summary_params = {"db": "pubmed", "retmode": "json", "id": ",".join(ids)}
-        if self.api_key:
-            summary_params["api_key"] = self.api_key
-        summary = await self.fetch_json(PUBMED_SUMMARY_URL, params=summary_params)
-        result = summary.get("result", {})
-        for uid in result.get("uids", []):
-            data = result.get(uid)
-            if data:
-                yield data
+        retstart = 0
+        while retstart < count:
+            summary_params = {
+                "db": "pubmed",
+                "retmode": "json",
+                "retstart": retstart,
+                "retmax": retmax,
+                "query_key": query_key,
+                "WebEnv": webenv,
+            }
+            if self.api_key:
+                summary_params["api_key"] = self.api_key
+            summary = await self.fetch_json(PUBMED_SUMMARY_URL, params=summary_params)
+            summary_result = summary.get("result", {})
+            uids: Iterable[str] = summary_result.get("uids", [])
+            fetch_params = {
+                "db": "pubmed",
+                "retmode": "xml",
+                "retstart": retstart,
+                "retmax": retmax,
+                "query_key": query_key,
+                "WebEnv": webenv,
+                "rettype": "abstract",
+            }
+            if self.api_key:
+                fetch_params["api_key"] = self.api_key
+            fetch_xml = await self.fetch_text(PUBMED_FETCH_URL, params=fetch_params)
+            details = self._parse_fetch_xml(fetch_xml)
+            for uid in uids:
+                combined = dict(details.get(uid, {}))
+                combined.update(summary_result.get(uid, {}))
+                if combined:
+                    yield combined
+            retstart += retmax

     def parse(self, raw: Any) -> Document:
-        uid = raw.get("uid") or raw["articleids"][0]["value"]
+        uid = str(raw.get("pmid") or raw.get("uid"))
         title = normalize_text(raw.get("title", ""))
-        abstract = normalize_text(raw.get("elocationid", ""))
+        abstract = normalize_text(raw.get("abstract", ""))
         payload = {
             "pmid": uid,
+            "pmcid": raw.get("pmcid"),
+            "doi": raw.get("doi"),
             "title": title,
             "abstract": abstract,
+            "authors": raw.get("authors", []),
+            "mesh_terms": raw.get("mesh_terms", []),
+            "journal": raw.get("journal"),
+            "pub_year": raw.get("pub_year"),
+            "pub_types": raw.get("pub_types", []),
             "pubdate": raw.get("pubdate"),
         }
         content = canonical_json(payload)
         doc_id = self.build_doc_id(identifier=uid, version=raw.get("sortpubdate", "unknown"), content=content)
         metadata = {
             "title": title,
             "pub_date": raw.get("pubdate"),
             "journal": raw.get("fulljournalname"),
+            "pmid": uid,
         }
         return Document(doc_id=doc_id, source=self.source, content=abstract or title, metadata=metadata, raw=payload)

     def validate(self, document: Document) -> None:
         pmid = document.raw["pmid"]  # type: ignore[index]
         if not PMID_RE.match(str(pmid)):
             raise ValueError(f"Invalid PMID: {pmid}")

+    @staticmethod
+    def _fetch_author_list(raw_authors: Iterable[dict[str, Any]]) -> list[str]:
+        authors: list[str] = []
+        for author in raw_authors:
+            if collective := author.get("CollectiveName"):
+                authors.append(normalize_text(str(collective)))
+                continue
+            last = normalize_text(str(author.get("LastName", "")))
+            fore = normalize_text(str(author.get("ForeName", "")))
+            name = " ".join(part for part in [fore, last] if part)
+            if name:
+                authors.append(name)
+        return authors
+
+    @staticmethod
+    def _parse_fetch_xml(xml: str) -> dict[str, dict[str, Any]]:
+        details: dict[str, dict[str, Any]] = {}
+        root = ET.fromstring(xml)
+
+        def strip(tag: str) -> str:
+            return tag.split("}")[-1]
+
+        for article in root.findall(".//PubmedArticle"):
+            medline = article.find("MedlineCitation")
+            if medline is None:
+                continue
+            pmid = medline.findtext("PMID")
+            if not pmid:
+                continue
+            article_data = medline.find("Article")
+            journal = None
+            pub_year = None
+            abstract_text = []
+            authors: list[str] = []
+            pub_types: list[str] = []
+            if article_data is not None:
+                title = article_data.findtext("ArticleTitle", default="")
+                abstract = article_data.find("Abstract")
+                if abstract is not None:
+                    for chunk in abstract.findall("AbstractText"):
+                        label = chunk.attrib.get("Label")
+                        text = normalize_text("".join(chunk.itertext()))
+                        abstract_text.append(f"{label}: {text}" if label else text)
+                author_list = article_data.find("AuthorList")
+                if author_list is not None:
+                    authors = PubMedAdapter._fetch_author_list(
+                        [
+                            {
+                                strip(child.tag): normalize_text("".join(child.itertext()))
+                                for child in author
+                            }
+                            for author in author_list.findall("Author")
+                        ]
+                    )
+                journal = article_data.findtext("Journal/Title")
+                pub_year = article_data.findtext("Journal/JournalIssue/PubDate/Year")
+                pub_types = [normalize_text(pt.text or "") for pt in article_data.findall("PublicationTypeList/PublicationType")]
+            mesh_terms = [normalize_text(node.text or "") for node in medline.findall("MeshHeadingList/MeshHeading/DescriptorName")]
+            article_ids = article.findall("PubmedData/ArticleIdList/ArticleId")
+            pmcid = None
+            doi = None
+            for identifier in article_ids:
+                id_type = identifier.attrib.get("IdType")
+                value = normalize_text(identifier.text or "")
+                if id_type == "pmc":
+                    pmcid = value
+                elif id_type == "doi":
+                    doi = value
+            details[pmid] = {
+                "pmid": pmid,
+                "title": normalize_text(article_data.findtext("ArticleTitle", default="")) if article_data is not None else "",
+                "abstract": normalize_text("\n".join(filter(None, abstract_text))),
+                "authors": authors,
+                "mesh_terms": [term for term in mesh_terms if term],
+                "journal": normalize_text(journal or ""),
+                "pub_year": pub_year,
+                "pub_types": [ptype for ptype in pub_types if ptype],
+                "pmcid": pmcid,
+                "doi": doi,
+            }
+        return details
+

 class PmcAdapter(HttpAdapter):
     source = "pmc"

-    async def fetch(self, set_spec: str, *, metadata_prefix: str = "oai_dc") -> AsyncIterator[Any]:
-        params = {"verb": "ListRecords", "set": set_spec, "metadataPrefix": metadata_prefix}
-        xml = await self.fetch_text(PMC_LIST_URL, params=params)
-        root = ET.fromstring(xml)
-        for record in root.findall(".//record"):
-            yield record
+    def __init__(self, context: AdapterContext, client: AsyncHttpClient) -> None:
+        super().__init__(context, client)
+        host = urlparse(PMC_LIST_URL).netloc
+        self.client.set_rate_limit(host, RateLimit(rate=3, per=1.0))
+
+    async def fetch(
+        self,
+        set_spec: str,
+        *,
+        metadata_prefix: str = "pmc",
+        from_date: str | None = None,
+        until_date: str | None = None,
+    ) -> AsyncIterator[Any]:
+        params: dict[str, Any] = {"verb": "ListRecords", "set": set_spec, "metadataPrefix": metadata_prefix}
+        if from_date:
+            params["from"] = from_date
+        if until_date:
+            params["until"] = until_date
+        while True:
+            xml = await self.fetch_text(PMC_LIST_URL, params=params)
+            root = ET.fromstring(xml)
+            records = self._findall(root, "record")
+            for record in records:
+                yield record
+            resumption = self._find(root, "resumptionToken")
+            token = (resumption.text or "").strip() if resumption is not None else ""
+            if not token:
+                break
+            params = {"verb": "ListRecords", "resumptionToken": token}

     def parse(self, raw: Any) -> Document:
-        header = raw.find("header")
-        identifier_text = header.findtext("identifier", default="") if header is not None else ""
+        header = self._find(raw, "header")
+        identifier_text = self._findtext(header, "identifier") or ""
         pmcid = identifier_text.split(":")[-1] if identifier_text else "unknown"
-        metadata = raw.find("metadata")
-        title = metadata.findtext("title", default="") if metadata is not None else ""
-        description = metadata.findtext("description", default="") if metadata is not None else ""
+        metadata = self._find(raw, "metadata")
+        article = None
+        if metadata is not None:
+            article = self._find(metadata, "article") or metadata
+        title = normalize_text(self._findtext(article, "article-title") or self._findtext(metadata, "title") or "")
+        abstract = normalize_text(self._collect_text(article, "abstract")) if article is not None else ""
+        sections = self._collect_sections(article)
+        tables = self._collect_table_like(article, "table-wrap")
+        figures = self._collect_table_like(article, "fig")
+        references = self._collect_references(article)
         payload = {
             "pmcid": pmcid,
-            "title": normalize_text(title),
-            "description": normalize_text(description),
+            "title": title,
+            "abstract": abstract,
+            "sections": sections,
+            "tables": tables,
+            "figures": figures,
+            "references": references,
         }
         content = canonical_json(payload)
-        datestamp = header.findtext("datestamp", default="unknown") if header is not None else "unknown"
+        datestamp = self._findtext(header, "datestamp") or "unknown"
         doc_id = self.build_doc_id(identifier=pmcid, version=datestamp, content=content)
-        meta = {"title": payload["title"], "datestamp": datestamp}
-        return Document(doc_id=doc_id, source=self.source, content=payload["description"] or payload["title"], metadata=meta, raw=payload)
+        meta = {"title": title, "datestamp": datestamp, "pmcid": pmcid}
+        body_text = "\n\n".join(section["text"] for section in sections if section["text"])
+        document_content = abstract or body_text or title
+        return Document(doc_id=doc_id, source=self.source, content=document_content, metadata=meta, raw=payload)

     def validate(self, document: Document) -> None:
         pmcid = document.raw["pmcid"]  # type: ignore[index]
         if not PMCID_RE.match(pmcid):
             raise ValueError(f"Invalid PMCID: {pmcid}")

+    @staticmethod
+    def _strip(tag: str) -> str:
+        return tag.split("}")[-1]
+
+    def _find(self, element: ET.Element | None, name: str) -> ET.Element | None:
+        if element is None:
+            return None
+        for child in element.iter():
+            if self._strip(child.tag) == name:
+                return child
+        return None
+
+    def _findall(self, element: ET.Element, name: str) -> list[ET.Element]:
+        return [child for child in element.iter() if self._strip(child.tag) == name]
+
+    def _findtext(self, element: ET.Element | None, name: str) -> str | None:
+        if element is None:
+            return None
+        for child in element.iter():
+            if self._strip(child.tag) == name:
+                return "".join(child.itertext())
+        return None
+
+    def _collect_text(self, element: ET.Element | None, name: str) -> str:
+        if element is None:
+            return ""
+        texts: list[str] = []
+        for child in element.iter():
+            if self._strip(child.tag) == name:
+                texts.append(normalize_text("".join(child.itertext())))
+        return "\n".join(texts)
+
+    def _collect_sections(self, article: ET.Element | None) -> list[dict[str, str]]:
+        sections: list[dict[str, str]] = []
+        if article is None:
+            return sections
+        for section in article.iter():
+            if self._strip(section.tag) != "sec":
+                continue
+            title = normalize_text(self._findtext(section, "title") or "")
+            text_chunks = [normalize_text("".join(node.itertext())) for node in section if self._strip(node.tag) != "title"]
+            text = "\n".join(chunk for chunk in text_chunks if chunk)
+            sections.append({"title": title, "text": text})
+        return sections
+
+    def _collect_table_like(self, article: ET.Element | None, name: str) -> list[dict[str, str]]:
+        items: list[dict[str, str]] = []
+        if article is None:
+            return items
+        for node in article.iter():
+            if self._strip(node.tag) != name:
+                continue
+            caption = normalize_text(self._findtext(node, "caption") or "")
+            label = normalize_text(self._findtext(node, "label") or "")
+            uri = None
+            graphic = self._find(node, "graphic")
+            if graphic is not None:
+                uri = graphic.attrib.get("{http://www.w3.org/1999/xlink}href") or graphic.attrib.get("href")
+            items.append({"label": label, "caption": caption, "uri": uri or ""})
+        return items
+
+    def _collect_references(self, article: ET.Element | None) -> list[dict[str, str]]:
+        refs: list[dict[str, str]] = []
+        if article is None:
+            return refs
+        for node in article.iter():
+            if self._strip(node.tag) != "ref":
+                continue
+            label = normalize_text(self._findtext(node, "label") or "")
+            citation = normalize_text(self._findtext(node, "mixed-citation") or "")
+            refs.append({"label": label, "citation": citation})
+        return refs
+

 class MedRxivAdapter(HttpAdapter):
     source = "medrxiv"

-    async def fetch(self, start: int = 0, chunk: int = 100) -> AsyncIterator[Any]:
-        params = {"from": start, "chunk": chunk}
-        payload = await self.fetch_json(MEDRXIV_URL, params=params)
-        for record in payload.get("results", []):
-            yield record
+    def __init__(
+        self,
+        context: AdapterContext,
+        client: AsyncHttpClient,
+        *,
+        bootstrap_records: Iterable[dict[str, Any]] | None = None,
+    ) -> None:
+        super().__init__(context, client)
+        self._bootstrap = list(bootstrap_records or [])
+
+    async def fetch(
+        self,
+        *,
+        search: str | None = None,
+        cursor: str | None = None,
+        page_size: int = 100,
+    ) -> AsyncIterator[Any]:
+        if self._bootstrap:
+            for record in self._bootstrap:
+                yield record
+            return
+        params: dict[str, Any] = {"page_size": page_size}
+        if search:
+            params["search"] = search
+        next_cursor = cursor
+        while True:
+            if next_cursor:
+                params["cursor"] = next_cursor
+            payload = await self.fetch_json(MEDRXIV_URL, params=params)
+            for record in payload.get("results", []):
+                yield record
+            next_cursor = payload.get("next_cursor")
+            if not next_cursor:
+                break

     def parse(self, raw: Any) -> Document:
         identifier = raw["doi"]
         title = normalize_text(raw.get("title", ""))
         abstract = normalize_text(raw.get("abstract", ""))
         payload = {
             "doi": identifier,
             "title": title,
             "abstract": abstract,
             "date": raw.get("date"),
         }
         content = canonical_json(payload)
         doc_id = self.build_doc_id(identifier=identifier, version=raw.get("version", "1"), content=content)
         metadata = {"title": title, "authors": raw.get("authors", [])}
         return Document(doc_id=doc_id, source=self.source, content=abstract or title, metadata=metadata, raw=payload)

     def validate(self, document: Document) -> None:
         if "/" not in document.raw["doi"]:  # type: ignore[index]
             raise ValueError("Invalid DOI")
diff --git a/src/Medical_KG/ingestion/http_client.py b/src/Medical_KG/ingestion/http_client.py
index 5c6e83c915b14f5b643ada24e7dc2b190d2f4e58..569722748f715160a8679d9c5b9867de12a35e71 100644
--- a/src/Medical_KG/ingestion/http_client.py
+++ b/src/Medical_KG/ingestion/http_client.py
@@ -1,28 +1,29 @@
 from __future__ import annotations

 import asyncio
+import random
 from collections import deque
 from contextlib import asynccontextmanager
 from dataclasses import dataclass
 from time import time
 from typing import Any, AsyncIterator, Dict, Mapping, MutableMapping
 from urllib.parse import urlparse

 import httpx

 try:  # pragma: no cover - optional dependency
     from prometheus_client import Counter, Histogram
 except ModuleNotFoundError:  # pragma: no cover - fallback in tests

     class _NoopMetric:
         def labels(self, *args: Any, **kwargs: Any) -> "_NoopMetric":
             return self

         def inc(self, *_args: Any, **_kwargs: Any) -> None:  # pragma: no cover - noop
             return None

         def observe(self, *_args: Any, **_kwargs: Any) -> None:  # pragma: no cover - noop
             return None

     def Counter(*_args: Any, **_kwargs: Any) -> _NoopMetric:  # type: ignore
         return _NoopMetric()
@@ -104,53 +105,57 @@ class AsyncHttpClient:
     async def aclose(self) -> None:
         await self._client.aclose()

     def _get_limiter(self, host: str) -> _SimpleLimiter:
         if host not in self._limiters:
             limit = self._limits.get(host, self._default_rate)
             self._limiters[host] = _SimpleLimiter(limit.rate, limit.per)
         return self._limiters[host]

     async def _execute(self, method: str, url: str, **kwargs: Any) -> httpx.Response:
         parsed = urlparse(url)
         limiter = self._get_limiter(parsed.netloc)

         async with limiter:
             backoff = 0.5
             last_error: Exception | None = None
             for _ in range(self._retries):
                 try:
                     start = time()
                     response = await self._client.request(method, url, **kwargs)
                     HTTP_REQUESTS.labels(method, parsed.netloc, str(response.status_code)).inc()
                     HTTP_LATENCY.observe(time() - start)
                     response.raise_for_status()
                     return response
                 except httpx.HTTPError as exc:  # pragma: no cover - exercised via tests
+                    status = getattr(getattr(exc, "response", None), "status_code", None)
+                    if status not in {429, 502, 503, 504}:
+                        raise
                     last_error = exc
                     HTTP_REQUESTS.labels(method, parsed.netloc, exc.__class__.__name__).inc()
-                    await asyncio.sleep(backoff)
+                    jitter = random.uniform(0, backoff / 2)
+                    await asyncio.sleep(backoff + jitter)
                     backoff = min(backoff * 2, 5.0)
             if last_error:
                 raise last_error
             raise RuntimeError("Retry loop exhausted")

     async def get(
         self,
         url: str,
         *,
         params: Mapping[str, Any] | None = None,
         headers: Mapping[str, str] | None = None,
     ) -> httpx.Response:
         return await self._execute("GET", url, params=params, headers=headers)

     async def post(
         self,
         url: str,
         *,
         data: Any | None = None,
         json: Any | None = None,
         headers: Mapping[str, str] | None = None,
     ) -> httpx.Response:
         return await self._execute("POST", url, data=data, json=json, headers=headers)

     @asynccontextmanager
@@ -171,25 +176,32 @@ class AsyncHttpClient:
         params: Mapping[str, Any] | None = None,
         headers: Mapping[str, str] | None = None,
     ) -> Any:
         response = await self.get(url, params=params, headers=headers)
         return response.json()

     async def get_text(
         self,
         url: str,
         *,
         params: Mapping[str, Any] | None = None,
         headers: Mapping[str, str] | None = None,
     ) -> str:
         response = await self.get(url, params=params, headers=headers)
         return response.text

     async def get_bytes(
         self,
         url: str,
         *,
         params: Mapping[str, Any] | None = None,
         headers: Mapping[str, str] | None = None,
     ) -> bytes:
         response = await self.get(url, params=params, headers=headers)
         return response.content
+
+    def set_rate_limit(self, host: str, limit: RateLimit) -> None:
+        """Override the per-host rate limit."""
+
+        self._limits[host] = limit
+        if host in self._limiters:
+            self._limiters[host] = _SimpleLimiter(limit.rate, limit.per)
diff --git a/tests/fixtures/ingestion/pubmed_fetch.xml b/tests/fixtures/ingestion/pubmed_fetch.xml
new file mode 100644
index 0000000000000000000000000000000000000000..1fce797855de6532c52d3d1469e8a25006b6bba7
--- /dev/null
+++ b/tests/fixtures/ingestion/pubmed_fetch.xml
@@ -0,0 +1,48 @@
+<PubmedArticleSet>
+  <PubmedArticle>
+    <MedlineCitation>
+      <PMID>12345678</PMID>
+      <Article>
+        <ArticleTitle>Lactate Guided Therapy</ArticleTitle>
+        <Abstract>
+          <AbstractText Label="Background">Background context</AbstractText>
+          <AbstractText>Sepsis patients benefit from guided therapy.</AbstractText>
+        </Abstract>
+        <AuthorList>
+          <Author>
+            <LastName>Smith</LastName>
+            <ForeName>Jane</ForeName>
+          </Author>
+          <Author>
+            <CollectiveName>Sepsis Collaborative</CollectiveName>
+          </Author>
+        </AuthorList>
+        <Journal>
+          <JournalIssue>
+            <PubDate>
+              <Year>2024</Year>
+            </PubDate>
+          </JournalIssue>
+          <Title>Critical Care</Title>
+        </Journal>
+        <PublicationTypeList>
+          <PublicationType>Journal Article</PublicationType>
+        </PublicationTypeList>
+      </Article>
+      <MeshHeadingList>
+        <MeshHeading>
+          <DescriptorName>Sepsis</DescriptorName>
+        </MeshHeading>
+        <MeshHeading>
+          <DescriptorName>Lactic Acid</DescriptorName>
+        </MeshHeading>
+      </MeshHeadingList>
+    </MedlineCitation>
+    <PubmedData>
+      <ArticleIdList>
+        <ArticleId IdType="pmc">PMC1234567</ArticleId>
+        <ArticleId IdType="doi">10.1000/abc123</ArticleId>
+      </ArticleIdList>
+    </PubmedData>
+  </PubmedArticle>
+</PubmedArticleSet>
diff --git a/tests/ingestion/test_adapters.py b/tests/ingestion/test_adapters.py
index 2cb0aca325438e752eec5ee33377b934da937854..5d514bcd6b90c29083bc72b6f5d2606d80016408 100644
--- a/tests/ingestion/test_adapters.py
+++ b/tests/ingestion/test_adapters.py
@@ -1,75 +1,92 @@
 import asyncio
 import json
 from pathlib import Path
+from urllib.parse import urlparse

 from Medical_KG.ingestion.adapters.base import AdapterContext
 from Medical_KG.ingestion.adapters.clinical import (
     AccessGudidAdapter,
     ClinicalTrialsGovAdapter,
     OpenFdaAdapter,
     RxNormAdapter,
 )
 from Medical_KG.ingestion.adapters.guidelines import (
     CdcSocrataAdapter,
     NiceGuidelineAdapter,
 )
 from Medical_KG.ingestion.adapters.literature import MedRxivAdapter, PmcAdapter, PubMedAdapter
 from Medical_KG.ingestion.adapters.terminology import Icd11Adapter, LoincAdapter, MeSHAdapter, SnomedAdapter, UMLSAdapter
 from Medical_KG.ingestion.http_client import AsyncHttpClient
 from Medical_KG.ingestion.ledger import IngestionLedger


 def _run(coro):
     loop = asyncio.new_event_loop()
     try:
         return loop.run_until_complete(coro)
     finally:
         loop.close()


 def test_pubmed_adapter_parses_fixture(monkeypatch, tmp_path: Path) -> None:
     ledger = IngestionLedger(tmp_path / "ledger.jsonl")
     context = AdapterContext(ledger=ledger)
     client = AsyncHttpClient()
     adapter = PubMedAdapter(context, client, api_key=None)

     async def fake_fetch_json(url: str, *, params: dict | None = None, headers: dict | None = None) -> dict:
         if "esearch" in url:
             return json.loads(Path("tests/fixtures/ingestion/pubmed_search.json").read_text())
         return json.loads(Path("tests/fixtures/ingestion/pubmed_summary.json").read_text())

+    async def fake_fetch_text(url: str, *, params: dict | None = None, headers: dict | None = None) -> str:
+        assert "efetch" in url
+        return Path("tests/fixtures/ingestion/pubmed_fetch.xml").read_text()
+
     monkeypatch.setattr(adapter, "fetch_json", fake_fetch_json)
+    monkeypatch.setattr(adapter, "fetch_text", fake_fetch_text)

     async def _exec() -> None:
         results = await adapter.run(term="lactate")
         assert len(results) == 1
         assert results[0].document.metadata["title"].startswith("Lactate")
+        assert results[0].document.raw["pmcid"] == "PMC1234567"
+        assert "Sepsis" in results[0].document.raw["mesh_terms"]
+        assert results[0].document.raw["doi"] == "10.1000/abc123"
+        assert "Jane Smith" in results[0].document.raw["authors"][0]
         await client.aclose()

     _run(_exec())

+    client_with_key = AsyncHttpClient()
+    adapter_with_key = PubMedAdapter(context, client_with_key, api_key="token")
+    host = urlparse("https://eutils.ncbi.nlm.nih.gov").netloc
+    assert adapter.client._limits[host].rate == 3
+    assert adapter_with_key.client._limits[host].rate == 10
+    _run(adapter_with_key.client.aclose())
+

 def test_clinicaltrials_adapter_validates_nct(tmp_path: Path) -> None:
     ledger = IngestionLedger(tmp_path / "ledger.jsonl")
     context = AdapterContext(ledger=ledger)
     client = AsyncHttpClient()
     payload = json.loads(Path("tests/fixtures/ingestion/ctgov_study.json").read_text())
     adapter = ClinicalTrialsGovAdapter(context, client, bootstrap_records=[payload])

     async def _exec() -> None:
         results = await adapter.run()
         assert results[0].document.metadata["record_version"] == "2024-01-01"
         await client.aclose()

     _run(_exec())


 def test_openfda_adapter_handles_identifier(tmp_path: Path) -> None:
     ledger = IngestionLedger(tmp_path / "ledger.jsonl")
     context = AdapterContext(ledger=ledger)
     client = AsyncHttpClient()
     payload = json.loads(Path("tests/fixtures/ingestion/openfda_faers.json").read_text())["results"][0]
     adapter = OpenFdaAdapter(context, client, bootstrap_records=[payload])

     async def _exec() -> None:
         results = await adapter.run(resource="drug/event")

EOF
)
