 (cd "$(git rev-parse --show-toplevel)" && git apply --3way <<'EOF'
diff --git a/src/Medical_KG/ir/builder.py b/src/Medical_KG/ir/builder.py
index eef9dbc1997687cac3029d1afc27a4681f449532..002da3bebcf4908358fee8d0407bd1009f4a671a 100644
--- a/src/Medical_KG/ir/builder.py
+++ b/src/Medical_KG/ir/builder.py
@@ -1,91 +1,284 @@
 from __future__ import annotations

-from typing import Any, Iterable, Mapping
+from collections.abc import Mapping, Sequence
+from typing import Any
+
+from bs4 import BeautifulSoup

 from Medical_KG.ir.models import Block, DocumentIR, Table
 from Medical_KG.ir.normalizer import TextNormalizer, section_from_heading


 class IrBuilder:
     """Base builder converting source payloads into IR objects."""

     def __init__(self, *, normalizer: TextNormalizer | None = None) -> None:
         self.normalizer = normalizer or TextNormalizer()

-    def build(self, *, doc_id: str, source: str, uri: str, text: str, metadata: Mapping[str, Any]) -> DocumentIR:
+    def build(
+        self,
+        *,
+        doc_id: str,
+        source: str,
+        uri: str,
+        text: str,
+        metadata: Mapping[str, Any] | None = None,
+    ) -> DocumentIR:
+        metadata = metadata or {}
         normalized = self.normalizer.normalize(text)
         document = DocumentIR(
             doc_id=doc_id,
             source=source,
             uri=uri,
             language=normalized.language,
             text=normalized.text,
             raw_text=normalized.raw_text,
         )
         document.span_map = normalized.span_map
-        document.provenance.update(metadata.get("provenance", {}))
+        if span_entries := metadata.get("span_map"):
+            document.span_map.extend_from_offset_map(span_entries)
+        if provenance := metadata.get("provenance"):
+            document.provenance.update(provenance)
         return document

-
-class ClinicalTrialsBuilder(IrBuilder):
-    def build_from_study(self, *, doc_id: str, uri: str, study: Mapping[str, Any]) -> DocumentIR:
-        sections: list[tuple[str, str, str]] = []
-        for name in ("title", "status", "eligibility"):
-            value = study.get(name, "") or ""
-            if value:
-                normalized = self.normalizer.normalize(str(value))
-                sections.append((name, normalized.text, str(value)))
-        combined_text = "\n\n".join(section_text for _, section_text, _ in sections) if sections else ""
-        document = super().build(doc_id=doc_id, source="clinicaltrials", uri=uri, text=combined_text, metadata={})
+    def _add_blocks(
+        self,
+        document: DocumentIR,
+        blocks: Sequence[tuple[str, str, str | None, dict[str, Any]]],
+        *,
+        separator: str = "\n\n",
+    ) -> None:
         offset = 0
-        for index, (section, section_text, raw_value) in enumerate(sections):
+        sep_len = len(separator)
+        for index, (block_type, text, section, meta) in enumerate(blocks):
+            normalized = self.normalizer.normalize(text)
             start = offset
-            end = start + len(section_text)
-            block_type = "heading" if section == "title" else "paragraph"
+            end = start + len(normalized.text)
             document.add_block(
                 Block(
                     type=block_type,
-                    text=section_text,
+                    text=normalized.text,
                     start=start,
                     end=end,
                     section=section,
-                    meta={"raw": raw_value},
+                    meta=meta,
                 )
             )
             offset = end
-            if index < len(sections) - 1:
-                offset += 2  # account for double newline separator
+            if index < len(blocks) - 1:
+                offset += sep_len
+
+
+class ClinicalTrialsBuilder(IrBuilder):
+    """Builds IR documents from ClinicalTrials.gov payloads."""
+
+    def build_from_study(self, *, doc_id: str, uri: str, study: Mapping[str, Any]) -> DocumentIR:
+        sections: list[tuple[str, str, str | None, dict[str, Any]]] = []
+        for field, section in (
+            ("title", "title"),
+            ("status", "status"),
+            ("eligibility", "eligibility"),
+            ("outcomes", "outcomes"),
+        ):
+            value = study.get(field)
+            if not value:
+                continue
+            text = value if isinstance(value, str) else "\n".join(str(v) for v in value)
+            sections.append(("heading" if field == "title" else "paragraph", text, section, {"raw": value}))
+
+        combined_text = "\n\n".join(section[1] for section in sections) if sections else ""
+        document = super().build(
+            doc_id=doc_id,
+            source="clinicaltrials",
+            uri=uri,
+            text=combined_text,
+            metadata={"provenance": study.get("provenance", {})},
+        )
+        self._add_blocks(document, sections)
+
+        outcomes = study.get("outcomes") or []
+        if isinstance(outcomes, Sequence) and outcomes:
+            headers = ["measure", "description", "time_frame"]
+            rows = [
+                [
+                    str(outcome.get("measure", "")),
+                    str(outcome.get("description", "")),
+                    str(outcome.get("timeFrame", "")),
+                ]
+                for outcome in outcomes  # type: ignore[arg-type]
+            ]
+            caption = "Primary Outcomes"
+            start = len(document.text)
+            end = start + len(caption)
+            document.add_table(Table(caption=caption, headers=headers, rows=rows, start=start, end=end, meta={}))
         return document


 class PmcBuilder(IrBuilder):
+    """Builds IR documents from PMC article payloads."""
+
     def build_from_article(self, *, doc_id: str, uri: str, article: Mapping[str, Any]) -> DocumentIR:
-        text = article.get("abstract", "")
-        document = super().build(doc_id=doc_id, source="pmc", uri=uri, text=text, metadata={})
-        blocks: Iterable[Mapping[str, Any]] = article.get("sections", [])  # type: ignore[assignment]
-        offset = 0
-        for block in blocks:
+        abstract = article.get("abstract", "") or ""
+        sections_payload = article.get("sections", [])
+        parts = [abstract] if abstract else []
+        parts.extend(section.get("text", "") for section in sections_payload)
+        combined_text = "\n\n".join(part for part in parts if part)
+        metadata = {
+            "provenance": article.get("provenance", {}),
+            "span_map": article.get("span_map", []),
+        }
+        document = super().build(doc_id=doc_id, source="pmc", uri=uri, text=combined_text, metadata=metadata)
+        sections: list[tuple[str, str, str | None, dict[str, Any]]] = []
+        if abstract:
+            sections.append(("paragraph", abstract, "abstract", {"heading": "Abstract"}))
+        for block in sections_payload:
             heading = block.get("heading", "")
-            normalized = self.normalizer.normalize(block.get("text", ""))
-            start = offset
-            end = start + len(normalized.text)
+            block_text = block.get("text", "")
+            section = section_from_heading(heading) if heading else "body"
+            sections.append(("heading" if heading else "paragraph", block_text, section, {"heading": heading}))
+        self._add_blocks(document, sections, separator="\n\n")
+
+        for table_payload in article.get("tables", []):
+            caption = str(table_payload.get("caption", ""))
+            headers = [str(header) for header in table_payload.get("headers", [])]
+            rows = [[str(cell) for cell in row] for row in table_payload.get("rows", [])]
+            start = len(document.text)
+            end = start + len(caption)
+            document.add_table(Table(caption=caption, headers=headers, rows=rows, start=start, end=end, meta={}))
+        return document
+
+
+class DailyMedBuilder(IrBuilder):
+    """Builds IR documents from DailyMed SPL payloads."""
+
+    def build_from_spl(self, *, doc_id: str, uri: str, spl: Mapping[str, Any]) -> DocumentIR:
+        sections = spl.get("sections", [])
+        combined_text = "\n\n".join(section.get("text", "") for section in sections)
+        document = super().build(
+            doc_id=doc_id,
+            source="dailymed",
+            uri=uri,
+            text=combined_text,
+            metadata={"provenance": spl.get("provenance", {})},
+        )
+
+        block_payloads: list[tuple[str, str, str | None, dict[str, Any]]] = []
+        for section in sections:
+            loinc = section.get("loinc")
+            text = section.get("text", "")
+            block_payloads.append(("paragraph", text, "loinc_section", {"loinc": loinc}))
+        self._add_blocks(document, block_payloads)
+
+        if ingredients := spl.get("ingredients"):
+            headers = ["name", "strength", "basis"]
+            rows = [[str(item.get(header, "")) for header in headers] for item in ingredients]
+            caption = "Ingredients"
+            start = len(document.text)
+            end = start + len(caption)
+            document.add_table(Table(caption=caption, headers=headers, rows=rows, start=start, end=end, meta={}))
+        return document
+
+
+class MinerUBuilder(IrBuilder):
+    """Builds IR documents from MinerU artifact bundles."""
+
+    def build_from_artifacts(self, *, doc_id: str, uri: str, artifacts: Mapping[str, Any]) -> DocumentIR:
+        markdown = artifacts.get("markdown", "")
+        document = super().build(
+            doc_id=doc_id,
+            source="mineru",
+            uri=uri,
+            text=markdown,
+            metadata={
+                "provenance": artifacts.get("provenance", {}),
+                "span_map": artifacts.get("offset_map", []),
+            },
+        )
+
+        canonical_text = document.text
+        cursor = 0
+        for block in artifacts.get("blocks", []):
+            block_text = str(block.get("text", ""))
+            normalized_block = self.normalizer.normalize(block_text).text
+            if not normalized_block:
+                continue
+            start = canonical_text.find(normalized_block, cursor)
+            if start == -1:
+                start = cursor
+            end = start + len(normalized_block)
+            cursor = end
             document.add_block(
                 Block(
-                    type="heading" if heading else "paragraph",
-                    text=normalized.text,
+                    type=str(block.get("type", "paragraph")),
+                    text=normalized_block,
                     start=start,
                     end=end,
-                    section=section_from_heading(heading) if heading else "body",
-                    meta={"heading": heading},
+                    section=block.get("section"),
+                    meta={"path": block.get("path")},
                 )
             )
-            offset = end + 1
-        for table_payload in article.get("tables", []):
-            caption = table_payload.get("caption", "")
-            headers = table_payload.get("headers", [])
-            rows = table_payload.get("rows", [])
-            start = offset
+
+        for table_payload in artifacts.get("tables", []):
+            caption = str(table_payload.get("caption", ""))
+            headers = [str(header) for header in table_payload.get("headers", [])]
+            rows = [[str(cell) for cell in row] for row in table_payload.get("rows", [])]
+            start = len(document.text)
             end = start + len(caption)
-            document.add_table(Table(caption=caption, headers=headers, rows=rows, start=start, end=end, meta={}))
-            offset = end + 1
+            document.add_table(
+                Table(
+                    caption=caption,
+                    headers=headers,
+                    rows=rows,
+                    start=start,
+                    end=end,
+                    meta={"page": table_payload.get("page")},
+                )
+            )
+        return document
+
+
+class GuidelineBuilder(IrBuilder):
+    """Builds IR documents from HTML guideline content."""
+
+    def build_from_html(self, *, doc_id: str, uri: str, html: str) -> DocumentIR:
+        soup = BeautifulSoup(html, "html.parser")
+        content_blocks: list[tuple[str, str, str | None, dict[str, Any]]] = []
+        for element in soup.find_all(["h1", "h2", "h3", "p", "li"]):
+            text = element.get_text(strip=True)
+            if not text:
+                continue
+            if element.name in {"h1", "h2", "h3"}:
+                section = element.name
+                block_type = "heading"
+            elif element.name == "li":
+                section = "list_item"
+                block_type = "list_item"
+            else:
+                section = "paragraph"
+                block_type = "paragraph"
+            content_blocks.append((block_type, text, section, {"tag": element.name}))
+
+        combined_text = "\n\n".join(block[1] for block in content_blocks)
+        document = super().build(
+            doc_id=doc_id,
+            source="guideline",
+            uri=uri,
+            text=combined_text,
+            metadata={},
+        )
+        self._add_blocks(document, content_blocks)
+
+        for table_tag in soup.find_all("table"):
+            headers: list[str] = []
+            header_row = table_tag.find("tr")
+            if header_row:
+                headers = [cell.get_text(strip=True) for cell in header_row.find_all(["th", "td"])]
+            rows: list[list[str]] = []
+            for row in table_tag.find_all("tr")[1:]:
+                rows.append([cell.get_text(strip=True) for cell in row.find_all(["th", "td"])] or [])
+            caption = table_tag.find("caption")
+            caption_text = caption.get_text(strip=True) if caption else "Guideline Table"
+            start = len(document.text)
+            end = start + len(caption_text)
+            document.add_table(Table(caption=caption_text, headers=headers, rows=rows, start=start, end=end, meta={}))
         return document
diff --git a/src/Medical_KG/ir/models.py b/src/Medical_KG/ir/models.py
index cbb56441004dfa60eeeed0a7636198ba45512b5b..c50c291ace7b520139ddea912c93f5c9a06ee395 100644
--- a/src/Medical_KG/ir/models.py
+++ b/src/Medical_KG/ir/models.py
@@ -1,59 +1,115 @@
 from __future__ import annotations

 from dataclasses import dataclass, field
 from datetime import datetime
-from typing import Any, Dict, List, MutableMapping, Sequence
+from typing import Any, Dict, Iterable, List, Mapping, MutableMapping, Sequence


 @dataclass(slots=True)
 class Span:
     raw_start: int
     raw_end: int
     canonical_start: int
     canonical_end: int
     transform: str
+    page: int | None = None
+    bbox: tuple[float, float, float, float] | None = None


 @dataclass(slots=True)
 class SpanMap:
     spans: List[Span] = field(default_factory=list)

-    def add(self, raw_start: int, raw_end: int, canonical_start: int, canonical_end: int, transform: str) -> None:
-        self.spans.append(Span(raw_start, raw_end, canonical_start, canonical_end, transform))
+    def add(
+        self,
+        raw_start: int,
+        raw_end: int,
+        canonical_start: int,
+        canonical_end: int,
+        transform: str,
+        *,
+        page: int | None = None,
+        bbox: Iterable[float] | None = None,
+    ) -> None:
+        bbox_tuple: tuple[float, float, float, float] | None = None
+        if bbox is not None:
+            values = tuple(float(value) for value in bbox)
+            if len(values) != 4:
+                raise ValueError("bbox must contain four coordinates")
+            bbox_tuple = values  # type: ignore[assignment]
+        self.spans.append(
+            Span(
+                raw_start,
+                raw_end,
+                canonical_start,
+                canonical_end,
+                transform,
+                page=page,
+                bbox=bbox_tuple,
+            )
+        )

     def to_list(self) -> List[Dict[str, Any]]:
-        return [
-            {
+        result: List[Dict[str, Any]] = []
+        for span in self.spans:
+            entry: Dict[str, Any] = {
                 "raw_start": span.raw_start,
                 "raw_end": span.raw_end,
                 "canonical_start": span.canonical_start,
                 "canonical_end": span.canonical_end,
                 "transform": span.transform,
             }
-            for span in self.spans
-        ]
+            if span.page is not None:
+                entry["page"] = span.page
+            if span.bbox is not None:
+                entry["bbox"] = list(span.bbox)
+            result.append(entry)
+        return result
+
+    def extend_from_offset_map(
+        self,
+        entries: Iterable[Mapping[str, Any]],
+        *,
+        transform: str = "offset_map",
+    ) -> None:
+        for entry in entries:
+            raw_start = int(entry.get("char_start", entry.get("raw_start", 0)))
+            raw_end = int(entry.get("char_end", entry.get("raw_end", 0)))
+            canonical_start = int(entry.get("canonical_start", raw_start))
+            canonical_end = int(entry.get("canonical_end", raw_end))
+            page = entry.get("page")
+            bbox = entry.get("bbox") or entry.get("bounding_box")
+            self.add(
+                raw_start,
+                raw_end,
+                canonical_start,
+                canonical_end,
+                transform,
+                page=int(page) if page is not None else None,
+                bbox=bbox,
+            )


 @dataclass(slots=True)
 class Block:
     type: str
     text: str
     start: int
     end: int
     section: str | None = None
     meta: MutableMapping[str, Any] = field(default_factory=dict)


 @dataclass(slots=True)
 class Table:
     caption: str
     headers: List[str]
     rows: List[List[str]]
     start: int
     end: int
     meta: MutableMapping[str, Any] = field(default_factory=dict)


 @dataclass(slots=True)
 class DocumentIR:
     doc_id: str
diff --git a/src/Medical_KG/ir/schemas/document.schema.json b/src/Medical_KG/ir/schemas/document.schema.json
index 3c3d00258dc2ea5f6323e88ae85cf429b028e8ae..c105d42603f6514aa3de62d2ac62af97d7d53e82 100644
--- a/src/Medical_KG/ir/schemas/document.schema.json
+++ b/src/Medical_KG/ir/schemas/document.schema.json
@@ -6,33 +6,40 @@
   "properties": {
     "doc_id": {"type": "string"},
     "source": {"type": "string"},
     "uri": {"type": "string"},
     "language": {"type": "string", "pattern": "^[a-z]{2}$"},
     "text": {"type": "string"},
     "raw_text": {"type": "string"},
     "blocks": {
       "type": "array",
       "items": {"$ref": "block.schema.json"}
     },
     "tables": {
       "type": "array",
       "items": {"$ref": "table.schema.json"}
     },
     "span_map": {
       "type": "array",
       "items": {
         "type": "object",
         "required": ["raw_start", "raw_end", "canonical_start", "canonical_end", "transform"],
         "properties": {
           "raw_start": {"type": "integer", "minimum": 0},
           "raw_end": {"type": "integer", "minimum": 0},
           "canonical_start": {"type": "integer", "minimum": 0},
           "canonical_end": {"type": "integer", "minimum": 0},
-          "transform": {"type": "string"}
+          "transform": {"type": "string"},
+          "page": {"type": "integer", "minimum": 1},
+          "bbox": {
+            "type": "array",
+            "items": {"type": "number"},
+            "minItems": 4,
+            "maxItems": 4
+          }
         }
       }
     },
     "provenance": {"type": "object"}
   },
   "additionalProperties": false
 }
diff --git a/src/Medical_KG/ir/storage.py b/src/Medical_KG/ir/storage.py
index f76fe6c0a2c51e08bf5adae99c01c1c18b355824..98e2f650e79ebe2bd199e309e0d252ba1575edc7 100644
--- a/src/Medical_KG/ir/storage.py
+++ b/src/Medical_KG/ir/storage.py
@@ -1,38 +1,42 @@
 from __future__ import annotations

 import hashlib
 import json
 from pathlib import Path
 from typing import Any, Iterable

 from Medical_KG.ir.models import DocumentIR


 class IrStorage:
     """Simple content-addressable storage using JSONL files."""

     def __init__(self, base_path: Path) -> None:
         self.base_path = base_path

-    def write(self, document: DocumentIR) -> Path:
+    def write(self, document: DocumentIR, *, ledger: Any | None = None) -> Path:
         payload = document.as_dict()
         encoded = json.dumps(payload, sort_keys=True).encode("utf-8")
         digest = hashlib.sha256(encoded).hexdigest()
         filename = f"{document.doc_id}-{digest[:12]}.json"
         path = self.base_path / document.source / filename
         path.parent.mkdir(parents=True, exist_ok=True)
         if path.exists() and path.read_bytes() == encoded:
+            if ledger is not None:
+                ledger.record(document.doc_id, "ir_exists", {"uri": str(path)})
             return path
         path.write_bytes(encoded)
+        if ledger is not None:
+            ledger.record(document.doc_id, "ir_written", {"uri": str(path)})
         return path

     def iter_documents(self, source: str) -> Iterable[dict[str, Any]]:
         directory = self.base_path / source
         if not directory.exists():
             return []
         documents: list[dict[str, Any]] = []
         for file in directory.glob("**/*"):
             if file.is_file():
                 data = json.loads(file.read_text())
                 documents.append(data)
         return documents
diff --git a/src/Medical_KG/ir/validator.py b/src/Medical_KG/ir/validator.py
index fa9470864907ac7eb22d261ca5785ed6fcaf6b36..696309133565bedfdbd61af6bf07a5f59cbebfdf 100644
--- a/src/Medical_KG/ir/validator.py
+++ b/src/Medical_KG/ir/validator.py
@@ -22,56 +22,75 @@ except ModuleNotFoundError:  # pragma: no cover - fallback for lightweight envir
                 if field not in instance:
                     raise _JsonSchemaError(f"Missing required field '{field}'")

 from Medical_KG.ir.models import DocumentIR, ensure_monotonic_spans


 class ValidationError(Exception):
     pass


 class IRValidator:
     def __init__(self, *, schema_dir: Path | None = None) -> None:
         base = schema_dir or Path(__file__).resolve().parent / "schemas"
         self.document_schema = self._load_schema(base / "document.schema.json")
         self.block_schema = self._load_schema(base / "block.schema.json")
         self.table_schema = self._load_schema(base / "table.schema.json")
         self.document_validator = Draft202012Validator(self.document_schema)
         self.block_validator = Draft202012Validator(self.block_schema)
         self.table_validator = Draft202012Validator(self.table_schema)

     def _load_schema(self, path: Path) -> Mapping[str, Any]:
         return json.loads(path.read_text())

     def validate_document(self, document: DocumentIR) -> None:
         payload = document.as_dict()
+        if not document.doc_id:
+            raise ValidationError("Document must have a doc_id")
+        if not document.uri:
+            raise ValidationError("Document must have a uri")
         try:
             self.document_validator.validate(payload)
         except _JsonSchemaError as exc:
             raise ValidationError(f"Document schema validation failed: {exc.message}") from exc

         for block_payload in payload["blocks"]:
             try:
                 self.block_validator.validate(block_payload)
             except _JsonSchemaError as exc:
                 raise ValidationError(f"Block validation failed: {exc.message}") from exc

         for table_payload in payload["tables"]:
             try:
                 self.table_validator.validate(table_payload)
             except _JsonSchemaError as exc:
                 raise ValidationError(f"Table validation failed: {exc.message}") from exc
+            if table_payload["end"] < table_payload["start"]:
+                raise ValidationError("Table span invalid")

         try:
             ensure_monotonic_spans(document.blocks)
         except ValueError as exc:
             raise ValidationError(str(exc)) from exc
         self._validate_offsets(document)
+        self._validate_span_map(payload["span_map"])

     def _validate_offsets(self, document: DocumentIR) -> None:
         text_length = len(document.text)
         for block in document.blocks:
             if block.end > text_length:
                 raise ValidationError("Block span exceeds document length")
-        for table in document.tables:
-            if table.end < table.start:
-                raise ValidationError("Table span invalid")
+            if block.section is not None and not isinstance(block.section, str):
+                raise ValidationError("Block section must be a string or None")
+
+    def _validate_span_map(self, span_map: list[Mapping[str, Any]]) -> None:
+        previous_end = 0
+        for entry in span_map:
+            canonical_start = entry["canonical_start"]
+            canonical_end = entry["canonical_end"]
+            if canonical_start > canonical_end:
+                raise ValidationError("Span map canonical offsets invalid")
+            if canonical_start < previous_end:
+                raise ValidationError("Span map must be monotonic")
+            if "page" in entry and entry["page"] is not None and entry["page"] < 1:
+                raise ValidationError("Span map page numbers must be >= 1")
+            previous_end = canonical_end
diff --git a/src/Medical_KG/kg/fhir.py b/src/Medical_KG/kg/fhir.py
index e7b1e27c81fc12775feaa134f1614abf6ee48b8c..dba7a71afce1506b8b18066e6fad0193d598ef7f 100644
--- a/src/Medical_KG/kg/fhir.py
+++ b/src/Medical_KG/kg/fhir.py
@@ -1,45 +1,103 @@
 from __future__ import annotations

 from dataclasses import dataclass
-from typing import Any, Mapping
+from typing import Any, Iterable, Mapping


 @dataclass(slots=True)
 class FhirResource:
     resource_type: str
     payload: Mapping[str, Any]


+class ConceptLexicon:
+    """Validates that codes originate from allowed code systems."""
+
+    def __init__(self, vocabulary: Mapping[str, Iterable[str]] | None = None) -> None:
+        self._vocabulary = {system: set(codes) for system, codes in (vocabulary or {}).items()}
+
+    def validate(self, system: str, code: str) -> None:
+        allowed = self._vocabulary.get(system)
+        if allowed is not None and code not in allowed:
+            raise ValueError(f"Code {code!r} is not registered for system {system!r}")
+
+
 class EvidenceExporter:
-    """Converts KG nodes to simplified FHIR resources."""
+    """Converts KG nodes into simplified FHIR Evidence resources."""
+
+    def __init__(
+        self,
+        *,
+        lexicon: ConceptLexicon | None = None,
+        ucum_codes: Iterable[str] | None = None,
+    ) -> None:
+        self.lexicon = lexicon or ConceptLexicon()
+        self.ucum_codes = set(ucum_codes or {"1", "mg", "g", "kg", "mL"})
+
+    def _validate_ucum(self, unit: str | None) -> None:
+        if unit and unit not in self.ucum_codes:
+            raise ValueError(f"UCUM code {unit!r} is not supported")

     def export_evidence(self, node: Mapping[str, Any]) -> FhirResource:
+        unit = node.get("unit_ucum")
+        self._validate_ucum(unit)
         statistic = {
-            "statisticType": node.get("type"),
+            "statisticType": {"coding": [{"code": node.get("type")}]} if node.get("type") else None,
             "value": node.get("value"),
             "sampleSize": node.get("n_total"),
         }
-        if node.get("ci_low") is not None and node.get("ci_high") is not None:
-            statistic["confidenceInterval"] = {
-                "low": node["ci_low"],
-                "high": node["ci_high"],
-            }
+        if unit:
+            statistic["unit"] = {"coding": [{"system": "http://unitsofmeasure.org", "code": unit}]}
+        ci_low = node.get("ci_low")
+        ci_high = node.get("ci_high")
+        if ci_low is not None and ci_high is not None:
+            statistic["confidenceInterval"] = {"low": ci_low, "high": ci_high}
+        statistic = {key: value for key, value in statistic.items() if value is not None}
+
         payload = {
             "resourceType": "Evidence",
             "id": node.get("id"),
             "status": "active",
-            "variableDefinition": node.get("variables", []),
+            "description": node.get("description"),
             "statistic": [statistic],
             "note": node.get("notes", []),
         }
         return FhirResource(resource_type="Evidence", payload=payload)

     def export_evidence_variable(self, node: Mapping[str, Any]) -> FhirResource:
+        characteristics = []
+        for entry in node.get("characteristic", []):
+            concept = entry.get("concept")
+            if concept:
+                system = concept.get("system")
+                code = concept.get("code")
+                if system and code:
+                    self.lexicon.validate(system, code)
+            characteristics.append(entry)
         payload = {
             "resourceType": "EvidenceVariable",
             "id": node.get("id"),
             "name": node.get("name"),
             "status": "active",
-            "characteristic": node.get("characteristic", []),
+            "characteristic": characteristics,
         }
         return FhirResource(resource_type="EvidenceVariable", payload=payload)
+
+    def export_provenance(
+        self,
+        extraction_activity: Mapping[str, Any],
+        *,
+        target_reference: str,
+    ) -> FhirResource:
+        agent = {
+            "type": {"coding": [{"system": "http://terminology.hl7.org/CodeSystem/provenance-participant-type", "code": "author"}]},
+            "who": {"display": extraction_activity.get("model")},
+        }
+        payload = {
+            "resourceType": "Provenance",
+            "target": [{"reference": target_reference}],
+            "recorded": extraction_activity.get("timestamp"),
+            "activity": {"display": extraction_activity.get("prompt_hash")},
+            "agent": [agent],
+        }
+        return FhirResource(resource_type="Provenance", payload=payload)
diff --git a/src/Medical_KG/kg/schema.py b/src/Medical_KG/kg/schema.py
index 7fb079ba4a41fd3609c20cf6385796fe0b2a34f2..8bdbb8da2e3220266fe5456ed26d78a823913f3b 100644
--- a/src/Medical_KG/kg/schema.py
+++ b/src/Medical_KG/kg/schema.py
@@ -1,43 +1,258 @@
 from __future__ import annotations

 from dataclasses import dataclass, field
-from typing import Dict, List
+from typing import Dict, List, Mapping
+
+
+@dataclass(slots=True)
+class NodeProperty:
+    name: str
+    type: str
+    required: bool = False
+    description: str = ""
+
+
+@dataclass(slots=True)
+class NodeSchema:
+    label: str
+    properties: List[NodeProperty] = field(default_factory=list)
+
+    def required_properties(self) -> List[NodeProperty]:
+        return [prop for prop in self.properties if prop.required]
+
+    def optional_properties(self) -> List[NodeProperty]:
+        return [prop for prop in self.properties if not prop.required]
+
+    def as_dict(self) -> Dict[str, List[Dict[str, str]]]:
+        return {
+            "required": [
+                {"name": prop.name, "type": prop.type, "description": prop.description}
+                for prop in self.required_properties()
+            ],
+            "optional": [
+                {"name": prop.name, "type": prop.type, "description": prop.description}
+                for prop in self.optional_properties()
+            ],
+        }
+
+
+@dataclass(slots=True)
+class RelationshipSchema:
+    type: str
+    start: str
+    end: str
+    properties: List[NodeProperty] = field(default_factory=list)
+
+    def as_dict(self) -> Dict[str, object]:
+        return {
+            "start": self.start,
+            "end": self.end,
+            "properties": [
+                {"name": prop.name, "type": prop.type, "description": prop.description}
+                for prop in self.properties
+            ],
+        }


 @dataclass(slots=True)
 class Constraint:
     statement: str
     description: str


 @dataclass(slots=True)
 class Index:
     statement: str
     description: str


 @dataclass(slots=True)
 class CDKOSchema:
+    nodes: Dict[str, NodeSchema] = field(default_factory=dict)
+    relationships: Dict[str, RelationshipSchema] = field(default_factory=dict)
     constraints: List[Constraint] = field(default_factory=list)
     indexes: List[Index] = field(default_factory=list)

     @classmethod
     def default(cls) -> "CDKOSchema":
+        nodes = {
+            "Document": NodeSchema(
+                label="Document",
+                properties=[
+                    NodeProperty("uri", "string", required=True, description="Canonical document URI"),
+                    NodeProperty("id", "string", required=True, description="Internal document identifier"),
+                    NodeProperty("source", "string", description="Ingestion source"),
+                    NodeProperty("title", "string", description="Document title"),
+                    NodeProperty("language", "string", description="ISO 639-1 code"),
+                    NodeProperty("meta", "map", description="Arbitrary metadata"),
+                    NodeProperty("created_at", "datetime", description="Ingestion timestamp"),
+                ],
+            ),
+            "Chunk": NodeSchema(
+                label="Chunk",
+                properties=[
+                    NodeProperty("id", "string", required=True, description="Chunk identifier"),
+                    NodeProperty("text", "string", description="Chunk text"),
+                    NodeProperty("path", "string", description="Section path"),
+                    NodeProperty("embedding_qwen", "vector<4096>", description="Qwen embedding"),
+                    NodeProperty("splade_terms", "map", description="SPLADE term weights"),
+                    NodeProperty("model_meta", "map", description="Embedding metadata"),
+                ],
+            ),
+            "Study": NodeSchema(
+                label="Study",
+                properties=[
+                    NodeProperty("nct_id", "string", required=True, description="ClinicalTrials.gov identifier"),
+                    NodeProperty("title", "string", description="Study title"),
+                    NodeProperty("status", "string", description="Recruitment status"),
+                ],
+            ),
+            "Arm": NodeSchema(
+                label="Arm",
+                properties=[
+                    NodeProperty("name", "string", required=True, description="Arm name"),
+                    NodeProperty("type", "string", description="Arm type"),
+                ],
+            ),
+            "Intervention": NodeSchema(
+                label="Intervention",
+                properties=[
+                    NodeProperty("id", "string", required=True, description="Intervention identifier"),
+                    NodeProperty("name", "string", description="Display name"),
+                    NodeProperty("rxcui", "string", description="RxNorm identifier"),
+                    NodeProperty("udi_di", "string", description="Device identifier"),
+                ],
+            ),
+            "Outcome": NodeSchema(
+                label="Outcome",
+                properties=[
+                    NodeProperty("id", "string", required=True),
+                    NodeProperty("name", "string", description="Outcome label"),
+                    NodeProperty("loinc", "string", description="LOINC code"),
+                    NodeProperty("unit_ucum", "string", description="UCUM unit"),
+                ],
+            ),
+            "EvidenceVariable": NodeSchema(
+                label="EvidenceVariable",
+                properties=[
+                    NodeProperty("id", "string", required=True),
+                    NodeProperty("population_json", "map", description="Population definition"),
+                    NodeProperty("interventions_json", "map", description="Interventions definition"),
+                    NodeProperty("comparators_json", "map", description="Comparators definition"),
+                ],
+            ),
+            "Evidence": NodeSchema(
+                label="Evidence",
+                properties=[
+                    NodeProperty("id", "string", required=True),
+                    NodeProperty("type", "string", description="Statistic type"),
+                    NodeProperty("value", "float", description="Point estimate"),
+                    NodeProperty("ci_low", "float", description="Lower CI"),
+                    NodeProperty("ci_high", "float", description="Upper CI"),
+                    NodeProperty("p_value", "float", description="P value"),
+                    NodeProperty("spans_json", "array", description="Supporting spans"),
+                    NodeProperty("provenance", "array", description="Extraction provenance"),
+                ],
+            ),
+            "AdverseEvent": NodeSchema(
+                label="AdverseEvent",
+                properties=[
+                    NodeProperty("id", "string", required=True),
+                    NodeProperty("pt_code", "string", description="MedDRA preferred term"),
+                    NodeProperty("grade", "integer", description="Grade"),
+                ],
+            ),
+            "EligibilityConstraint": NodeSchema(
+                label="EligibilityConstraint",
+                properties=[
+                    NodeProperty("id", "string", required=True),
+                    NodeProperty("type", "string", description="Inclusion or exclusion"),
+                    NodeProperty("logic_json", "map", description="Structured logic"),
+                    NodeProperty("human_text", "string", description="Canonical text"),
+                    NodeProperty("provenance", "array", description="Extraction provenance"),
+                ],
+            ),
+            "ExtractionActivity": NodeSchema(
+                label="ExtractionActivity",
+                properties=[
+                    NodeProperty("id", "string", required=True),
+                    NodeProperty("model", "string", description="Extractor model"),
+                    NodeProperty("version", "string", description="Model version"),
+                    NodeProperty("prompt_hash", "string", description="Prompt hash"),
+                    NodeProperty("schema_hash", "string", description="Schema hash"),
+                    NodeProperty("timestamp", "datetime", description="Extraction timestamp"),
+                ],
+            ),
+        }
+
+        relationships = {
+            "HAS_CHUNK": RelationshipSchema(
+                type="HAS_CHUNK",
+                start="Document",
+                end="Chunk",
+                properties=[NodeProperty("order", "integer", description="Chunk order")],
+            ),
+            "HAS_ARM": RelationshipSchema("HAS_ARM", "Study", "Arm"),
+            "USES_INTERVENTION": RelationshipSchema(
+                type="USES_INTERVENTION",
+                start="Arm",
+                end="Intervention",
+                properties=[NodeProperty("dose", "map", description="Dosing information")],
+            ),
+            "HAS_OUTCOME": RelationshipSchema(
+                type="HAS_OUTCOME",
+                start="Study",
+                end="Outcome",
+                properties=[NodeProperty("timeframe", "string", description="Outcome timeframe")],
+            ),
+            "REPORTS": RelationshipSchema("REPORTS", "EvidenceVariable", "Document"),
+            "MEASURES": RelationshipSchema(
+                type="MEASURES",
+                start="Evidence",
+                end="Outcome",
+                properties=[NodeProperty("confidence", "float", description="Model confidence")],
+            ),
+            "DERIVES_FROM": RelationshipSchema("DERIVES_FROM", "Evidence", "EvidenceVariable"),
+            "HAS_AE": RelationshipSchema(
+                type="HAS_AE",
+                start="Study",
+                end="AdverseEvent",
+                properties=[
+                    NodeProperty("count", "integer", description="Number of events"),
+                    NodeProperty("denominator", "integer", description="Sample size"),
+                    NodeProperty("grade", "integer", description="Worst grade"),
+                ],
+            ),
+            "SATISFIES": RelationshipSchema("SATISFIES", "EligibilityConstraint", "Study"),
+            "WAS_GENERATED_BY": RelationshipSchema("WAS_GENERATED_BY", "Evidence", "ExtractionActivity"),
+        }
+
         constraints = [
             Constraint("CREATE CONSTRAINT doc_uri_unique IF NOT EXISTS FOR (d:Document) REQUIRE d.uri IS UNIQUE", "Unique document URIs"),
             Constraint("CREATE CONSTRAINT chunk_id_unique IF NOT EXISTS FOR (c:Chunk) REQUIRE c.id IS UNIQUE", "Chunk id uniqueness"),
             Constraint("CREATE CONSTRAINT study_nct_unique IF NOT EXISTS FOR (s:Study) REQUIRE s.nct_id IS UNIQUE", "Study id uniqueness"),
-            Constraint("CREATE CONSTRAINT drug_rxcui_unique IF NOT EXISTS FOR (d:Drug) REQUIRE d.rxcui IS UNIQUE", "Drug RxCUI uniqueness"),
-            Constraint("CREATE CONSTRAINT device_udi_unique IF NOT EXISTS FOR (x:Device) REQUIRE x.udi_di IS UNIQUE", "Device UDI uniqueness"),
+            Constraint("CREATE CONSTRAINT outcome_loinc_unique IF NOT EXISTS FOR (o:Outcome) REQUIRE o.loinc IS UNIQUE", "Outcome LOINC uniqueness"),
+            Constraint("CREATE CONSTRAINT evidence_id_unique IF NOT EXISTS FOR (e:Evidence) REQUIRE e.id IS UNIQUE", "Evidence id uniqueness"),
+            Constraint("CREATE CONSTRAINT evvar_id_unique IF NOT EXISTS FOR (v:EvidenceVariable) REQUIRE v.id IS UNIQUE", "Evidence variable id uniqueness"),
         ]
+
         indexes = [
-            Index("CREATE VECTOR INDEX chunk_qwen_idx IF NOT EXISTS FOR (c:Chunk) ON (c.embedding_qwen) OPTIONS {indexConfig: {`vector.dimensions`: 4096, `vector.similarity_function`: 'cosine'}}", "Chunk embedding vector index"),
-            Index("CREATE FULLTEXT INDEX chunk_text_ft IF NOT EXISTS FOR (n:Chunk) ON EACH [n.text] OPTIONS {analyzer: 'english'}", "Chunk full text index"),
+            Index(
+                "CREATE VECTOR INDEX chunk_qwen_idx IF NOT EXISTS FOR (c:Chunk) ON (c.embedding_qwen) OPTIONS {indexConfig: {`vector.dimensions`: 4096, `vector.similarity_function`: 'cosine'}}",
+                "Chunk embedding vector index",
+            ),
+            Index(
+                "CREATE FULLTEXT INDEX chunk_text_ft IF NOT EXISTS FOR (n:Chunk) ON EACH [n.text] OPTIONS {analyzer: 'english'}",
+                "Chunk full text index",
+            ),
         ]
-        return cls(constraints=constraints, indexes=indexes)

-    def as_statements(self) -> Dict[str, List[str]]:
+        return cls(nodes=nodes, relationships=relationships, constraints=constraints, indexes=indexes)
+
+    def describe(self) -> Mapping[str, object]:
         return {
-            "constraints": [c.statement for c in self.constraints],
-            "indexes": [i.statement for i in self.indexes],
+            "nodes": {label: schema.as_dict() for label, schema in self.nodes.items()},
+            "relationships": {rtype: schema.as_dict() for rtype, schema in self.relationships.items()},
+            "constraints": [constraint.statement for constraint in self.constraints],
+            "indexes": [index.statement for index in self.indexes],
         }
diff --git a/src/Medical_KG/kg/validators.py b/src/Medical_KG/kg/validators.py
index cbaaa85dddaaf4f9632ab1cc062908793212de7d..e302cb2b76134e4bfb38620f240b7bebf86fcb7e 100644
--- a/src/Medical_KG/kg/validators.py
+++ b/src/Medical_KG/kg/validators.py
@@ -1,63 +1,131 @@
 from __future__ import annotations

+import hashlib
+import json
 from dataclasses import dataclass
-from typing import Any, Iterable, Mapping
+from typing import Any, Iterable, List, Mapping


 class KgValidationError(Exception):
-    pass
+    """Raised when the knowledge-graph payload fails validation."""


 @dataclass(slots=True)
 class ValidationIssue:
-    node_id: str
     reason: str
+    payload_hash: str
+    payload: Mapping[str, Any]
+
+
+class DeadLetterQueue:
+    """Collects invalid payloads for later inspection."""
+
+    def __init__(self) -> None:
+        self.entries: List[ValidationIssue] = []
+
+    def record(self, reason: str, payload: Mapping[str, Any]) -> ValidationIssue:
+        digest = hashlib.sha256(json.dumps(payload, sort_keys=True).encode("utf-8")).hexdigest()
+        issue = ValidationIssue(reason=reason, payload_hash=digest, payload=payload)
+        self.entries.append(issue)
+        return issue


 class KgValidator:
-    """Lightweight validator emulating SHACL rules."""
+    """Performs lightweight SHACL-style validations on KG nodes and relationships."""

-    def __init__(self, *, ucum_codes: Iterable[str] | None = None) -> None:
-        self.ucum_codes = set(ucum_codes or {"1", "mg", "kg", "mL"})
+    def __init__(self, *, ucum_codes: Iterable[str] | None = None, dead_letter: DeadLetterQueue | None = None) -> None:
+        self.ucum_codes = set(ucum_codes or {"1", "mg", "g", "kg", "mL"})
+        self.dead_letter = dead_letter or DeadLetterQueue()

-    def validate_ucum(self, node: Mapping[str, Any]) -> None:
-        unit = node.get("unit_ucum") or node.get("unit")
-        if unit and unit not in self.ucum_codes:
-            raise KgValidationError(f"Invalid UCUM code: {unit}")
+    def validate_batch(self, nodes: Iterable[Mapping[str, Any]], relationships: Iterable[Mapping[str, Any]]) -> DeadLetterQueue:
+        outcomes_by_id: dict[str, Mapping[str, Any]] = {}
+        evidence_by_id: dict[str, Mapping[str, Any]] = {}

-    def validate_spans(self, node: Mapping[str, Any]) -> None:
-        spans = node.get("spans_json")
-        if spans:
-            for span in spans:
-                start = span.get("start")
-                end = span.get("end")
-                length = span.get("length", 0)
-                if start is None or end is None or start < 0 or end < 0 or end < start:
-                    raise KgValidationError("Invalid span offsets")
-                if length and end - start != length:
-                    raise KgValidationError("Span length mismatch")
-        else:
-            raise KgValidationError("spans_json missing or empty")
-
-    def ensure_provenance(self, node: Mapping[str, Any]) -> None:
-        provenance = node.get("provenance", [])
-        if not provenance:
-            raise KgValidationError("Node missing provenance references")
+        for node in nodes:
+            label = node.get("label")
+            try:
+                self.validate_node(node)
+            except KgValidationError as exc:
+                self.dead_letter.record(str(exc), node)
+                continue
+            if label == "Outcome":
+                outcomes_by_id[str(node.get("id"))] = node
+            if label == "Evidence":
+                evidence_by_id[str(node.get("id"))] = node

-    def validate_relationship(self, relationship: Mapping[str, Any]) -> None:
-        if relationship.get("type") == "HAS_AE":
-            count = relationship.get("count", 0)
-            if count < 0:
-                raise KgValidationError("Adverse event count must be non-negative")
-            grade = relationship.get("grade")
-            if grade is not None and grade not in {1, 2, 3, 4, 5}:
-                raise KgValidationError("Grade must be between 1 and 5")
+        for relationship in relationships:
+            try:
+                self.validate_relationship(relationship)
+            except KgValidationError as exc:
+                self.dead_letter.record(str(exc), relationship)
+
+        self._validate_code_presence(evidence_by_id, outcomes_by_id, relationships)
+
+        if self.dead_letter.entries:
+            raise KgValidationError(f"Knowledge graph validation produced {len(self.dead_letter.entries)} issue(s)")
+        return self.dead_letter

     def validate_node(self, node: Mapping[str, Any]) -> None:
         label = node.get("label")
+        if not label:
+            raise KgValidationError("Node missing label")
         if label in {"Evidence", "Outcome"}:
-            self.validate_ucum(node)
+            self._validate_ucum(node)
         if label in {"Evidence", "EvidenceVariable", "EligibilityConstraint"}:
-            self.ensure_provenance(node)
-        if "spans_json" in node:
-            self.validate_spans(node)
+            self._ensure_provenance(node)
+        if node.get("spans_json"):
+            self._validate_spans(node["spans_json"])
+
+    def validate_relationship(self, relationship: Mapping[str, Any]) -> None:
+        rel_type = relationship.get("type")
+        if not rel_type:
+            raise KgValidationError("Relationship missing type")
+        if rel_type == "HAS_AE":
+            count = relationship.get("count", 0)
+            denom = relationship.get("denominator", 0)
+            grade = relationship.get("grade")
+            if count is not None and count < 0:
+                raise KgValidationError("Adverse event count must be non-negative")
+            if denom is not None and denom < 0:
+                raise KgValidationError("Adverse event denominator must be non-negative")
+            if grade is not None and grade not in {1, 2, 3, 4, 5}:
+                raise KgValidationError("Adverse event grade must be between 1 and 5")
+
+    def _validate_ucum(self, node: Mapping[str, Any]) -> None:
+        unit = node.get("unit_ucum") or node.get("properties", {}).get("unit_ucum")
+        if unit and unit not in self.ucum_codes:
+            raise KgValidationError(f"Invalid UCUM code: {unit}")
+
+    def _ensure_provenance(self, node: Mapping[str, Any]) -> None:
+        provenance = node.get("provenance") or node.get("properties", {}).get("provenance")
+        if not provenance:
+            raise KgValidationError("Node missing provenance references")
+
+    def _validate_spans(self, spans: Iterable[Mapping[str, Any]]) -> None:
+        for span in spans:
+            start = span.get("start")
+            end = span.get("end")
+            if start is None or end is None or start < 0 or end < 0 or end < start:
+                raise KgValidationError("Invalid span offsets")
+
+    def _validate_code_presence(
+        self,
+        evidence_nodes: Mapping[str, Mapping[str, Any]],
+        outcome_nodes: Mapping[str, Mapping[str, Any]],
+        relationships: Iterable[Mapping[str, Any]],
+    ) -> None:
+        evidence_measure_links: dict[str, str] = {}
+        for relationship in relationships:
+            if relationship.get("type") == "MEASURES":
+                evidence_measure_links[str(relationship.get("start_id"))] = str(relationship.get("end_id"))
+
+        for evidence_id, node in evidence_nodes.items():
+            outcome_loinc = node.get("outcome_loinc")
+            if not outcome_loinc:
+                continue
+            linked_outcome_id = evidence_measure_links.get(str(node.get("id", evidence_id)))
+            if not linked_outcome_id:
+                raise KgValidationError("Evidence node missing MEASURES relationship")
+            outcome = outcome_nodes.get(linked_outcome_id)
+            if not outcome or outcome.get("loinc") != outcome_loinc:
+                raise KgValidationError("Evidence outcome_loinc does not match linked Outcome node")
diff --git a/src/Medical_KG/kg/writer.py b/src/Medical_KG/kg/writer.py
index 13a67a4dd67034010e424c63d815d7c7a784c6bb..3ad593685683b8698c80128c45ae7130ba6dcb94 100644
--- a/src/Medical_KG/kg/writer.py
+++ b/src/Medical_KG/kg/writer.py
@@ -1,64 +1,211 @@
 from __future__ import annotations

 from dataclasses import dataclass
-from typing import Any, Dict, Iterable, Mapping
+from typing import Any, Dict, Iterable, Mapping, MutableMapping


 @dataclass(slots=True)
 class WriteStatement:
     cypher: str
     parameters: Dict[str, Any]


+NODE_KEYS: Mapping[str, str] = {
+    "Document": "uri",
+    "Chunk": "id",
+    "Study": "nct_id",
+    "Arm": "id",
+    "Intervention": "id",
+    "Outcome": "id",
+    "EvidenceVariable": "id",
+    "Evidence": "id",
+    "AdverseEvent": "id",
+    "EligibilityConstraint": "id",
+    "ExtractionActivity": "id",
+}
+
+
 class KnowledgeGraphWriter:
-    """Generates Cypher statements for Neo4j upserts."""
+    """Generates idempotent Cypher statements for Neo4j upserts."""

     def __init__(self) -> None:
         self._statements: list[WriteStatement] = []

     @property
     def statements(self) -> Iterable[WriteStatement]:
         return list(self._statements)

+    def clear(self) -> None:
+        self._statements.clear()
+
+    def _merge_node(self, label: str, payload: Mapping[str, Any]) -> None:
+        key = NODE_KEYS.get(label)
+        if key is None:
+            raise ValueError(f"Unknown node label '{label}'")
+        if key not in payload:
+            raise ValueError(f"Payload for {label} missing key '{key}'")
+        cypher = f"MERGE (n:{label} {{{key}: $props.{key}}}) SET n += $props"
+        self._statements.append(WriteStatement(cypher=cypher, parameters={"props": dict(payload)}))
+
     def write_document(self, payload: Mapping[str, Any]) -> None:
+        self._merge_node("Document", payload)
+
+    def write_chunk(self, payload: Mapping[str, Any], *, document_uri: str | None = None, order: int | None = None) -> None:
+        self._merge_node("Chunk", payload)
+        if document_uri:
+            params: Dict[str, Any] = {"doc_uri": document_uri, "chunk_id": payload["id"]}
+            cypher = (
+                "MATCH (d:Document {uri: $doc_uri}) MATCH (c:Chunk {id: $chunk_id}) "
+                "MERGE (d)-[r:HAS_CHUNK]->(c)"
+            )
+            if order is not None:
+                cypher += " SET r.order = $order"
+                params["order"] = order
+            self._statements.append(WriteStatement(cypher=cypher, parameters=params))
+
+    def write_study(self, payload: Mapping[str, Any], *, document_uri: str | None = None) -> None:
+        self._merge_node("Study", payload)
+        if document_uri:
+            cypher = (
+                "MATCH (d:Document {uri: $doc_uri}) MATCH (s:Study {nct_id: $nct_id}) "
+                "MERGE (d)-[:DESCRIBES]->(s)"
+            )
+            self._statements.append(
+                WriteStatement(cypher=cypher, parameters={"doc_uri": document_uri, "nct_id": payload["nct_id"]})
+            )
+
+    def write_arm(self, payload: Mapping[str, Any], *, study_nct_id: str) -> None:
+        self._merge_node("Arm", payload)
         cypher = (
-            "MERGE (d:Document {id: $id}) "
-            "SET d += {source: $source, uri: $uri, title: $title, language: $language, publication_date: $publication_date, meta: $meta, updated_at: timestamp()}"
+            "MATCH (s:Study {nct_id: $nct_id}) MATCH (a:Arm {id: $arm_id}) "
+            "MERGE (s)-[:HAS_ARM]->(a)"
         )
-        params = {
-            "id": payload["id"],
-            "source": payload.get("source"),
-            "uri": payload.get("uri"),
-            "title": payload.get("title"),
-            "language": payload.get("language"),
-            "publication_date": payload.get("publication_date"),
-            "meta": payload.get("meta", {}),
-        }
+        params = {"nct_id": study_nct_id, "arm_id": payload["id"]}
         self._statements.append(WriteStatement(cypher=cypher, parameters=params))

-    def write_chunk(self, payload: Mapping[str, Any]) -> None:
+    def write_intervention(self, payload: Mapping[str, Any], *, arm_id: str) -> None:
+        self._merge_node("Intervention", payload)
         cypher = (
-            "MERGE (c:Chunk {id: $id}) SET c += {text: $text, section: $section, start: $start, end: $end, token_count: $token_count, intent: $intent, path: $path}"  # noqa: E501
+            "MATCH (a:Arm {id: $arm_id}) MATCH (i:Intervention {id: $intervention_id}) "
+            "MERGE (a)-[r:USES_INTERVENTION]->(i) SET r += $rel_props"
         )
-        params = {
-            "id": payload["id"],
-            "text": payload.get("text"),
-            "section": payload.get("section"),
-            "start": payload.get("start"),
-            "end": payload.get("end"),
-            "token_count": payload.get("token_count"),
-            "intent": payload.get("intent"),
-            "path": payload.get("path"),
-        }
+        rel_props = {key: payload[key] for key in ("dose",) if key in payload}
+        params = {"arm_id": arm_id, "intervention_id": payload["id"], "rel_props": rel_props}
         self._statements.append(WriteStatement(cypher=cypher, parameters=params))

-    def write_relationship(self, rel_type: str, start_id: str, end_id: str, properties: Mapping[str, Any] | None = None) -> None:
+    def write_outcome(self, payload: Mapping[str, Any], *, study_nct_id: str | None = None) -> None:
+        self._merge_node("Outcome", payload)
+        if study_nct_id:
+            cypher = (
+                "MATCH (s:Study {nct_id: $nct_id}) MATCH (o:Outcome {id: $outcome_id}) "
+                "MERGE (s)-[:HAS_OUTCOME]->(o)"
+            )
+            self._statements.append(
+                WriteStatement(cypher=cypher, parameters={"nct_id": study_nct_id, "outcome_id": payload["id"]})
+            )
+
+    def write_evidence_variable(self, payload: Mapping[str, Any], *, document_uri: str | None = None) -> None:
+        self._merge_node("EvidenceVariable", payload)
+        if document_uri:
+            cypher = (
+                "MATCH (ev:EvidenceVariable {id: $ev_id}) MATCH (d:Document {uri: $doc_uri}) "
+                "MERGE (ev)-[:REPORTS]->(d)"
+            )
+            self._statements.append(
+                WriteStatement(cypher=cypher, parameters={"ev_id": payload["id"], "doc_uri": document_uri})
+            )
+
+    def write_evidence(
+        self,
+        payload: Mapping[str, Any],
+        *,
+        outcome_id: str,
+        variable_id: str,
+        extraction_activity_id: str | None = None,
+    ) -> None:
+        self._merge_node("Evidence", payload)
+        self._statements.append(
+            WriteStatement(
+                cypher=(
+                    "MATCH (e:Evidence {id: $evidence_id}) MATCH (o:Outcome {id: $outcome_id}) "
+                    "MERGE (e)-[r:MEASURES]->(o) SET r += $rel_props"
+                ),
+                parameters={
+                    "evidence_id": payload["id"],
+                    "outcome_id": outcome_id,
+                    "rel_props": (
+                        {"confidence": payload["confidence"]}
+                        if payload.get("confidence") is not None
+                        else {}
+                    ),
+                },
+            )
+        )
+        self._statements.append(
+            WriteStatement(
+                cypher=(
+                    "MATCH (e:Evidence {id: $evidence_id}) MATCH (v:EvidenceVariable {id: $variable_id}) "
+                    "MERGE (e)-[:DERIVES_FROM]->(v)"
+                ),
+                parameters={"evidence_id": payload["id"], "variable_id": variable_id},
+            )
+        )
+        if extraction_activity_id:
+            self.link_generated_by("Evidence", payload["id"], extraction_activity_id)
+
+    def write_adverse_event(self, payload: Mapping[str, Any], *, study_nct_id: str) -> None:
+        self._merge_node("AdverseEvent", payload)
+        rel_props: MutableMapping[str, Any] = {}
+        for field in ("count", "denominator", "grade"):
+            if field in payload:
+                rel_props[field] = payload[field]
         cypher = (
-            "MATCH (a {id: $start_id}) MATCH (b {id: $end_id}) "
-            f"MERGE (a)-[r:{rel_type}]->(b) SET r += $properties"
+            "MATCH (s:Study {nct_id: $nct_id}) MATCH (ae:AdverseEvent {id: $ae_id}) "
+            "MERGE (s)-[r:HAS_AE]->(ae) SET r += $rel_props"
         )
-        params = {"start_id": start_id, "end_id": end_id, "properties": dict(properties or {})}
+        params = {"nct_id": study_nct_id, "ae_id": payload["id"], "rel_props": rel_props}
         self._statements.append(WriteStatement(cypher=cypher, parameters=params))

-    def clear(self) -> None:
-        self._statements.clear()
+    def write_eligibility_constraint(self, payload: Mapping[str, Any], *, study_nct_id: str) -> None:
+        self._merge_node("EligibilityConstraint", payload)
+        cypher = (
+            "MATCH (e:EligibilityConstraint {id: $constraint_id}) MATCH (s:Study {nct_id: $nct_id}) "
+            "MERGE (e)-[:SATISFIES]->(s)"
+        )
+        params = {"constraint_id": payload["id"], "nct_id": study_nct_id}
+        self._statements.append(WriteStatement(cypher=cypher, parameters=params))
+
+    def write_extraction_activity(self, payload: Mapping[str, Any]) -> None:
+        self._merge_node("ExtractionActivity", payload)
+
+    def link_generated_by(self, node_label: str, node_id: str, activity_id: str) -> None:
+        key = NODE_KEYS.get(node_label)
+        if key is None:
+            raise ValueError(f"Unknown node label '{node_label}'")
+        cypher = (
+            f"MATCH (n:{node_label} {{{key}: $node_id}}) MATCH (a:ExtractionActivity {{id: $activity_id}}) "
+            "MERGE (n)-[:WAS_GENERATED_BY]->(a)"
+        )
+        self._statements.append(WriteStatement(cypher=cypher, parameters={"node_id": node_id, "activity_id": activity_id}))
+
+    def write_relationship(
+        self,
+        rel_type: str,
+        *,
+        start_label: str,
+        start_key: str,
+        start_value: Any,
+        end_label: str,
+        end_key: str,
+        end_value: Any,
+        properties: Mapping[str, Any] | None = None,
+    ) -> None:
+        cypher = (
+            f"MATCH (a:{start_label} {{{start_key}: $start_val}}) MATCH (b:{end_label} {{{end_key}: $end_val}}) "
+            f"MERGE (a)-[r:{rel_type}]->(b)"
+        )
+        params: Dict[str, Any] = {"start_val": start_value, "end_val": end_value}
+        if properties:
+            cypher += " SET r += $props"
+            params["props"] = dict(properties)
+        self._statements.append(WriteStatement(cypher=cypher, parameters=params))
diff --git a/tests/ir/test_ir_pipeline.py b/tests/ir/test_ir_pipeline.py
new file mode 100644
index 0000000000000000000000000000000000000000..4b5f62ad30030b61188fe650d149028893ed8008
--- /dev/null
+++ b/tests/ir/test_ir_pipeline.py
@@ -0,0 +1,108 @@
+from __future__ import annotations
+
+from pathlib import Path
+
+import pytest
+
+from Medical_KG.ir.builder import ClinicalTrialsBuilder, DailyMedBuilder, GuidelineBuilder, MinerUBuilder
+from Medical_KG.ir.models import Block
+from Medical_KG.ir.normalizer import TextNormalizer
+from Medical_KG.ir.storage import IrStorage
+from Medical_KG.ir.validator import IRValidator, ValidationError
+
+
+class _MemoryLedger:
+    def __init__(self) -> None:
+        self.calls: list[tuple[str, str, dict[str, str]]] = []
+
+    def record(self, doc_id: str, state: str, metadata: dict[str, str]) -> None:
+        self.calls.append((doc_id, state, metadata))
+
+
+def test_text_normalization_handles_dehyphenation() -> None:
+    normalizer = TextNormalizer(dictionary={"treatment"})
+    result = normalizer.normalize("treat-\nment improves outcomes\n\nNew paragraph")
+    assert "treatment" in result.text
+    assert result.language == "en"
+
+
+def test_clinical_trials_builder_creates_blocks(tmp_path: Path) -> None:
+    builder = ClinicalTrialsBuilder()
+    study = {
+        "title": "Trial Title",
+        "status": "Completed",
+        "eligibility": "Adults with sepsis",
+        "outcomes": [
+            {"measure": "Mortality", "description": "28 day", "timeFrame": "28 days"},
+        ],
+    }
+    document = builder.build_from_study(doc_id="study#1", uri="https://clinicaltrials.gov/study#1", study=study)
+    assert any(isinstance(block, Block) and block.section == "eligibility" for block in document.blocks)
+    validator = IRValidator()
+    validator.validate_document(document)
+    storage = IrStorage(tmp_path)
+    ledger = _MemoryLedger()
+    path = storage.write(document, ledger=ledger)
+    assert path.exists()
+    assert any(state == "ir_written" for _, state, _ in ledger.calls)
+
+
+def test_daily_med_builder_creates_loinc_blocks() -> None:
+    builder = DailyMedBuilder()
+    spl = {
+        "sections": [
+            {"loinc": "34084-4", "text": "Dosage guidelines"},
+            {"loinc": "43683-2", "text": "Contraindications"},
+        ],
+        "ingredients": [
+            {"name": "Drug", "strength": "5 mg", "basis": "per tablet"},
+        ],
+    }
+    document = builder.build_from_spl(doc_id="spl#1", uri="https://dailymed/setid", spl=spl)
+    assert {block.meta["loinc"] for block in document.blocks} == {"34084-4", "43683-2"}
+    IRValidator().validate_document(document)
+
+
+def test_mineru_builder_uses_offset_map() -> None:
+    builder = MinerUBuilder()
+    artifacts = {
+        "markdown": "# Title\n\nParagraph text",
+        "blocks": [
+            {"type": "heading", "text": "Title", "section": "h1", "path": "0"},
+            {"type": "paragraph", "text": "Paragraph text", "section": "body", "path": "1"},
+        ],
+        "tables": [
+            {"caption": "Table 1", "headers": ["A"], "rows": [["B"]], "page": 1},
+        ],
+        "offset_map": [
+            {"char_start": 0, "char_end": 5, "canonical_start": 0, "canonical_end": 5, "page": 1, "bbox": [0, 0, 10, 10]},
+            {"char_start": 6, "char_end": 20, "canonical_start": 6, "canonical_end": 20, "page": 1, "bbox": [0, 10, 10, 20]},
+        ],
+    }
+    document = builder.build_from_artifacts(doc_id="mineru#1", uri="s3://bucket/doc", artifacts=artifacts)
+    assert document.span_map.to_list()[0]["page"] == 1
+    IRValidator().validate_document(document)
+
+
+def test_guideline_builder_parses_html() -> None:
+    builder = GuidelineBuilder()
+    html = """
+    <html><body>
+        <h1>Guideline Title</h1>
+        <p>Recommendation paragraph.</p>
+        <ul><li>List item</li></ul>
+    </body></html>
+    """
+    document = builder.build_from_html(doc_id="guideline#1", uri="https://guideline", html=html)
+    types = {block.type for block in document.blocks}
+    assert types.issuperset({"heading", "paragraph", "list_item"})
+    IRValidator().validate_document(document)
+
+
+def test_validator_rejects_bad_span_map() -> None:
+    builder = ClinicalTrialsBuilder()
+    study = {"title": "Title", "status": "Active", "eligibility": "Patients"}
+    document = builder.build_from_study(doc_id="study#2", uri="uri", study=study)
+    document.span_map.add(10, 5, 10, 8, "invalid")
+    with pytest.raises(ValidationError):
+        IRValidator().validate_document(document)
diff --git a/tests/kg/test_kg.py b/tests/kg/test_kg.py
new file mode 100644
index 0000000000000000000000000000000000000000..5754b87c9133c8d0eaf28a36bd4bad49f4f777fe
--- /dev/null
+++ b/tests/kg/test_kg.py
@@ -0,0 +1,77 @@
+from __future__ import annotations
+
+import pytest
+
+from Medical_KG.kg.fhir import ConceptLexicon, EvidenceExporter
+from Medical_KG.kg.schema import CDKOSchema
+from Medical_KG.kg.validators import DeadLetterQueue, KgValidationError, KgValidator
+from Medical_KG.kg.writer import KnowledgeGraphWriter
+
+
+def test_schema_default_contains_expected_nodes() -> None:
+    schema = CDKOSchema.default()
+    description = schema.describe()
+    assert "Document" in description["nodes"]
+    assert "HAS_CHUNK" in description["relationships"]
+    assert any("doc_uri_unique" in constraint for constraint in description["constraints"])
+
+
+def test_knowledge_graph_writer_generates_statements() -> None:
+    writer = KnowledgeGraphWriter()
+    writer.write_document({"id": "doc-1", "uri": "doc://1", "source": "pmc"})
+    writer.write_chunk({"id": "chunk-1", "text": "hello"}, document_uri="doc://1", order=0)
+    writer.write_outcome({"id": "out-1", "loinc": "1234-5"})
+    writer.write_evidence_variable({"id": "ev-1", "provenance": ["activity"]}, document_uri="doc://1")
+    writer.write_evidence({"id": "e-1", "provenance": ["activity"], "unit_ucum": "mg"}, outcome_id="out-1", variable_id="ev-1")
+    statements = list(writer.statements)
+    assert any("MERGE (n:Document" in stmt.cypher for stmt in statements)
+    assert any("HAS_CHUNK" in stmt.cypher for stmt in statements)
+
+
+def test_validator_records_dead_letters() -> None:
+    dead_letter = DeadLetterQueue()
+    validator = KgValidator(ucum_codes={"mg"}, dead_letter=dead_letter)
+    nodes = [
+        {"label": "Outcome", "id": "out-1", "loinc": "1234-5", "unit_ucum": "mg"},
+        {"label": "Evidence", "id": "ev-1", "unit_ucum": "invalid", "provenance": ["run"], "outcome_loinc": "1234-5"},
+    ]
+    relationships = [{"type": "MEASURES", "start_id": "ev-1", "end_id": "out-1"}]
+    with pytest.raises(KgValidationError):
+        validator.validate_batch(nodes, relationships)
+    assert any("UCUM" in issue.reason for issue in dead_letter.entries)
+
+
+def test_validator_requires_provenance() -> None:
+    validator = KgValidator(ucum_codes={"mg"})
+    nodes = [
+        {"label": "Outcome", "id": "out-1", "loinc": "1234-5", "unit_ucum": "mg"},
+        {"label": "Evidence", "id": "ev-1", "unit_ucum": "mg", "outcome_loinc": "1234-5"},
+    ]
+    relationships = [{"type": "MEASURES", "start_id": "ev-1", "end_id": "out-1"}]
+    with pytest.raises(KgValidationError):
+        validator.validate_batch(nodes, relationships)
+
+
+def test_fhir_exporter_validates_codes() -> None:
+    lexicon = ConceptLexicon({"http://snomed.info/sct": {"123"}})
+    exporter = EvidenceExporter(lexicon=lexicon, ucum_codes={"mg"})
+    node = {
+        "id": "evvar-1",
+        "name": "Population",
+        "characteristic": [
+            {"concept": {"system": "http://snomed.info/sct", "code": "123"}},
+        ],
+    }
+    resource = exporter.export_evidence_variable(node)
+    assert resource.payload["resourceType"] == "EvidenceVariable"
+
+    with pytest.raises(ValueError):
+        exporter.export_evidence_variable(
+            {
+                "id": "evvar-2",
+                "name": "Population",
+                "characteristic": [
+                    {"concept": {"system": "http://snomed.info/sct", "code": "999"}},
+                ],
+            }
+        )

EOF
)
